<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://inlong.apache.org/blog</id>
    <title>Apache InLong Blog</title>
    <updated>2025-01-03T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://inlong.apache.org/blog"/>
    <subtitle>Apache InLong Blog</subtitle>
    <icon>https://inlong.apache.org/img/logo.svg</icon>
    <entry>
        <title type="html"><![CDATA[Release 2.1.0]]></title>
        <id>https://inlong.apache.org/blog/2025/01/03/release-2.1.0</id>
        <link href="https://inlong.apache.org/blog/2025/01/03/release-2.1.0"/>
        <updated>2025-01-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache InLong has recently released version 2.1.0, which has closed over 120 issue, including more than 4 major features and over 110 optimizations.]]></summary>
        <content type="html"><![CDATA[<p>Apache InLong has recently released version 2.1.0, which has closed over 120 issue, including more than 4 major features and over 110 optimizations.
The main accomplishments include Dashboard supports batch operation of nodes, Manager supports multiple scheduling engines, Agent supports COS data sources,
Sort supports archiving dirty data through the InLong SDK. Simultaneously optimize the user experience of Apache InLong operations and maintenance. In Apache InLong 2.1.0 version, a large number of other features have also been completed.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-apache-inlong">About Apache InLong<a href="#about-apache-inlong" class="hash-link" aria-label="Direct link to About Apache InLong" title="Direct link to About Apache InLong">​</a></h2><p>As the industry's first one-stop, all-scenario massive data integration framework, Apache InLong provides automated, secure, reliable,
and high-performance data transmission capabilities, enabling businesses to quickly build stream-based data analysis, modeling, and applications.
Currently, InLong is widely used in various industries including advertising, payment, social networking, gaming, and artificial intelligence,
serving thousands of businesses, with high-performance scenarios processing over hundreds of billions of records per day and highly reliable scenarios
handling over tens of trillions of records per day.</p><p>The core keywords for InLong's project positioning are "one-stop," "all-scenario," and "massive data." For "one-stop,"
we aim to shield technical details, provide complete data integration and supporting services, and achieve out-of-the-box usability;
for "all-scenario," we aim to offer comprehensive solutions covering common data integration scenarios in the big data field;
for "massive data," we hope to leverage architectural advantages such as layered data links, fully extensible components,
and built-in multi-cluster management to stably support even larger data volumes based on hundreds of billions of records per day.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="210-overview">2.1.0 Overview<a href="#210-overview" class="hash-link" aria-label="Direct link to 2.1.0 Overview" title="Direct link to 2.1.0 Overview">​</a></h2><p>Apache InLong has recently released version 2.1.0, which has closed over 120 issue, including more than 4 major features and over 110 optimizations.
The main accomplishments include </p><ul><li>Dashboard supports batch operation of nodes</li><li>Manager supports multiple scheduling engines</li><li>Agent supports COS data sources</li><li>Sort supports archiving dirty data through the InLong SDK. </li></ul><p>Simultaneously optimize the user experience of Apache InLong operations and maintenance. In Apache InLong 2.1.0 version, a large number of other features have also been completed.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dashboard-module">Dashboard Module<a href="#dashboard-module" class="hash-link" aria-label="Direct link to Dashboard Module" title="Direct link to Dashboard Module">​</a></h3><ul><li>Support COS data source</li><li>Support batch operation of agents: restart, upgrade</li><li>Support exporting audit data as CSV files</li><li>Support sorting of audit data and comparison of differences</li><li>Support queries for all types of indicators</li><li>Support data preview field segmentation</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-module">Manager Module<a href="#manager-module" class="hash-link" aria-label="Direct link to Manager Module" title="Direct link to Manager Module">​</a></h3><ul><li>Support COS data source</li><li>Support managing multiple scheduling engines: AirFlow、DolphinScheduler</li><li>Support dirty data management and querying</li><li>Support querying heartbeat information based on IP</li><li>Limit one IP to only belong to one cluster</li><li>Provide an API for querying of dirty data archiving</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-module">Agent Module<a href="#agent-module" class="hash-link" aria-label="Direct link to Agent Module" title="Direct link to Agent Module">​</a></h3><ul><li>Support COS data source</li><li>Support quick startup and shutdown</li><li>Support starting multiple instances</li><li>Support data supplementation in chronological order</li><li>Optimize the logic of the Installer process guardian for Agent</li><li>Support supplementary recording based on local data time</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-module">Sort Module<a href="#sort-module" class="hash-link" aria-label="Direct link to Sort Module" title="Direct link to Sort Module">​</a></h3><ul><li>Added Elasticsearch connector based on Flink 1.18</li><li>Support KV separation on Kafka Sink</li><li>Support audit data reporting</li><li>Tube Connector source supports dirty data archiving</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sdk-module">SDK Module<a href="#sdk-module" class="hash-link" aria-label="Direct link to SDK Module" title="Direct link to SDK Module">​</a></h3><ul><li>Transform SDK adds 7 new functions</li><li>Add Dirty Data Archiving SDK</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="audit-module">Audit Module<a href="#audit-module" class="hash-link" aria-label="Direct link to Audit Module" title="Direct link to Audit Module">​</a></h3><ul><li>Audit Proxy increases metric reporting</li><li>Audit Store adds metric reporting</li><li>Audit Service increases metric reporting</li><li>Add asynchronous flush audit data interface</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tubemq-module">TubeMQ Module<a href="#tubemq-module" class="hash-link" aria-label="Direct link to TubeMQ Module" title="Direct link to TubeMQ Module">​</a></h3><ul><li>Write the consumption offset information to a local file</li><li>Optimize the load balancing logic of the Go version SDK</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="others">Others<a href="#others" class="hash-link" aria-label="Direct link to Others" title="Direct link to Others">​</a></h3><ul><li>Pipeline supports parallel build</li><li>Support Manager to configure volumes</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="210-feature-introduction">2.1.0 Feature Introduction<a href="#210-feature-introduction" class="hash-link" aria-label="Direct link to 2.1.0 Feature Introduction" title="Direct link to 2.1.0 Feature Introduction">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dashboard-supports-batch-operation-of-agents">Dashboard supports batch operation of agents<a href="#dashboard-supports-batch-operation-of-agents" class="hash-link" aria-label="Direct link to Dashboard supports batch operation of agents" title="Direct link to Dashboard supports batch operation of agents">​</a></h3><p>This feature is mainly used for the operation of Inlong Agent: mainly for upgrading and restarting:</p><ul><li><p>After finding the cluster in cluster management, select multiple nodes to operate on and click on batch operation.</p><p><img loading="lazy" alt="2.1.0-dashboard-select.png" src="/assets/images/2.1.0-dashboard-select-cdf7c75680a97f34117d376cbd02970a.png" width="2483" height="611" class="img_ev3q"></p></li><li><p>Select the operation type and fill in the required parameters for the corresponding operation, then click OK.</p><p><img loading="lazy" alt="2.1.0-dashboard-operate.png" src="/assets/images/2.1.0-dashboard-operate-d51a19561036f39ff2d14377033c95f1.png" width="2412" height="651" class="img_ev3q"></p></li></ul><p>This feature optimizes the operation and maintenance experience of Inlong: interface based operation eliminates the need to operate DB and increases the cohesion of Inlong:</p><ul><li>Visual Agent version upgrade, which can be upgraded in batches and at regular intervals to control upgrade risks.</li><li>During agent fault recovery, this function can be used to quickly restart.</li></ul><p>Thanks to @<a href="https://github.com/wohainilaodou" target="_blank" rel="noopener noreferrer">wohainilaodou</a> for their contributions to this feature. For more details, please refer to <a href="https://github.com/apache/inlong/issues/11187" target="_blank" rel="noopener noreferrer">INLONG-11187</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-supports-multiple-scheduling-engines">Manager supports multiple scheduling engines<a href="#manager-supports-multiple-scheduling-engines" class="hash-link" aria-label="Direct link to Manager supports multiple scheduling engines" title="Direct link to Manager supports multiple scheduling engines">​</a></h3><p>Previously, for offline data synchronization, Inlong only supported Quartz scheduling engine. This version has added two third-party engines: DolphinScheduler and AirFlow.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="airflow-engine">AirFlow engine<a href="#airflow-engine" class="hash-link" aria-label="Direct link to AirFlow engine" title="Direct link to AirFlow engine">​</a></h4><ul><li>In order to facilitate the maintenance and expansion of AirFlow interface support in the future, AirflowApi interface and BaseAirflowApi abstract class have been designed, and subsequent extensions only need to be based on this foundation.</li><li>Implement a unified request class AirflowServerClient for the interface.</li><li>Add two interceptors in OkHttpClient: AirflowAuthInterceptor for unified authorization of interfaces; LoggingInterceptor is used for logging.</li></ul><p>Thanks to @<a href="https://github.com/Zkplo" target="_blank" rel="noopener noreferrer">Zkplo</a> for their contributions to this feature. For more details, please refer to <a href="https://github.com/apache/inlong/issues/11400" target="_blank" rel="noopener noreferrer">INLONG-11400</a></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="dolphinscheduler-engine">DolphinScheduler engine<a href="#dolphinscheduler-engine" class="hash-link" aria-label="Direct link to DolphinScheduler engine" title="Direct link to DolphinScheduler engine">​</a></h4><p>-Add the DolphinScheduler package to org.apache.inlong.manager.schedule
-Add client and engine for DS, as well as util for operating open APIs for DS
-Add pojo class for DS interaction</p><p>Thanks to @<a href="https://github.com/emptyOVO" target="_blank" rel="noopener noreferrer">emptyOVO</a> for their contributions to this feature. For more details, please refer to <a href="https://github.com/apache/inlong/issues/11401" target="_blank" rel="noopener noreferrer">INLONG-11401</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-supports-cos-data-source">Agent supports COS data source<a href="#agent-supports-cos-data-source" class="hash-link" aria-label="Direct link to Agent supports COS data source" title="Direct link to Agent supports COS data source">​</a></h3><ul><li><p>Create a new COS type node and fill in the corresponding bucket name, credential ID, credential key, and region.</p><p><img loading="lazy" alt="2.1.0-agent-node.png" src="/assets/images/2.1.0-agent-node-931c0d784429b1a4f75d4a71b920ef92.png" width="2477" height="808" class="img_ev3q"></p></li><li><p>Create a new COS type data source, select the corresponding node, IP, and file path.</p><p><img loading="lazy" alt="2.1.0-agent-type.png" src="/assets/images/2.1.0-agent-type-4c3e9e13c836c3bd581b8070a02af7e8.png" width="936" height="421" class="img_ev3q"></p><p><img loading="lazy" alt="2.1.0-agent-param.png" src="/assets/images/2.1.0-agent-param-67a9805b60a4f15fed84e048af7a0739.png" width="837" height="744" class="img_ev3q"></p></li></ul><p>This feature supports direct data collection from COS storage, and businesses do not need to download COS files locally for data collection.
Thanks to @<a href="https://github.com/justinwwhuang" target="_blank" rel="noopener noreferrer">justinwwhuang</a> for their contributions to this feature. For more details, please refer to <a href="https://github.com/apache/inlong/issues/11187" target="_blank" rel="noopener noreferrer">INLONG-11187</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-supports-archiving-dirty-data-through-the-inlong-sdk">Sort supports archiving dirty data through the InLong SDK.<a href="#sort-supports-archiving-dirty-data-through-the-inlong-sdk" class="hash-link" aria-label="Direct link to Sort supports archiving dirty data through the InLong SDK." title="Direct link to Sort supports archiving dirty data through the InLong SDK.">​</a></h3><p>Added the ability to report dirty data to specified GroupId and StreamId through the InLong SDK. Users can choose to export dirty data or consume it independently from Pulsar.</p><p><img loading="lazy" alt="2.1.0-sort-dirty.png" src="/assets/images/2.1.0-sort-dirty-b167c4568a5d31d7ffe49701acda5d89.png" width="1500" height="1187" class="img_ev3q"></p><p>The following configuration needs to be added to the Connector:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">'dirty.side-output.inlong-sdk.inlong-auth-key' = 'your auth key',</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">'dirty.side-output.inlong-sdk.inlong-auth-id' = 'your auth id',</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">'dirty.side-output.enable' = 'true',</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">'dirty.side-output.inlong-sdk.inlong-group-id' = 'target_inlong_group_id',</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">'dirty.side-output.inlong-sdk.inlong-stream-id' = 'target_inlong_stream_id',</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">'dirty.side-output.labels' = 'groupId=xx&amp;streamId=xx&amp;serverType=tube&amp;dataflowId=xx',</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">'dirty.side-output.inlong-sdk.inlong-manager-addr' = 'xxx',</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">'dirty.side-output.connector' = 'inlong-sdk',</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">'dirty.ignore' = 'true',`</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Thanks to @<a href="https://github.com/vernedeng" target="_blank" rel="noopener noreferrer">vernedeng</a> and @<a href="https://github.com/fuweng11" target="_blank" rel="noopener noreferrer">fuweng11</a> for their contributions to this feature. For more details,
please refer to <a href="https://github.com/apache/inlong/issues/11481" target="_blank" rel="noopener noreferrer">INLONG-11481</a> and <a href="https://github.com/apache/inlong/issues/11508" target="_blank" rel="noopener noreferrer">INLONG-11508</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="future-plans">Future Plans<a href="#future-plans" class="hash-link" aria-label="Direct link to Future Plans" title="Direct link to Future Plans">​</a></h2><p>In version 2.1.0, we have enriched and improved our operational capabilities. Welcome everyone to use it. If you have more scenarios and requirements,
or encounter any problems during use, please feel free to raise issues and PR. In future versions, the InLong community will continue to:</p><ul><li><p>Support more data source collection capabilities</p></li><li><p>Enrich Flink 1.15, 1.18 Connector</p></li><li><p>Continuously enhance Transform capabilities.</p></li><li><p>Provide real-time synchronization support for more data sources and targets.</p></li><li><p>Optimize SDK capabilities and user experience</p></li><li><p>Optimize Dashboard experience</p></li></ul><p>We also look forward to more developers interested in InLong to contribute and help drive the project's development!</p>]]></content>
        <author>
            <name>justinwwhuang</name>
            <uri>https://github.com/justinwwhuang</uri>
        </author>
        <category label="Apache InLong" term="Apache InLong"/>
        <category label="Version" term="Version"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Release 2.0.0]]></title>
        <id>https://inlong.apache.org/blog/2024/10/20/release-2.0.0</id>
        <link href="https://inlong.apache.org/blog/2024/10/20/release-2.0.0"/>
        <updated>2024-10-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache InLong（应龙）has recently released version 2.0.0, which has closed over 315 issues, including more than 6 major features and over 96 optimizations.]]></summary>
        <content type="html"><![CDATA[<p>Apache InLong（应龙）has recently released version 2.0.0, which has closed over 315 issues, including more than 6 major features and over 96 optimizations.
The main accomplishments include support for the Transform SDK and integrated into the ES Sink of Sort Standalone, OceanBase data source management,
adaptive resource configuration for Sort tasks, HTTP output for SortStandalone, community documentation restructuring, and full-link support for offline synchronization.</p><p>After the 2.0.0 release, Apache InLong has added transform capabilities, improved support for the Agent Pulsar Source, enriched the capabilities and applicable scenarios of Sort,
and optimized the display of the InLong Dashboard, addressing various issues encountered during the operation and maintenance of InLong.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-apache-inlong">About Apache InLong<a href="#about-apache-inlong" class="hash-link" aria-label="Direct link to About Apache InLong" title="Direct link to About Apache InLong">​</a></h2><p>As the industry's first one-stop, all-scenario massive data integration framework, Apache InLong provides automated, secure, reliable,
and high-performance data transmission capabilities, enabling businesses to quickly build stream-based data analysis, modeling, and applications.
Currently, InLong is widely used in various industries including advertising, payment, social networking, gaming, and artificial intelligence,
serving thousands of businesses, with high-performance scenarios processing over hundreds of billions of records per day and highly reliable scenarios
handling over tens of trillions of records per day.</p><p>The core keywords for InLong's project positioning are "one-stop," "all-scenario," and "massive data." For "one-stop,"
we aim to shield technical details, provide complete data integration and supporting services, and achieve out-of-the-box usability;
for "all-scenario," we aim to offer comprehensive solutions covering common data integration scenarios in the big data field;
for "massive data," we hope to leverage architectural advantages such as layered data links, fully extensible components,
and built-in multi-cluster management to stably support even larger data volumes based on hundreds of billions of records per day.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="from-10-to-20">From 1.0 to 2.0<a href="#from-10-to-20" class="hash-link" aria-label="Direct link to From 1.0 to 2.0" title="Direct link to From 1.0 to 2.0">​</a></h2><p>In version 1.13.0, InLong added the underlying framework for offline synchronization tasks and supported Flink's stream-batch integration capability.
Version 2.0.0 fixed several issues with the built-in scheduler, connected the front and back ends, and enabled the configuration of offline
synchronization tasks on the Dashboard page, providing full process support for the configuration and management of offline synchronization tasks.
Based on this capability, users can manage both real-time and offline synchronization tasks uniformly.</p><p>In historical versions, InLong's standard and lightweight architecture focused on data collection, reporting, and data storage in lakes,
with relatively weak support for fine-grained operations on data. Therefore, after version 1.13.0, a very important task for the InLong community
was to enhance support for Transform capabilities, allowing users to perform more flexible processing of data at any stage of data integration.</p><p>Transform is based on general SQL semantics and has now completed the framework for Transform capabilities, supporting over 180 custom Transform functions
while ensuring the extensibility of Transform from a design perspective, allowing users to flexibly define new Transform capabilities.</p><p>In addition to the Transform feature, another focus of the InLong community has been the restructuring of community documentation.
On one hand, we have filled in missing documentation and updated outdated content;
on the other hand, we have reorganized the documentation according to user guidance, core system introduction, development guidance, and system management,
providing better explanations from user, developer, and operational perspectives. With the restructured documentation,
users can more easily utilize InLong and better understand its operational and management mechanisms,
as well as quickly develop custom plugins to meet specific needs.</p><p>Overall, by the time version 2.0.0 was released:</p><ul><li>InLong has achieved full-link support for offline synchronization tasks, possessing stream-batch integrated data processing capabilities.</li><li>InLong has greatly enriched the capabilities of T, laying the foundation for future support of ELT/EtLT Pipelines.</li><li>The optimization of documentation has made InLong more user-friendly, better attracting users to understand, use, and co-build InLong.</li></ul><p>InLong now supports both standard and lightweight architectures, stream-batch integrated data synchronization capabilities, and flexible data Transform capabilities. Therefore, the community has decided to upgrade InLong to version 2.0.0, marking the official entry of Apache InLong into the 2.0 era.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="200-overview">2.0.0 Overview<a href="#200-overview" class="hash-link" aria-label="Direct link to 2.0.0 Overview" title="Direct link to 2.0.0 Overview">​</a></h2><p>Apache InLong has recently released version 2.0.0, which has closed over 315 issues, including more than 6 major features and over 96 optimizations, achieving the following:</p><ul><li>Support for Transform SDK and integrated into the ES Sink of Sort Standalone</li><li>New capabilities for OceanBase data source management</li><li>Adaptive resource configuration for Sort tasks</li><li>HTTP output for Sort Standalone</li><li>Restructured and optimized community documentation</li><li>Stream-batch integration, support for offline synchronization capabilities</li></ul><p>In addition to the above features, version 2.0.0 also:</p><ul><li>Improved support for the Agent Pulsar Source</li><li>Enriched the capabilities and usage scenarios of Sort</li><li>Fixed some issues encountered during InLong operation and maintenance</li><li>Optimized the display and user experience of the Dashboard</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-module">Agent Module<a href="#agent-module" class="hash-link" aria-label="Direct link to Agent Module" title="Direct link to Agent Module">​</a></h3><ul><li>Optimized the Pulsar Source implementation to fix issues with inaccurate consumption offsets.</li><li>Added support for data re-insertion filtering capabilities.</li><li>Introduced the ability to report Agent status.</li><li>Updated the implementations for Redis, Oracle, SQLServer, and MQTT data sources.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dashboard-module">Dashboard Module<a href="#dashboard-module" class="hash-link" aria-label="Direct link to Dashboard Module" title="Direct link to Dashboard Module">​</a></h3><ul><li>Added an offline synchronization configuration page for data synchronization.</li><li>Optimized the style and structure of data preview.</li><li>Introduced a heartbeat display page for cluster node management.</li><li>Added cluster name display to data source information.</li><li>Supported custom ASCII code options for source data field delimiters.</li><li>Merged metric items with other items on the module review page.</li><li>Enabled delete operations for cluster management and template management.</li><li>Fixed errors in data preview.</li><li>Added support for OceanBase data sources.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-module">Manager Module<a href="#manager-module" class="hash-link" aria-label="Direct link to Manager Module" title="Direct link to Manager Module">​</a></h3><ul><li>Added support for OceanBase data source management.</li><li>Introduced TubeMQ configuration capabilities for Sort Standalone.</li><li>Supported asynchronous installation of Agent and the display of Agent installation logs.</li><li>Enabled configuration for HTTP type Sink.</li><li>Supported paginated queries for detailed information on sorting tasks.</li><li>Data preview now supports KV data types, escape characters, and filtering Tube data by StreamId.</li><li>Added data filtering functionality.</li><li>Permission optimization: Regular users are not allowed to modify Group information if they are not the Owner.</li><li>Fixed issues with offline synchronization updates.</li><li>Resolved alignment issues in data preview fields.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sdk-module">SDK Module<a href="#sdk-module" class="hash-link" aria-label="Direct link to SDK Module" title="Direct link to SDK Module">​</a></h3><ul><li>Transform supports data partitioning using GroupBy semantics.</li><li>Transform can parse Map nodes in JSON or PB data.</li><li>Transform JSON data source supports multidimensional arrays.</li><li>Transform supports ELT functionality.</li><li>Transform supports configuration and parsing of Transform annotations.</li><li>Transform supports various data source types: JSON, PB, XML, YAML, BSON, AVRO, ORC, PARQUET, etc.</li><li>Transform supports arithmetic functions: ceil, floor, sin, cos, cot, tanh, cosh, asin, atan, mod, etc.</li><li>Transform supports date and time functions: year, quarter, month, week, from_unixtime, unix_timestamp, to_timestamp, etc.</li><li>Transform supports string functions: substring, replace, reverse, etc.</li><li>Transform supports common encoding and encryption functions: MD5, ASCII, SHA.</li><li>Transform supports numeral and bitwise operation functions: HEX, Bitwise.</li><li>Transform supports compression and decompression functions: GZIP, ZIP, etc.</li><li>Transform includes other common functions: case conversion, IN, NOT IN, EXISTS, etc.</li><li>DataProxy Java SDK: Shaded Native Library to reduce conflicts with other SDKs.</li><li>DataProxy Java SDK: Optimized sending jitter issue during metadata changes.</li><li>DataProxy CPP SDK: Improved memory management and optimized build scripts.</li><li>DataProxy CPP SDK: Supports multiple protocols.</li><li>DataProxy CPP SDK: Added message manager and optimized data reception capabilities.</li><li>DataProxy CPP SDK: Supports forking subprocesses in DataProxy CPP SDK.</li><li>DataProxy Python SDK: Updated build scripts and supports skipping the CPP SDK build step.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-module">Sort Module<a href="#sort-module" class="hash-link" aria-label="Direct link to Sort Module" title="Direct link to Sort Module">​</a></h3><ul><li>Adjust resources required for Sort tasks based on data scale.</li><li>Added support for OceanBase data source.</li><li>Flink 1.18 now supports Elasticsearch 6 and Elasticsearch 7 connectors.</li><li>SortStandalone Elasticsearch Sink supports Transform.</li><li>SortStandalone supports HTTP Sink and batch sorting.</li><li>Connector supports OpenTelemetry log reporting.</li><li>Optimized producer parameters for Kafka connector.</li><li>Added end-to-end test cases for Flink 1.15.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="audit-module">Audit Module<a href="#audit-module" class="hash-link" aria-label="Direct link to Audit Module" title="Direct link to Audit Module">​</a></h3><ul><li>Supports global memory control for the audit SDK.</li><li>Optimized daily dimension audit data statistics.</li><li>Audit SDK allows custom local IP settings.</li><li>Unified audit aggregation interval range.</li><li>Resolved Protobuf version conflicts between Audit SDK and other components.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="200-feature-introduction">2.0.0 Feature Introduction<a href="#200-feature-introduction" class="hash-link" aria-label="Direct link to 2.0.0 Feature Introduction" title="Direct link to 2.0.0 Feature Introduction">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-transform-capabilities">New Transform Capabilities<a href="#new-transform-capabilities" class="hash-link" aria-label="Direct link to New Transform Capabilities" title="Direct link to New Transform Capabilities">​</a></h3><p>InLong Transform enhances InLong's ability to expand access and distribution capabilities by adapting to a wider range of data protocols and reporting scenarios on the input side,
while accommodating complex and diverse data analysis scenarios on the output side. This improves data quality and collaboration, providing computing capabilities such as connection,
aggregation, filtering, grouping, value extraction, and sampling, all decoupled from the computation engine.</p><p>It simplifies the pre-processing operations for users reporting data, lowers the barriers to data usage, and streamlines the pre-operations required before users can start analyzing data.
The focus is on the business value of data, achieving the goal of making data "visible and usable."</p><p><img loading="lazy" alt="2.0.0-transform-background.png" src="/assets/images/2.0.0-transform-background-1e25279b42500875af1934af120596f0.png" width="1186" height="499" class="img_ev3q"></p><p>Transform has a wide range of application scenarios. Here are some typical examples:</p><ul><li>Data Cleaning: During the data integration process, Transform capabilities can effectively eliminate errors, duplicates, and inconsistencies in data from different sources, improving data quality.</li><li>Data Fusion: Combining data from different sources for unified analysis and reporting. Transform capabilities can handle various formats and structures of data, achieving data fusion and integration.</li><li>Data Standardization: Converting data into a unified standard format for cross-system and cross-platform data analysis. Transform capabilities help enterprises achieve data standardization and normalization.</li><li>Data Partitioning and Indexing: To enhance the performance of data queries and analysis, Transform capabilities can dynamically adjust field values for data partitioning and indexing, thereby improving data warehouse performance.</li><li>Data Aggregation and Calculation: In the data analysis process, Transform capabilities can perform complex data aggregation and calculations to extract valuable business information, covering multidimensional data analysis.</li></ul><p>Main Features of Transform：</p><ul><li>Support for Rich Data Protocols: Enables integration with a variety of data protocols.</li><li>Decoupled from Computing Engine: Allows flexibility in processing without being tied to a specific computing engine.</li><li>Support for Rich Transformation Functions: Provides a wide range of functions for data transformation.</li><li>Lossless and Transparent Changes: Ensures that changes can be made without data loss or noticeable impact.</li><li>Automatic Scaling: Supports dynamic scaling up and down based on workload.</li></ul><p>Currently, Transform supports a variety of data formats and custom functions, allowing users to flexibly process data using SQL.
Special thanks to contributors such as @luchunliang, @vernedeng, @emptyOVO, @ying-hua, @Zkplo, @MOONSakura0614, and @Ybszzzziz for their efforts.
For more details, please refer to <a href="https://github.com/aloyszhang/inlong/blob/master/CHANGES.md#sdk" target="_blank" rel="noopener noreferrer">Transform SDK Issues</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="community-documentation-restructuring">Community Documentation Restructuring<a href="#community-documentation-restructuring" class="hash-link" aria-label="Direct link to Community Documentation Restructuring" title="Direct link to Community Documentation Restructuring">​</a></h3><p>With the continuous development of the InLong community, the capabilities of InLong are also constantly enhancing. However, there have been issues with missing or outdated community documentation.
To address this, the InLong community has initiated a restructuring of the community documentation to better assist users in understanding and utilizing InLong.</p><p>Main Content Includes:</p><ul><li>Optimized Document Structure: Better organization of document content. </li><li>Enhanced Quick Start Examples:<ul><li>Offline synchronization usage examples</li><li>Transform SDK usage examples</li><li>Data subscription usage examples</li><li>HTTP message reporting usage examples</li></ul></li><li>Improved SDK Documentation:<ul><li>DataProxy: C++, Java, Golang, Python SDKs, and HTTP data reporting manuals</li><li>TubeMQ SDK: C++, Java, Golang SDK usage manuals</li></ul></li><li>Enhanced Development Guidelines:<ul><li>Code compilation guidelines</li><li>Documentation for data protocols of each component</li><li>Documentation for extension development of each component</li><li>REST API documentation</li></ul></li><li>Improved Management Articles: Documentation on user management, approval management, tenant management, node management, cluster management, tag management, template management, and agent management.</li></ul><p>Currently, the community documentation has seen significant improvements in usage guidelines, development guidelines, and management guidelines.
Special thanks to contributors such as @aloyszhang, @fuweng11, @vernedeng, @luchunliang, @gosonzhang, @doleyzi, @baomingyu, @justinwwhuang, and @wohainilaodou for their contributions to the documentation improvement.
For more details, please refer to the <a href="https://inlong.apache.org/docs/next/introduction" target="_blank" rel="noopener noreferrer">official website</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-for-oceanbase-data-source">Support for OceanBase Data Source<a href="#support-for-oceanbase-data-source" class="hash-link" aria-label="Direct link to Support for OceanBase Data Source" title="Direct link to Support for OceanBase Data Source">​</a></h3><p>OceanBase Database is a distributed relational database characterized by high availability and scalability, suitable for large-scale data storage and processing scenarios. InLong version 2.0.0 adds support for OceanBase data sources, allowing data to be imported from data sources into OceanBase.</p><p>Managing OceanBase data nodes is similar to MySQL. To create a new OceanBase node, you need to fill in the node name, type (OceanBase), username, password, address, and other key information.</p><p><img loading="lazy" alt="2.0.0-oceanbase-detail.png" src="/assets/images/2.0.0-oceanbase-detail-b71f51f236b69bd10cff6f4b17e3394f.png" width="712" height="578" class="img_ev3q"></p><p>To write data into OceanBase, you first need to create a data target of type <code>OceanBase</code>.</p><p><img loading="lazy" alt="2.0.0-oceanbase-type.png" src="/assets/images/2.0.0-oceanbase-type-2445117e4506d1743dc9c16697996848.png" width="1251" height="317" class="img_ev3q"></p><p>Then fill in the relevant information, including the name, data node information, database and table names of the data target, and the primary key information of the target table.</p><p><img loading="lazy" alt="2.0.0-oceanbase-target.png" src="/assets/images/2.0.0-oceanbase-target-6a654152f4aa976322ed8968c3c3b5bb.png" width="1250" height="438" class="img_ev3q"></p><p>Thanks to @xxsc0529 for their contributions to this feature. For more details, please refer to <a href="https://github.com/apache/inlong/pull/10700" target="_blank" rel="noopener noreferrer">INLONG-10700</a>, <a href="https://github.com/apache/inlong/pull/10701" target="_blank" rel="noopener noreferrer">INLONG-10701</a>, <a href="https://github.com/apache/inlong/pull/10704" target="_blank" rel="noopener noreferrer">INLONG-10704</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dynamic-resource-calculation-for-sort-tasks">Dynamic Resource Calculation for Sort Tasks<a href="#dynamic-resource-calculation-for-sort-tasks" class="hash-link" aria-label="Direct link to Dynamic Resource Calculation for Sort Tasks" title="Direct link to Dynamic Resource Calculation for Sort Tasks">​</a></h3><p>The total resources (task parallelism) for Flink Sort Jobs come from the configuration file <code>flink-sort-plugin.properties</code>, meaning that all submitted sorting jobs will use the same amount of resources. When the data scale is large, resources may be insufficient, and when the data scale is small, resources may be wasted.</p><p>Therefore, dynamically calculating the required resource quantity based on data volume is a much-needed feature. InLong now supports dynamically calculating the total resource requirements for tasks based on data volume, involving two core pieces of data:</p><ul><li>The data volume of the task: This relies on the audit system and is derived from the average data volume recorded by <code>DataProxy</code> over the past hour.</li><li>The processing capacity per core: This depends on the maximum number of messages per core configured in the <code>flink-sort-plugin.properties</code> file.</li></ul><p>With these two pieces of data, the total resource requirements for a task can be calculated. This feature supports a switch to enable or disable it as needed.</p><p>Thanks to @PeterZh6 for their contributions to this feature. For more details, please refer to <a href="https://github.com/apache/inlong/pull/10916" target="_blank" rel="noopener noreferrer">INLONG-10916</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sortstandalone-supports-http-sink">SortStandalone Supports HTTP Sink<a href="#sortstandalone-supports-http-sink" class="hash-link" aria-label="Direct link to SortStandalone Supports HTTP Sink" title="Direct link to SortStandalone Supports HTTP Sink">​</a></h3><p>Inlong SortStandalone is responsible for consuming data from MQ and distributing it to various data storage modules, supporting multiple data stores such as ElasticSearch and CLS.</p><p>Compared to SortFlink, SortStandalone offers higher performance and lower latency, making it suitable for scenarios with high performance requirements.</p><p>HTTP is a widely used communication protocol, and SortStandalone supports HTTP output, allowing data to be sent to HTTP interfaces without worrying about specific storage implementations, thus adapting more flexibly to different business scenarios.</p><p>The processing flow for HTTP output is as follows:</p><p><img loading="lazy" alt="2.0.0-sortstandalone-http.png" src="/assets/images/2.0.0-sortstandalone-http-ac60f52aecb648d113acf8c77190c28c.png" width="1600" height="951" class="img_ev3q"></p><p>HTTP output has the following features:</p><ul><li>SortSDK is responsible for consuming data from MQ</li><li>Supports semaphore-based traffic control capabilities</li><li>Metadata management relies on Manager, supporting dynamic updates</li><li>The output protocol is HTTP, decoupling specific storage implementations</li><li>Supports retry strategies</li></ul><p>Thanks to @yfsn666 and @fuweng11 for their contributions to this feature. For more details, please refer to <a href="https://github.com/apache/inlong/pull/10831" target="_blank" rel="noopener noreferrer">INLONG-10831</a> and <a href="https://github.com/apache/inlong/pull/10884" target="_blank" rel="noopener noreferrer">INLONG-10884</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="full-link-management-of-offline-synchronization">Full-Link Management of Offline Synchronization<a href="#full-link-management-of-offline-synchronization" class="hash-link" aria-label="Direct link to Full-Link Management of Offline Synchronization" title="Direct link to Full-Link Management of Offline Synchronization">​</a></h3><p>Version 2.0.0 adds full-link management capabilities for offline data synchronization tasks, with configuration methods for offline synchronization tasks similar to real-time synchronization. The specific process is as follows:</p><p>First, create the Group information for the synchronization task.</p><p><img loading="lazy" alt="2.0.0-offline-sync-group.png" src="/assets/images/2.0.0-offline-sync-group-4cdb02e5fe45f6ff6d5b9bc11e98deed.png" width="824" height="422" class="img_ev3q"></p><p>Note that the "Sync Type" should be selected as "Offline."</p><p>The second step is to configure the scheduling information for the offline task.</p><p><img loading="lazy" alt="2.0.0-offline-schedule-common.png" src="/assets/images/2.0.0-offline-schedule-common-02c7267d3c2271809eea6e4bda897d5c.png" width="813" height="333" class="img_ev3q"></p><p>The conventional scheduling configuration requires setting the following parameters:</p><ul><li>Scheduling unit: Supports minute, hour, day, month, year, and single execution (single execution means it will only run once)</li><li>Scheduling cycle: Represents the time interval between two task schedules</li><li>Delay time: Represents the delay time for task startup</li><li>Valid time: Includes start and end times; scheduled tasks will only run within this time range</li></ul><p>In addition to conventional scheduling methods, Crontab configuration is also supported.</p><p><img loading="lazy" alt="2.0.0-offline-schedule-crontab.png" src="/assets/images/2.0.0-offline-schedule-crontab-6609c746fb04fdbd7cd555f2039d394e.png" width="812" height="244" class="img_ev3q"></p><p>Crontab scheduling requires setting the following parameters:</p><ul><li>Valid time: Includes start and end times; scheduled tasks will only run within this time range</li><li>Crontab expression: Represents the task cycle, for example, <code>0 */5 * * * ?</code></li></ul><p>The third step is to create a Stream and configure data source and target information, which is consistent with real-time synchronization and will not be repeated here.
For more details, please refer to <a href="https://inlong.apache.org/docs/next/quick_start/offline_data_sync/pulsar_mysql_example" target="_blank" rel="noopener noreferrer">Offline Synchronization Pulsar-&gt;MySQL</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="summary-and-future-plans">Summary and Future Plans<a href="#summary-and-future-plans" class="hash-link" aria-label="Direct link to Summary and Future Plans" title="Direct link to Summary and Future Plans">​</a></h2><p>Version 2.0.0 is the first version of the 2.x series, and the technical capability framework has been basically established. We welcome everyone to use it.
If you have more scenarios and requirements, or encounter issues during use, please feel free to raise issues and PRs.
In future versions, the InLong community will continue to:</p><ul><li>Support more data source collection capabilities.</li><li>Enrich Flink 1.15 and 1.18 connectors.</li><li>Continuously enhance Transform capabilities.</li><li>Provide real-time synchronization support for more data sources and targets.</li><li>Advance offline integration, supporting third-party scheduling engines.</li><li>Optimize SDK capabilities and user experience.</li><li>Improve Dashboard experience.</li></ul><p>We also look forward to more developers interested in InLong to contribute and help drive the project's development!</p>]]></content>
        <author>
            <name>Aloys Zhang</name>
            <uri>https://github.com/aloyszhang</uri>
        </author>
        <category label="Apache InLong" term="Apache InLong"/>
        <category label="Version" term="Version"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Release 1.13.0]]></title>
        <id>https://inlong.apache.org/blog/2024/07/18/release-1.13.0</id>
        <link href="https://inlong.apache.org/blog/2024/07/18/release-1.13.0"/>
        <updated>2024-07-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache InLong recently released version 1.13.0, which closed about 275+ issues, including 6+ major features and 100+ optimizations. The main features include supporting SSH installation of agents, managing field templates, support configuring offline synchronization tasks, and collecting PostgreSQL through agents]]></summary>
        <content type="html"><![CDATA[<p>Apache InLong recently released version 1.13.0, which closed about 275+ issues, including 6+ major features and 100+ optimizations. The main features include supporting SSH installation of agents, managing field templates, support configuring offline synchronization tasks, and collecting PostgreSQL through agents
. After the release of 1.13.0, Apache InLong has enriched and optimized Agent function scenarios, enhanced the accuracy of Audit data measurement, and enriched the capabilities and applicable scenarios of Sort, solved the demand for quick troubleshooting in development and operation, and optimized the user experience of Apache InLong operation and maintenance.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-apache-inlong">About Apache InLong<a href="#about-apache-inlong" class="hash-link" aria-label="Direct link to About Apache InLong" title="Direct link to About Apache InLong">​</a></h2><p>As the industry's first one-stop, full-scenario, open-source massive data integration framework, Apache InLong provides automatic, safe, reliable, and high-performance data transmission capabilities to facilitate businesses to build stream-based data analysis, modeling, and applications quickly. At present, InLong is widely used in various industries such as advertising, payment, social networking, games, artificial intelligence, etc., serving thousands of businesses, among which the scale of high-performance scene data exceeds 1 trillion lines per day, and the scale of high-reliability scene data exceeds 10 trillion lines per day.</p><p>The core keywords of InLong project positioning are "one-stop" and "massive data". For "one-stop", we hope to shield technical details, provide complete data integration and support services, and implement out-of-the-box; With its advantages, such as multi-cluster management, it can stably support larger-scale data volumes based on trillions of lines per day.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="1130-version-overview">1.13.0 Version Overview<a href="#1130-version-overview" class="hash-link" aria-label="Direct link to 1.13.0 Version Overview" title="Direct link to 1.13.0 Version Overview">​</a></h2><p>Apache InLong recently released version 1.13.0, which closed about 275+ issues, including 6+ major features and 100+ optimizations. The main features include supporting SSH installation of agents, managing field templates, support configuring offline synchronization tasks, and collecting PostgreSQL through agents
. After the release of 1.13.0, Apache InLong has enriched and optimized Agent function scenarios, enhanced the accuracy of Audit data measurement, and enriched the capabilities and applicable scenarios of Sort, solved the demand for quick troubleshooting in development and operation, and optimized the user experience of Apache InLong operation and maintenance. In Apache InLong 1.13.0 version, a large number of other features have also been completed, mainly including:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-module">Agent Module<a href="#agent-module" class="hash-link" aria-label="Direct link to Agent Module" title="Direct link to Agent Module">​</a></h3><ul><li>Support data version numbers to distinguish between normal data and supplementary data</li><li>Location storage supports plugins, currently supporting Rocksdb and Zookeeper</li><li>Support configuration version number comparison to prevent repeated configuration</li><li>Support minute level file collection</li><li>Add PostgreSQL and MongoDB data source collection</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-module">Manager Module<a href="#manager-module" class="hash-link" aria-label="Direct link to Manager Module" title="Direct link to Manager Module">​</a></h3><ul><li>Support installing agents through SSH</li><li>Switch audit ID query from direct interaction with database to Audit SDK</li><li>Offline synchronization supports Pulsar -&gt; MySQL</li><li>Support offline synchronous scheduling information management</li><li>File collection supports multi IP collection</li><li>Support obtaining Agent configuration information</li><li>Support automatic synchronization to Sink after modifying Stream field information</li><li>Support field template management</li><li>Data preview supports KV format</li><li>Data preview supports querying based on field filtering criteria</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dashboard-module">Dashboard Module<a href="#dashboard-module" class="hash-link" aria-label="Direct link to Dashboard Module" title="Direct link to Dashboard Module">​</a></h3><ul><li>Add Source Data Field Template Page</li><li>Add monitoring and auditing page</li><li>Support installing agents through SSH key based authentication</li><li>Audit supports displaying total and variance audit data</li><li>File type data stream supports minute level cycles</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="audit-module">Audit Module<a href="#audit-module" class="hash-link" aria-label="Direct link to Audit Module" title="Direct link to Audit Module">​</a></h3><ul><li>Unified allocation and management of audit items using the Audit SDK</li><li>The Audit SDK supports automatic management of Audit Proxy addresses</li><li>Audit Store supports the universal JDBC protocol</li><li>Restarting the Audit Store optimization process may lead to data loss issues</li><li>Audit Service optimizes thread pool management</li><li>Audit Service compatible with historical audit data with empty Audit Tag</li><li>Audit Service Optimization for OpenAPI Audit Transmission Delay Calculation</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-module">Sort Module<a href="#sort-module" class="hash-link" aria-label="Direct link to Sort Module" title="Direct link to Sort Module">​</a></h3><ul><li>Add JDBC connector on Flink 1.15</li><li>Added Pulsar connector on Flink 1.18</li><li>Redis connector supports reporting audit information </li><li>Kafka connector supports reporting audit information</li><li>MongoDB connector supports reporting audit information</li><li>PostgreSQL connector supports reporting audit information</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="1130-version-feature-introduction">1.13.0 Version Feature Introduction<a href="#1130-version-feature-introduction" class="hash-link" aria-label="Direct link to 1.13.0 Version Feature Introduction" title="Direct link to 1.13.0 Version Feature Introduction">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-supports-installing-agent-by-ssh">Manager supports installing Agent by SSH<a href="#manager-supports-installing-agent-by-ssh" class="hash-link" aria-label="Direct link to Manager supports installing Agent by SSH" title="Direct link to Manager supports installing Agent by SSH">​</a></h3><p>Through this feature, operation and maintenance personnel can install agents through the Dashboard, which currently supports SSH and manual installation methods. Users can create a new Agent cluster on the cluster management page.</p><p><img loading="lazy" alt="1.13.0-agent-cluster.png" src="/assets/images/1.13.0-agent-cluster-83acaa86fa0b9673b8552387748c0f99.png" width="1004" height="680" class="img_ev3q"></p><p>Afterwards, enter the node, select the new node, and configure the SSH username and password to achieve SSH installation agent capability.
Thanks to @haifxu and @fuweng11. For more information, please refer to INLONG-10409.</p><p><img loading="lazy" alt="1.13.0-agent-install.png" src="/assets/images/1.13.0-agent-install-1422187b12da432a4ad5be185d8dab34.png" width="998" height="1310" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-supports-field-template-management">Manager supports field template management<a href="#manager-supports-field-template-management" class="hash-link" aria-label="Direct link to Manager supports field template management" title="Direct link to Manager supports field template management">​</a></h3><p>With this feature, users can pre-configure field templates, and when creating a new Stream, they can select the pre-configured field templates, thus achieving the purpose of reusing configurations across multiple Streams.
Thanks to @kamianlaida and @fuweng11. For more information, please refer to: INLONG-10330.</p><p><img loading="lazy" alt="1.13.0-create-template.png" src="/assets/images/1.13.0-create-template-cb5a5dd7e4c1b0f5cffb32a4b6bbf490.png" width="1962" height="976" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-the-construction-of-the-underlying-framework-for-configuring-offline-synchronization-tasks">Support the construction of the underlying framework for configuring offline synchronization tasks<a href="#support-the-construction-of-the-underlying-framework-for-configuring-offline-synchronization-tasks" class="hash-link" aria-label="Direct link to Support the construction of the underlying framework for configuring offline synchronization tasks" title="Direct link to Support the construction of the underlying framework for configuring offline synchronization tasks">​</a></h3><p>In version 1.13.0, Manager supports the configuration of offline synchronization tasks. Compared to real-time synchronization, offline data synchronization pays more attention to synchronization throughput and efficiency.
Real-time synchronization tasks run in the manner of Flink stream tasks, while offline synchronization runs in the manner of Flink batch tasks. This approach can ensure the consistency of real-time and offline synchronization tasks' code as much as possible, reducing maintenance costs.
The offline synchronization function of InLong will be combined with the scheduling system to synchronize the complete or incremental data of the data source information to the data target. The offline synchronization task is created by InLong Manager (including scheduling information), and the specific data synchronization logic is implemented through the InLong Sort module.</p><p><img loading="lazy" alt="1.13.0-offline-architecture.png" src="/assets/images/1.13.0-offline-architecture-eef859714bb87205a461252ee499c44a.png" width="778" height="676" class="img_ev3q"></p><p>Key Competency:</p><ul><li>Job Type: Support single or periodic offline data synchronization.</li><li>Scheduling:The scheduling function is plugin based, with a built-in simple periodic scheduling function that allows customization of third-party scheduling systems to support more complex capabilities, such as task dependencies.</li><li>Compute Engine: Flink</li><li>Offline Job Operation and Maintenance: Job start,stop and running status monitoring</li><li>Special Handling: Dirty Data Processing Capability</li></ul><p>The following is the core process:</p><p><img loading="lazy" alt="1.13.0-dataflow-architecture.png" src="/assets/images/1.13.0-dataflow-architecture-6750793161a566bf2fca9f938acea1a9.png" width="971" height="1360" class="img_ev3q"></p><p>Thanks to @aloyszhang. For more information, please refer to: INLONG-10054, INLONG-10053, INLONG-10055, INLONG-10069.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimize-the-sort-standalone-configuration-process">Optimize the Sort Standalone configuration process<a href="#optimize-the-sort-standalone-configuration-process" class="hash-link" aria-label="Direct link to Optimize the Sort Standalone configuration process" title="Direct link to Optimize the Sort Standalone configuration process">​</a></h3><p>In version 1.13.0, the distribution process of Sort Standalone configuration was modified. In previous versions, there were the following issues with the distribution of Sort Standalone configuration:</p><ul><li>Configuration changes are unreliable. Configuration changes will be updated in real-time to the manager's cache. Once the configuration is changed, Sort Standalone will sense it and write data based on the new configuration without verifying the configuration.</li><li>The configuration and construction process is repetitive and cumbersome. When pulling the Sort configuration from the database, it is a full pull and real-time build is performed after the pull is complete.</li></ul><p>In the new version, after modifying the data target, the Sort configuration will not take effect in real time, but will need to be built and written into the sort_config table after executing the workflow. The following figure shows a process comparison:</p><p><img loading="lazy" alt="1.13.0-manager-standalone.png" src="/assets/images/1.13.0-manager-standalone-9462c43bdaf5cb43dabcc5f1c2a860a8.png" width="990" height="731" class="img_ev3q"></p><p>Thanks to @fuweng11, @vernedeng. For more information, please refer to: INLONG-9867, INLONG-10017.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-ability-for-collecting-data-from-postgresql">Agent ability for collecting data from PostgreSQL<a href="#agent-ability-for-collecting-data-from-postgresql" class="hash-link" aria-label="Direct link to Agent ability for collecting data from PostgreSQL" title="Direct link to Agent ability for collecting data from PostgreSQL">​</a></h3><p>In version 1.13.0, Agent supports data collection from PostgreSQL. When creating a data source, you can directly select PostgreSQL and fill all relevant data source information to start using it. The parameters include:</p><ul><li>Data source name: Used to distinguish between different data sources</li><li>Cluster name: The cluster of data source belongs</li><li>Data source IP: The IP information of data source</li><li>Server Host: PostgreSQL Server Host </li><li>Port: PostgreSQL port</li></ul><p>Thanks to @haifxu. For more information, please refer to: INLONG-10318.</p><p><img loading="lazy" alt="1.13.0-agent-postgreSQL.png" src="/assets/images/1.13.0-agent-postgreSQL-7758b22cfa5ebb3f930dfac1b283bed3.png" width="1532" height="1766" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-connectors-reporting-audit-information-supports-exactly-once">Sort connectors reporting audit information supports exactly once<a href="#sort-connectors-reporting-audit-information-supports-exactly-once" class="hash-link" aria-label="Direct link to Sort connectors reporting audit information supports exactly once" title="Direct link to Sort connectors reporting audit information supports exactly once">​</a></h3><p>Sort Connectors support for exactly once semantics in reporting audit information.
When an operator encounters an exception and the snapshot fails, each Connector ensures that the audit information is reported exactly once.
As shown in the following diagram, when data is transferred, the current data will be written to the current operator's AuditBuffer and the corresponding checkpointId. When the checkpoint is completed (within the notifyCompleteCheckpoint method), the data written to the buffer will be reported.</p><p><img loading="lazy" alt="1.13.0-sort-exactly.png" src="/assets/images/1.13.0-sort-exactly-36c81aac716070dd4f670bb47313edb8.png" width="1302" height="563" class="img_ev3q"></p><p>During the execution of snapShot, the checkpointId written to the buffer will be changed to the checkpointId in the snapShot, and when the checkpoint is completed,
the audit information corresponding to the checkpointId will be uploaded to the Audit Server within the notifyCompleteCheckpoint method. In the Audit SDK,
it is already supportedto report all information in the Audit buffer that is less than the specified checkpointId.</p><p><img loading="lazy" alt="1.13.0-sort-audit.png" src="/assets/images/1.13.0-sort-audit-c42f5f6914ef55f013bad4648473c966.png" width="1264" height="772" class="img_ev3q"></p><p>Thanks to @XiaoYou201. For more information, please refer to: INLONG-10311, INLONG-10312, INLONG-10317, INLONG-10355, INLONG-10357, INLONG-10358, INLONG-10401.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="future-plans">Future plans<a href="#future-plans" class="hash-link" aria-label="Direct link to Future plans" title="Direct link to Future plans">​</a></h2><p>In version 1.13.0, the community refactored the process of Manager issuing Agent, Sort tasks, enriched features such as Flink 1.15 Connector, Inlong Agent collecting PostgreSQL and other functions. In subsequent versions, InLong will continue to enrich Flink 1.15 Connector, enhance Transform capabilities, support offline data integration, unify DataProxy data protocols, optimize Dashboard experience, etc. We look forward to more developers participating and contributing.</p>]]></content>
        <author>
            <name>Wenkai Fu</name>
            <uri>https://github.com/fuweng11</uri>
        </author>
        <category label="Apache InLong" term="Apache InLong"/>
        <category label="Version" term="Version"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Release 1.12.0]]></title>
        <id>https://inlong.apache.org/blog/2024/04/21/release-1.12.0</id>
        <link href="https://inlong.apache.org/blog/2024/04/21/release-1.12.0"/>
        <updated>2024-04-21T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache InLong recently released version 1.12.0, which closed about 140+ issues, including 7+ major features and 90+ optimizations. The main features include Manager supports for agent install package management and it's self-upgrading processe, Agent ability for self-upgrading process, Agent ability for collecting data from Kafka、Pulsar and MongoDB, Support for Redis connector in Sort module, Optimization for Audit and enhancement of its capabilities]]></summary>
        <content type="html"><![CDATA[<p>Apache InLong recently released version 1.12.0, which closed about 140+ issues, including 7+ major features and 90+ optimizations. The main features include Manager supports for agent install package management and it's self-upgrading processe, Agent ability for self-upgrading process, Agent ability for collecting data from Kafka、Pulsar and MongoDB, Support for Redis connector in Sort module, Optimization for Audit and enhancement of its capabilities
. After the release of 1.12.0, Apache InLong has enriched and optimized Agent function scenarios, enhanced the accuracy of Audit data measurement, and enriched the capabilities and applicable scenarios of Sort, solved the demand for quick troubleshooting in development and operation, and optimized the user experience of Apache InLong operation and maintenance.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-apache-inlong">About Apache InLong<a href="#about-apache-inlong" class="hash-link" aria-label="Direct link to About Apache InLong" title="Direct link to About Apache InLong">​</a></h2><p>As the industry's first one-stop, full-scenario, open-source massive data integration framework, Apache InLong provides automatic, safe, reliable, and high-performance data transmission capabilities to facilitate businesses to build stream-based data analysis, modeling, and applications quickly. At present, InLong is widely used in various industries such as advertising, payment, social networking, games, artificial intelligence, etc., serving thousands of businesses, among which the scale of high-performance scene data exceeds 1 trillion lines per day, and the scale of high-reliability scene data exceeds 10 trillion lines per day.</p><p>The core keywords of InLong project positioning are "one-stop" and "massive data". For "one-stop", we hope to shield technical details, provide complete data integration and support services, and implement out-of-the-box; With its advantages, such as multi-cluster management, it can stably support larger-scale data volumes based on trillions of lines per day.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="1120-version-overview">1.12.0 Version Overview<a href="#1120-version-overview" class="hash-link" aria-label="Direct link to 1.12.0 Version Overview" title="Direct link to 1.12.0 Version Overview">​</a></h2><p>Apache InLong recently released version 1.12.0, which closed about 140+ issues, including 7+ major features and 90+ optimizations. The main features include Manager supports for agent install package management and it's self-upgrading processe, Agent ability for self-upgrading process, Agent ability for collecting data from Kafka、Pulsar and MongoDB, Support for Redis connector in Sort module, Optimization for Audit and enhancement of its capabilities
. After the release of 1.12.0, Apache InLong has enriched and optimized Agent function scenarios, enhanced the accuracy of Audit data measurement, and enriched the capabilities and applicable scenarios of Sort, solved the demand for quick troubleshooting in development and operation, and optimized the user experience of Apache InLong operation and maintenance. In Apache InLong 1.12.0 version, a large number of other features have also been completed, mainly including:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-module">Agent Module<a href="#agent-module" class="hash-link" aria-label="Direct link to Agent Module" title="Direct link to Agent Module">​</a></h3><ul><li>Agent ability for self-upgrading process</li><li>Optimize initialization logic to reduce IO usage</li><li>Optimize message acknowledgment logic to reduce semaphore competition</li><li>Increase auditing for sending exceptions and resending</li><li>Optimize message recovery logic to avoid data loss caused by too many supplementary files</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-module">Manager Module<a href="#manager-module" class="hash-link" aria-label="Direct link to Manager Module" title="Direct link to Manager Module">​</a></h3><ul><li>Add an agent installer module management for agent installation</li><li>Support parsing specific field information based on data types such as CSV while previewing data</li><li>Supports pulsar multi cluster while previewing data</li><li>Supports returning header and specific field information while previewing data</li><li>Support adding data and tasks for file collection</li><li>Audit data query switches from jdbc to Audit OpenAPI.</li><li>Support to set compression type in Pulsar DataNode </li><li>Provide OpenAPI for batch saving InLongGroup, InLongStream, and other information.</li><li>Support to config Kafka data node</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dashboard-module">Dashboard Module<a href="#dashboard-module" class="hash-link" aria-label="Direct link to Dashboard Module" title="Direct link to Dashboard Module">​</a></h3><ul><li>Optimize audit data query</li><li>Optimize audit data display</li><li>Support for underscore "_" in Sink field mapping</li><li>Support paginated display of resource details</li><li>Support MongoDB data source configuration</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="audit-module">Audit Module<a href="#audit-module" class="hash-link" aria-label="Direct link to Audit Module" title="Direct link to Audit Module">​</a></h3><ul><li>Support user-defined ways to obtain Audit proxy information</li><li>Audit SDK supports reporting version numbers</li><li>Audit SDK supports both singleton and non-singleton usage</li><li>Audit SDK supports data reporting in Flink Checkpoint feature</li><li>Audit Service supports HA (High Availability) capabilities</li><li>Audit Service supports local caching and OpenAPI</li><li>Audit Service supports multi-datasource</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-module">Sort Module<a href="#sort-module" class="hash-link" aria-label="Direct link to Sort Module" title="Direct link to Sort Module">​</a></h3><ul><li>Supports using state key during StarRocks connector sinitialization</li><li>Supports parsing KV and CSV data containing split symbols</li><li>Using ZLIB as the default compression type for Pulsar Sink</li><li>Pulsar Connector supports authentication configuration</li><li>Pulsar Sink supports authentication configuration</li><li>Redis Source supports String, Hash, and ZSet data types</li><li>Redis Sink supports Bitmap, Hash, and String data types</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="1120-version-feature-introduction">1.12.0 Version Feature Introduction<a href="#1120-version-feature-introduction" class="hash-link" aria-label="Direct link to 1.12.0 Version Feature Introduction" title="Direct link to 1.12.0 Version Feature Introduction">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-supports-for-agent-install-package-management-and-its-self-upgrading-processe">Manager supports for agent install package management and it's self-upgrading processe<a href="#manager-supports-for-agent-install-package-management-and-its-self-upgrading-processe" class="hash-link" aria-label="Direct link to Manager supports for agent install package management and it's self-upgrading processe" title="Direct link to Manager supports for agent install package management and it's self-upgrading processe">​</a></h3><p>In version 1.12.0, Operator can manage Agent installation packages through the Dashboard, including Agent installation, upgrade, heartbeat management, etc. Users can create/manage installation packages on the System Operation -&gt; Installation Packages -&gt; Agent page. Thanks to @haifxu and @fuweng11. For more information, please refer to: INLONG-9932.
<img loading="lazy" alt="1.12.0-agent-package.png" src="/assets/images/1.12.0-agent-package-f34367169ace4692b95248f63559ac3d.png" width="2508" height="1008" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-ability-for-self-upgrading-process">Agent ability for self-upgrading process<a href="#agent-ability-for-self-upgrading-process" class="hash-link" aria-label="Direct link to Agent ability for self-upgrading process" title="Direct link to Agent ability for self-upgrading process">​</a></h3><p>Agents can perform self-upgrades through a pre-deployed Installer.The Installer will obtain the upgrade configuration information from InLong Manager via IP and determine whether to proceed with the upgrade based on the configuration. The main process includes:</p><ul><li>ADD: Download the installation package -&gt; Unzip the installation package -&gt; Start the process</li><li>DELETE: Stop the process -&gt; Delete the installation files</li><li>UPDATE: Download the installation package -&gt; Stop the process -&gt; Delete the installation files -&gt; Unzip the installation package -&gt; Start the process
Thanks to @justinwwhuang. For more information, please refer to: INLONG-9801.
<img loading="lazy" alt="1.12.0-agent-upgrade.png" src="/assets/images/1.12.0-agent-upgrade-9f282143b8a7c7b56778bba76bf713f2.png" width="1500" height="834" class="img_ev3q"></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-ability-for-collecting-data-from-kafka">Agent ability for collecting data from Kafka<a href="#agent-ability-for-collecting-data-from-kafka" class="hash-link" aria-label="Direct link to Agent ability for collecting data from Kafka" title="Direct link to Agent ability for collecting data from Kafka">​</a></h3><p>In version 1.12.0, Agent supports data collection from Kafka. When creating a data source, you can directly select Kafka and fill all relevant data source information to start using it. The parameters include:</p><ul><li>Data source name: Used to distinguish between different data sources</li><li>Cluster name: The cluster of data source belongs</li><li>Data source IP: The IP information of data source</li><li>Bootstrap Servers: Kafka cluster address</li><li>Pulsar namespace: Pulsar namespace</li><li>Kafka topic: Kafka topic</li><li>Automatic offset reset: Set offset strategy</li><li>Partition offset: Set specific partition offset
Thanks to @justinwwhuang. For more information, please refer to: INLONG-9741.
<img loading="lazy" alt="1.12.0-agent-kafka.png" src="/assets/images/1.12.0-agent-kafka-b52f733af5618c5b5ef424239621e24f.png" width="1518" height="1202" class="img_ev3q"></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-ability-for-collecting-data-from-pulsar">Agent ability for collecting data from Pulsar<a href="#agent-ability-for-collecting-data-from-pulsar" class="hash-link" aria-label="Direct link to Agent ability for collecting data from Pulsar" title="Direct link to Agent ability for collecting data from Pulsar">​</a></h3><p>In version 1.12.0, Agent supports data collection from Pulsar. When creating a data source, you can directly select Pulsar and fill all relevant data source information to start using it. The parameters include:</p><ul><li>Data source name: Used to distinguish between different data sources</li><li>Cluster name: The cluster of data source belongs</li><li>Data source IP: The IP information of data source</li><li>Pulsar tenant: Pulsar tenant</li><li>Pulsar namespace: Pulsar namespace</li><li>Pulsar topic: Pulsar topic</li><li>Pulsar admin url: Pulsar admin url</li><li>Pulsar service url: Pulsar service url
Thanks to @justinwwhuang. For more information, please refer to: INLONG-9804.
<img loading="lazy" alt="1.12.0-agent-pulsar.png" src="/assets/images/1.12.0-agent-pulsar-ff0dba9ac904dca3c9d803fef533ab8f.png" width="1530" height="1882" class="img_ev3q"></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-ability-for-collecting-data-from-mongodb">Agent ability for collecting data from MongoDB<a href="#agent-ability-for-collecting-data-from-mongodb" class="hash-link" aria-label="Direct link to Agent ability for collecting data from MongoDB" title="Direct link to Agent ability for collecting data from MongoDB">​</a></h3><p>In version 1.12.0, Agent supports data collection from MongoDB. When creating a data source, you can directly select MongoDB and fill all relevant data source information to start using it. The parameters include:</p><ul><li>Data source name: Used to distinguish between different data sources</li><li>Cluster name: The cluster of data source belongs</li><li>Data source IP: The IP information of data source</li><li>Server host: MongoDB address</li><li>Username: MongoDB username</li><li>Password: MongoDB password</li><li>Database name: MongoDB database name</li><li>Collection name: MongoDB collection name</li><li>Read mode: Optional "Full + Incremental" or "Incremental"
Thanks to @justinwwhuang. For more information, please refer to: INLONG-10006。.
<img loading="lazy" alt="1.12.0-agent-mongodb.png" src="/assets/images/1.12.0-agent-mongodb-a483a9fac25cf9f5f3e9eb7a8bb2203c.png" width="1514" height="1434" class="img_ev3q"></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimization-for-audit-and-enhancement-of-its-capabilities">Optimization for Audit and enhancement of its capabilities<a href="#optimization-for-audit-and-enhancement-of-its-capabilities" class="hash-link" aria-label="Direct link to Optimization for Audit and enhancement of its capabilities" title="Direct link to Optimization for Audit and enhancement of its capabilities">​</a></h3><p>In version 1.12.0, InLong enhancement the Audit reconciliation scenarios, including support for Agent data supplementation scenarios, Sort on Flink Checkpoint scenarios, etc.
Thanks to @doleyzi. For more information, please refer to: INLONG-9904、INLONG-9926、INLONG-9928、INLONG-9957.</p><ul><li>Support OpenAPI capabilities
In version 1.12.0, Audit has supported the OpenAPI, and each OpenAPI can be elected as the leader through HA. The leader node is responsible for real-time and retroactive aggregation of audit data, and the aggregated results are saved in the DB. The slave node is responsible for caching the data in the DB to memory and providing services externally. The leader node also provides the same service.
<img loading="lazy" alt="1.12.0-audit-process.png" src="/assets/images/1.12.0-audit-process-4578adddeb3ce63482dc4ced0e3a7a59.png" width="1500" height="1008" class="img_ev3q"></li><li>Support Agent data supplementation capabilities
In version 1.12.0, Audit supports the Agent data supplementation scenario by adding audit-version, which distinguishes the audit reconciliation for each supplementation.
<img loading="lazy" alt="1.12.0-audit-recovery.png" src="/assets/images/1.12.0-audit-recovery-a509e8a719ad1c11e31c030a21b894e4.png" width="1500" height="552" class="img_ev3q"></li><li>Support Sort Flink Checkpoint capabilities
In version 1.12.0, Audit supports the Sort Flink checkpoint scenario, ensuring that audit data is not lost or duplicated when the Flink job restarts or fails over.
<img loading="lazy" alt="1.12.0-audit-checkpoint.png" src="/assets/images/1.12.0-audit-checkpoint-ed769620899f9bce47d8d3a04ebf47fd.png" width="1500" height="760" class="img_ev3q"></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-for-redis-connector-in-sort-module">Support for Redis connector in Sort module<a href="#support-for-redis-connector-in-sort-module" class="hash-link" aria-label="Direct link to Support for Redis connector in Sort module" title="Direct link to Support for Redis connector in Sort module">​</a></h3><p>In version 1.12.0, An additional Flink 1.15-based Redis connector implementation has been added, supporting read and write operations for String, Hash, ZSet, and Bitmap, four common data types in Redis clusters and standalone instances. Schema conversion is supported in the Redis connector, allowing users to specify a Schema that can be converted to different Redis data types. The specific Schema conversion logic is shown in the following figure. In the bitmap conversion logic in the figure below, field1 as the key for the bitmap, while filed2 and field4 as the position (index) in the Bitmap. filed3 and field5 represent the set values (0 or 1). For specifics, refer to the native Redis command SETBIT key index value.
Thanks to @XiaoYou201. For more information, please refer to: INLONG-9835、INLONG-8948 .</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="future-plans">Future plans<a href="#future-plans" class="hash-link" aria-label="Direct link to Future plans" title="Direct link to Future plans">​</a></h2><p>In version 1.12.0, the community refactored InLong Agent, InLong
Audit, enriched Flink 1.15 Connector and other functions. In subsequent versions, InLong will continue to enrich Flink 1.15 Connector, enhance Transform capabilities, support offline data integration, unify DataProxy data protocols, optimize Dashboard experience, etc. We look forward to more developers participating and contributing.</p>]]></content>
        <author>
            <name>Mingyu Bao</name>
            <uri>https://github.com/baomingyu</uri>
        </author>
        <category label="Apache InLong" term="Apache InLong"/>
        <category label="Version" term="Version"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Release 1.10.0]]></title>
        <id>https://inlong.apache.org/blog/2023/12/13/release-1.10.0</id>
        <link href="https://inlong.apache.org/blog/2023/12/13/release-1.10.0"/>
        <updated>2023-12-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache InLong recently released version 1.10.0, which closed about 200+ issues, including 6+ major features and 30+ optimizations. The main features include Manager supporting viewing operation logs, supporting Group migration between clusters, Agent supporting periodic collection and task supplementation, Sort real-time synchronization supporting Transform, supporting MySQL to Iceberg whole database synchronization, and supporting automatic table creation. After the release of 1.10.0, Apache InLong has enriched and optimized Agent function scenarios, added the ability of whole database synchronization and automatic table creation, supported viewing operation logs, solved the demand for quick troubleshooting in development and operation, and optimized the user experience of Apache InLong operation and maintenance.]]></summary>
        <content type="html"><![CDATA[<p>Apache InLong recently released version 1.10.0, which closed about 200+ issues, including 6+ major features and 30+ optimizations. The main features include Manager supporting viewing operation logs, supporting Group migration between clusters, Agent supporting periodic collection and task supplementation, Sort real-time synchronization supporting Transform, supporting MySQL to Iceberg whole database synchronization, and supporting automatic table creation. After the release of 1.10.0, Apache InLong has enriched and optimized Agent function scenarios, added the ability of whole database synchronization and automatic table creation, supported viewing operation logs, solved the demand for quick troubleshooting in development and operation, and optimized the user experience of Apache InLong operation and maintenance.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-apache-inlong">About Apache InLong<a href="#about-apache-inlong" class="hash-link" aria-label="Direct link to About Apache InLong" title="Direct link to About Apache InLong">​</a></h2><p>As the industry's first one-stop, full-scenario, open-source massive data integration framework, Apache InLong provides automatic, safe, reliable, and high-performance data transmission capabilities to facilitate businesses to build stream-based data analysis, modeling, and applications quickly. At present, InLong is widely used in various industries such as advertising, payment, social networking, games, artificial intelligence, etc., serving thousands of businesses, among which the scale of high-performance scene data exceeds 1 trillion lines per day, and the scale of high-reliability scene data exceeds 10 trillion lines per day.</p><p>The core keywords of InLong project positioning are "one-stop" and "massive data". For "one-stop", we hope to shield technical details, provide complete data integration and support services, and implement out-of-the-box; With its advantages, such as multi-cluster management, it can stably support larger-scale data volumes based on trillions of lines per day.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="1100-version-overview">1.10.0 Version Overview<a href="#1100-version-overview" class="hash-link" aria-label="Direct link to 1.10.0 Version Overview" title="Direct link to 1.10.0 Version Overview">​</a></h2><p>Apache InLong recently released version 1.10.0, which closed about 200+ issues, including 6+ major features and 30+ optimizations. The main features include Manager supporting viewing operation logs, supporting Group migration between clusters, Agent supporting periodic collection and task supplementation, Sort real-time synchronization supporting Transform, supporting MySQL to Iceberg whole database synchronization, and supporting automatic table creation. After the release of 1.10.0, Apache InLong has enriched and optimized Agent function scenarios, added the ability of whole database synchronization and automatic table creation, supported viewing operation logs, solved the demand for quick troubleshooting in development and operation, and optimized the user experience of Apache InLong operation and maintenance. In Apache InLong 1.10.0 version, a large number of other features have also been completed, mainly including:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-module">Agent Module<a href="#agent-module" class="hash-link" aria-label="Direct link to Agent Module" title="Direct link to Agent Module">​</a></h3><ul><li>Support periodic collection and task supplementation</li><li>Global memory control to avoid OOM caused by large business data</li><li>Achieve final consistency between Agent and Manager tasks</li><li>Enrich Agent audit dimensions</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-module">Sort Module<a href="#sort-module" class="hash-link" aria-label="Direct link to Sort Module" title="Direct link to Sort Module">​</a></h3><ul><li>Support MySQL to Iceberg whole database synchronization</li><li>Support richer Flink 1.15 Connectors: MongoDB, Iceberg, SQLServer, HBase</li><li>Support task status management, able to view real-time synchronization task status</li><li>Real-time tasks support automatic table creation</li><li>Support configuration of Transform</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-module">Manager Module<a href="#manager-module" class="hash-link" aria-label="Direct link to Manager Module" title="Direct link to Manager Module">​</a></h3><ul><li>Support Sort Standalone management</li><li>Separate transmission protocol and data protocol types</li><li>Support Group cluster switching</li><li>Support query audit data size</li><li>Support viewing operation logs</li><li>Apache Iceberg supports automatic Schema information fetching</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dashboard-module">Dashboard Module<a href="#dashboard-module" class="hash-link" aria-label="Direct link to Dashboard Module" title="Direct link to Dashboard Module">​</a></h3><ul><li>Support displaying resource creators and recent modifiers simultaneously</li><li>Support viewing operation logs</li><li>Approval directory displays Group corresponding to consumer groups</li><li>Support batch parsing of source fields</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="audit-module">Audit Module<a href="#audit-module" class="hash-link" aria-label="Direct link to Audit Module" title="Direct link to Audit Module">​</a></h3><ul><li>Support automatic creation of required Kafka Topics when the service starts</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sdk-module">SDK Module<a href="#sdk-module" class="hash-link" aria-label="Direct link to SDK Module" title="Direct link to SDK Module">​</a></h3><ul><li>Resolve dependency version conflicts in DataProxy Java SDK</li><li>Update dependency version in DataProxy Go SDK</li><li>Use UUID instead of snowflake algorithm to generate data ID in DataProxy Go SDK</li><li>DataProxy C++ SDK supports dynamic load balancing</li><li>DataProxy C++ SDK supports multi-dimensional resource isolation</li><li>Optimize the ability of DataProxy C++ SDK to access nearby resources in multi-location deployment</li><li>DataProxy C++ SDK supports local disaster recovery</li><li>DataProxy C++ SDK supports dynamic update of DataProxy node information</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="others">Others<a href="#others" class="hash-link" aria-label="Direct link to Others" title="Direct link to Others">​</a></h3><ul><li>Strengthen protection against request forgery attacks</li><li>Update Snappy version</li><li>Add Master branch protection strategy</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="1100-version-feature-introduction">1.10.0 Version Feature Introduction<a href="#1100-version-feature-introduction" class="hash-link" aria-label="Direct link to 1.10.0 Version Feature Introduction" title="Direct link to 1.10.0 Version Feature Introduction">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-supports-periodic-collection">Agent Supports Periodic Collection<a href="#agent-supports-periodic-collection" class="hash-link" aria-label="Direct link to Agent Supports Periodic Collection" title="Direct link to Agent Supports Periodic Collection">​</a></h3><p>In version 1.10.0, InLong adds the ability to perform periodic file collection tasks. The periodic strategy includes daily, hourly, and real-time, and users can specify this strategy when creating a new file data source. At the same time, users can also configure time offsets to delay or advance collection. Thanks to @Justinhuang, @Blue
<img loading="lazy" alt="1.10.0-periodic-collection.png" src="/assets/images/1.10.0-periodic-collection-ce884efd696d2af000738de0153dbddb.png" width="1197" height="802" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-for-viewing-agent-audit-by-ip-dimension">Support for viewing Agent audit by IP dimension<a href="#support-for-viewing-agent-audit-by-ip-dimension" class="hash-link" aria-label="Direct link to Support for viewing Agent audit by IP dimension" title="Direct link to Support for viewing Agent audit by IP dimension">​</a></h3><p>In order to better monitor the status of InLong Agent and quickly discover problems in live network operation, in version 1.10.0, users can view different Agent audit indicators based on the IP dimension in the system operation and maintenance -&gt; audit module. Thanks to @fuwen11, @Bluewang, and @Justinhuang for their contributions, for more details, please see INLONG-9443, INLONG-9446, and INLONG-9458.
<img loading="lazy" alt="1.10.0-agent-audit.png" src="/assets/images/1.10.0-agent-audit-35e73a9f5dd731a8f4cdf5b92f6a9b0e.png" width="1942" height="741" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-adds-group-operation-logs">Manager adds Group operation logs<a href="#manager-adds-group-operation-logs" class="hash-link" aria-label="Direct link to Manager adds Group operation logs" title="Direct link to Manager adds Group operation logs">​</a></h3><p>In version 1.10.0, InLong supports viewing operation logs, including the creation and modification of Group/Stream, and the addition and removal of Sink/Source operations. Operation logs can quickly help users track historical behavior, allowing users to quickly view key data stream operations for easier live network maintenance. Users can view all operation logs under a Group in the Data Access -&gt; Group Details -&gt; Operation Logs section.
<img loading="lazy" alt="1.10.0-group-operation-log.png" src="/assets/images/1.10.0-group-operation-log-ceb82b6cdccae8cbe49c3602bc030522.png" width="1676" height="856" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-ability-to-switch-group-clusters">New ability to switch Group clusters<a href="#new-ability-to-switch-group-clusters" class="hash-link" aria-label="Direct link to New ability to switch Group clusters" title="Direct link to New ability to switch Group clusters">​</a></h3><p>In order to support resource integration and cost allocation capabilities, InLong introduced the Group cluster switching feature in version 1.10.0. Direct switching will inevitably lead to problems such as unsynchronized metadata between modules and data loss. To achieve seamless cluster switching for businesses, Group cluster switching is divided into three states and two steps. Thanks to @Vernedeng for the contribution, for more details, please see INLONG-9314.
<img loading="lazy" alt="1.10.0-group-switching-1.png" src="/assets/images/1.10.0-group-switching-1-79c6d3f94cee8920d7c6332a3ff1e5ed.png" width="1500" height="673" class="img_ev3q">
Before switching clusters, the data is on Cluster 1.
<img loading="lazy" alt="1.10.0-group-switching-2.png" src="/assets/images/1.10.0-group-switching-2-e880f8a1d3f6a78fd047758c4e628a10.png" width="1500" height="673" class="img_ev3q">
After starting the cluster switch, the original routing configuration is copied, new data is written to Cluster 2, and the unsent data on Cluster 1 continues to be sent.
<img loading="lazy" alt="1.10.0-group-switching-3.png" src="/assets/images/1.10.0-group-switching-3-252b68c477b2892f5075cd88e7077b59.png" width="1500" height="673" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="c-sdk-supports-multi-dimensional-isolation">C++ SDK supports multi-dimensional isolation<a href="#c-sdk-supports-multi-dimensional-isolation" class="hash-link" aria-label="Direct link to C++ SDK supports multi-dimensional isolation" title="Direct link to C++ SDK supports multi-dimensional isolation">​</a></h3><p>In the old version of the C++ SDK, all Groups competed for the internal resources of the SDK. If a particular Group has a particularly large flow, it will inevitably squeeze other Groups, causing small flow Groups to be unable to obtain resources. In version 1.10.0, DataProxy C++ SDK supports resource isolation at the Cluster and Group levels, and users can enable or disable it by configuring enable_isolation. Thanks to @doleyzi for the contribution, for more details, please see INLONG-9213.
<img loading="lazy" alt="1.10.0-sdk-isolation.png" src="/assets/images/1.10.0-sdk-isolation-eaa5c499569e4ebd5077d19f4adc9e77.png" width="1500" height="791" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="future-plans">Future plans<a href="#future-plans" class="hash-link" aria-label="Direct link to Future plans" title="Direct link to Future plans">​</a></h2><p>In version 1.10.0, the community refactored InLong Agent, enriched Flink 1.15 Connector, and completed support for viewing operation logs and other functions. In subsequent versions, InLong will continue to enrich Flink 1.15 Connector, enhance Transform capabilities, unify DataProxy data protocols, support integration with Apache Paimon, optimize Dashboard experience, etc. We look forward to more developers participating and contributing.</p>]]></content>
        <author>
            <name>Verne Deng</name>
            <uri>https://github.com/vernedeng</uri>
        </author>
        <category label="Apache InLong" term="Apache InLong"/>
        <category label="Version" term="Version"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Release 1.9.0]]></title>
        <id>https://inlong.apache.org/blog/2023/09/25/release-1.9.0</id>
        <link href="https://inlong.apache.org/blog/2023/09/25/release-1.9.0"/>
        <updated>2023-09-25T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache InLong recently released version 1.9.0, which closed about 200+ issues, including 2+ major features and 30+ optimizations. The main improvements include building observability capabilities and optimizing DataProxySDK-CPP. After the release of version 1.9.0, Apache InLong has enhanced its observability capabilities in areas such as end-to-end tracing, metric collection, access and visualization, and alerting. This addresses the need for rapid problem diagnosis and performance optimization during development and operations, improving the user experience for Apache InLong's operation and maintenance.]]></summary>
        <content type="html"><![CDATA[<p>Apache InLong recently released version 1.9.0, which closed about 200+ issues, including 2+ major features and 30+ optimizations. The main improvements include building observability capabilities and optimizing DataProxySDK-CPP. After the release of version 1.9.0, Apache InLong has enhanced its observability capabilities in areas such as end-to-end tracing, metric collection, access and visualization, and alerting. This addresses the need for rapid problem diagnosis and performance optimization during development and operations, improving the user experience for Apache InLong's operation and maintenance.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-apache-inlong">About Apache InLong<a href="#about-apache-inlong" class="hash-link" aria-label="Direct link to About Apache InLong" title="Direct link to About Apache InLong">​</a></h2><p>As the industry's first one-stop, full-scenario, open-source massive data integration framework, Apache InLong provides automatic, safe, reliable, and high-performance data transmission capabilities to facilitate businesses to build stream-based data analysis, modeling, and applications quickly. At present, InLong is widely used in various industries such as advertising, payment, social networking, games, artificial intelligence, etc., serving thousands of businesses, among which the scale of high-performance scene data exceeds 1 trillion lines per day, and the scale of high-reliability scene data exceeds 10 trillion lines per day.</p><p>The core keywords of InLong project positioning are "one-stop" and "massive data". For "one-stop", we hope to shield technical details, provide complete data integration and support services, and implement out-of-the-box; With its advantages, such as multi-cluster management, it can stably support larger-scale data volumes based on trillions of lines per day.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview-of-version-190">Overview of version 1.9.0<a href="#overview-of-version-190" class="hash-link" aria-label="Direct link to Overview of version 1.9.0" title="Direct link to Overview of version 1.9.0">​</a></h2><p>Apache InLong recently released version 1.9.0, which closed about 200+ issues, including 5+ major features and 30+ optimizations. The main work completed in this version includes building observability capabilities, optimizing DataProxy C++ SDK, optimizing DataProxy metadata configuration updates, adding command-line tools for TubeMQ, automatically switching write methods for Iceberg, and supporting resource migration between tenants in Manager. After the release of version 1.9.0, Apache InLong has enhanced its observability capabilities in areas such as end-to-end tracing, metric collection, access and visualization, and alerting. This addresses the need for rapid problem diagnosis and performance optimization during development and operations, while also improving the user experience for Apache InLong's operation and maintenance.</p><p>In addition, Apache InLong 1.9.0 includes a large number of other features, mainly including:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-module">Agent module<a href="#agent-module" class="hash-link" aria-label="Direct link to Agent module" title="Direct link to Agent module">​</a></h3><ul><li>Support end-to-end log tracking and reporting, enhancing observability capabilities</li><li>Remove metric reporting deregistration logic during TaskManager initialization</li><li>Remove capacity limit for setting blacklist</li><li>Optimize Agent's JVM parameters</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dataproxy-module">DataProxy module<a href="#dataproxy-module" class="hash-link" aria-label="Direct link to DataProxy module" title="Direct link to DataProxy module">​</a></h3><ul><li>Optimize metadata update logic</li><li>Optimize DataProxy metric statistics</li><li>Optimize retry logic after failed sending</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-module">Sort module<a href="#sort-module" class="hash-link" aria-label="Direct link to Sort module" title="Direct link to Sort module">​</a></h3><ul><li>Data synchronization supports audit metric reporting</li><li>Iceberg supports dynamic switching between append and upsert modes</li><li>When writing data to Kafka, support writing to different partitions based on custom fields</li><li>PostgreSQL supports multi-concurrent reading</li><li>Doris write supports automatic Schema changes</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-module">Manager module<a href="#manager-module" class="hash-link" aria-label="Direct link to Manager module" title="Direct link to Manager module">​</a></h3><ul><li>Support end-to-end log tracking and reporting, enhancing observability capabilities</li><li>Support data targets such as Tencent CLS, Pulsar cluster, Iceberg, and StarRocks</li><li>Support Iceberg and StarRocks as data sources on Flink 1.15</li><li>Improve multi-tenant capabilities: including tenant status validation before deleting a tenant, OpenAPI support for tenant parameters, and multi-tenant configuration support for data sources</li><li>ManagerClient supports paginated queries for data stream Source and Sink resources</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dashboard-module">Dashboard module<a href="#dashboard-module" class="hash-link" aria-label="Direct link to Dashboard module" title="Direct link to Dashboard module">​</a></h3><ul><li>Support management of Pulsar data clusters and support output to Pulsar</li><li>Optimize the performance of tenant permission queries</li><li>Improve the usability of the interface display</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="audit-module">Audit module<a href="#audit-module" class="hash-link" aria-label="Direct link to Audit module" title="Direct link to Audit module">​</a></h3><ul><li>Add audit_tag information to distinguish between data sources and data targets</li><li>Optimize audit Proxy log output</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sdk-module">SDK module<a href="#sdk-module" class="hash-link" aria-label="Direct link to SDK module" title="Direct link to SDK module">​</a></h3><ul><li>Optimize DataProxySDK-CPP, improving performance and reliability during network instability</li><li>SortSDK supports parallel creation of cache layer consumption</li><li>Optimize failure retry strategy for DataProxySDK-Java</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tubemq-module">TubeMQ module<a href="#tubemq-module" class="hash-link" aria-label="Direct link to TubeMQ module" title="Direct link to TubeMQ module">​</a></h3><ul><li>Add TubeMQ command-line tool</li><li>TubeMQ Manager adds restart script</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="others">Others<a href="#others" class="hash-link" aria-label="Direct link to Others" title="Direct link to Others">​</a></h3><ul><li>Add ASF DOAP file</li><li>Add Mysql connector management image</li><li>Optimize third-party dependencies to address security risks</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="190-feature-introduction">1.9.0 Feature Introduction<a href="#190-feature-introduction" class="hash-link" aria-label="Direct link to 1.9.0 Feature Introduction" title="Direct link to 1.9.0 Feature Introduction">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="observability-capability-building">Observability Capability Building<a href="#observability-capability-building" class="hash-link" aria-label="Direct link to Observability Capability Building" title="Direct link to Observability Capability Building">​</a></h3><p>In the application process of Apache InLong, the following scenario requirements are often encountered:</p><ul><li>Locate the problematic code through detailed link call data (Tracing)</li><li>Query and analyze the abnormal module and associated logs to find the core error information (Logs)</li><li>Open the monitoring dashboard to find abnormal phenomena and locate the abnormal module through queries (Metrics)</li><li>Discover anomalies through various preset alarms (Metrics/Logs)</li></ul><p>In version 1.9.0, @ZhaoNiuniu has contributed to the complete observability capability building of InLong based on OpenTelemetry. OpenTelemetry provides flexible, convenient, and rich Span customization capabilities, such as adding custom attributes, adding events, etc. By embedding points in the core positions of the code, the data can be displayed in the backend UI. The entire solution mainly includes the following steps:</p><ul><li>The application pushes the Trace, Log, and Metric data collected through instrumentation to the otel collector using the otlp-exporter;</li><li>The otel collector collects and converts the data, then exports it to Jaeger, Prometheus, and Elasticsearch;</li><li>Grafana configures the three data sources for unified display, query, monitoring, and alerting.</li></ul><p><img loading="lazy" alt="1.9.0-observability.png" src="/assets/images/1.9.0-observability-5e36d4e44ea55c85bcfc87239b0f004f.png" width="1500" height="608" class="img_ev3q"></p><p>Thanks to @ZhaoNiuniu's contribution, see INLONG-8611 and INLONG-8799 for details.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimize-dataproxy-c-sdk">Optimize DataProxy C++ SDK<a href="#optimize-dataproxy-c-sdk" class="hash-link" aria-label="Direct link to Optimize DataProxy C++ SDK" title="Direct link to Optimize DataProxy C++ SDK">​</a></h3><p>The older version of DataProxy C++ SDK was developed based on the C language, which had limited extensibility. The use of the singleton pattern resulted in performance limitations, and the handling of failed sending scenarios was unreasonable, which could easily trigger coredump. Therefore, the SDK has been optimized with the following improvements:</p><ul><li>Adopt the C++ development mode, refactor DataProxy C++ SDK, no longer limited by the C language, enhancing the extensibility of the code;</li><li>A single process supports multiple SDK instances, allowing horizontal scaling by adding SDK instances, breaking through performance limitations;</li><li>Decouple data reception and data transmission, allowing separate configuration of the number of threads for receiving and sending, improving the SDK's scalability;</li><li>Decouple the SDK's data packaging logic from the business thread, reducing the impact on the business thread while improving the packaging performance;</li><li>Resolve the issue of the older version SDK causing coredump due to sending failures from the root.</li></ul><p>With these optimizations, the DataProxy C++ SDK becomes more efficient, scalable, and reliable, providing better data processing capabilities for Apache InLong users.</p><p><img loading="lazy" alt="1.9.0-dataproxycplussdk.png" src="/assets/images/1.9.0-dataproxycplussdk-b3babf4ae87ac01754f1a54483b8eb4a.png" width="1500" height="411" class="img_ev3q"></p><p>Thanks to @doleyzi's contribution, see INLONG-8747 for details.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimize-metadata-update-logic-of-dataproxy">Optimize metadata update logic of DataProxy<a href="#optimize-metadata-update-logic-of-dataproxy" class="hash-link" aria-label="Direct link to Optimize metadata update logic of DataProxy" title="Direct link to Optimize metadata update logic of DataProxy">​</a></h3><p>DataProxy fetches metadata configuration from Manager, but sometimes due to network issues, misoperations, or other reasons, DataProxy may obtain incorrect metadata configuration from Manager. In version 1.9.0, DataProxy has added a protection mechanism for metadata configuration updates, enhancing the reliability of Apache InLong in extreme scenarios. The logic of the metadata configuration update protection mechanism is as follows:</p><ul><li>When DataProxy receives a new metadata configuration from Manager, it first checks the configuration's validity, such as checking for null values, incorrect formats, etc., to ensure that the received configuration is correct and complete.</li><li>DataProxy compares the new configuration with the current configuration. If there are no significant differences, it ignores the new configuration and continues using the current one. This helps avoid unnecessary updates and reduces the impact on the system.</li><li>If the new configuration has significant differences from the current configuration, DataProxy will update its internal data structures and processing logic accordingly, ensuring that the system can adapt to the new configuration without causing errors or data loss.</li><li>In case the new configuration causes errors or issues, DataProxy will roll back to the previous configuration and report the problem to Manager, allowing the system to maintain its stability and reliability.</li></ul><p>By implementing this protection mechanism for metadata configuration updates, Apache InLong can better handle extreme scenarios and ensure the reliability of its data processing capabilities.</p><p><img loading="lazy" alt="1.9.0-dataproxymetadataupdate.png" src="/assets/images/1.9.0-dataproxymetadataupdate-0c8dd3ec82c2555493b9c9fdc97022cb.png" width="1500" height="375" class="img_ev3q"></p><p>Thanks to @gosonzhang's contribution, see INLONG-8758 and INLONG-8899 for details.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tubemq-command-line-tool">TubeMQ command-line tool<a href="#tubemq-command-line-tool" class="hash-link" aria-label="Direct link to TubeMQ command-line tool" title="Direct link to TubeMQ command-line tool">​</a></h3><p>To enable Apache InLong operations and maintenance personnel to perform various operations simply, quickly, and flexibly in terminal windows in server management and DevOps scenarios, TubeMQ provides command-line tools to manage topics, produce and consume messages, and manage consumer groups through command-line parameters or options. For detailed usage documentation, please refer to <a href="https://inlong.apache.org/docs/modules/tubemq/commandline_tools/" target="_blank" rel="noopener noreferrer">TubeMQ Command-line Tools</a>. The main features include:</p><ul><li>Managing topics: creating, deleting, and querying topics, as well as setting topic attributes such as the number of partitions and replication factor.</li><li>Producing messages: sending messages to specified topics, with support for custom message content, key, and partition.</li><li>Consuming messages: consuming messages from specified topics and partitions, with support for specifying consumer groups and offsets.</li><li>Managing consumer groups: creating, deleting, and querying consumer groups, as well as setting consumer group attributes such as rebalancing and offset management.</li><li>Monitoring and statistics: collecting and displaying various statistics and metrics related to topics, producers, consumers, and consumer groups, helping users understand the system's performance and status.</li></ul><p>By providing these command-line tools, Apache InLong allows operations and maintenance personnel to manage and operate TubeMQ more easily and efficiently in various scenarios.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ bin/tubectl [options] [command] [command options]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ bin/tubectl topic -h</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ bin/tubectl message produce</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ bin/tubectl message consume</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ bin/tubectl cgroup list</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Thanks to @fancycoderzf's contribution, see INLONG-4972 for details.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="automatically-switch-write-modes-of-iceberg">Automatically switch write modes of Iceberg<a href="#automatically-switch-write-modes-of-iceberg" class="hash-link" aria-label="Direct link to Automatically switch write modes of Iceberg" title="Direct link to Automatically switch write modes of Iceberg">​</a></h3><p>Currently, the real-time synchronization from MySQL to Iceberg uses the upsert mode by default during the full data phase. The upsert implementation in Iceberg involves a delete operation followed by an add operation, with the delete operation resulting in the creation of a large number of delete files. This significantly impacts query efficiency. This optimization enables the task to use Append mode during the full data phase and automatically switch to Upsert mode during the incremental phase. The implementation involves the source carrying metadata fields in the data to determine which write mode to use in the Iceberg writer. When switching write modes, the data is refreshed, and the metadata fields are removed during the actual write operation.</p><p><img loading="lazy" alt="1.9.0-icebergswitch.png" src="/assets/images/1.9.0-icebergswitch-69ebf3849f1eac9efacd5fe0eac3462b.png" width="1411" height="342" class="img_ev3q"></p><p>Thanks to <a href="https://github.com/lordcheng10" target="_blank" rel="noopener noreferrer">@lordcheng10</a> and @Emsnap's contribution.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-support-resource-migration-between-tenants">Manager support resource migration between tenants<a href="#manager-support-resource-migration-between-tenants" class="hash-link" aria-label="Direct link to Manager support resource migration between tenants" title="Direct link to Manager support resource migration between tenants">​</a></h3><p>After InLong supports multi-tenancy, InLong Groups upgraded from older versions will be defined under the public tenant, while newly created InLong Groups will be defined within the tenant under the user's name. This leads to users frequently switching between multiple tenants to use resources. This optimization supports the migration of InLong Groups between different tenants, while also validating and automatically creating corresponding cluster labels, data nodes, and other resources.</p><p><img loading="lazy" alt="1.9.0-managertenant.png" src="/assets/images/1.9.0-managertenant-3cd4b2aa32313c19eabec62be823f41b.png" width="1500" height="203" class="img_ev3q"></p><p>Thanks to @vernedeng's contribution.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="follow-up-planning">Follow-up planning<a href="#follow-up-planning" class="hash-link" aria-label="Direct link to Follow-up planning" title="Direct link to Follow-up planning">​</a></h2><p>In the 1.9.0 version, the community has also refactored the DataProxy C++ SDK, enriched the Flink 1.15 Connector, and improved data synchronization features. In subsequent versions, InLong will continue to enrich the Flink 1.15 Connector, enhance the scheduling capabilities of data synchronization, and improve the Agent's file collection capabilities. We look forward to more developers participating and contributing.</p>]]></content>
        <author>
            <name>luchunliang</name>
            <uri>https://github.com/luchunliang</uri>
        </author>
        <category label="Apache InLong" term="Apache InLong"/>
        <category label="Version" term="Version"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Release 1.8.0]]></title>
        <id>https://inlong.apache.org/blog/2023/07/24/release-1.8.0</id>
        <link href="https://inlong.apache.org/blog/2023/07/24/release-1.8.0"/>
        <updated>2023-07-24T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache InLong recently released version 1.8.0, which closed about 190+ issues, including 6+ major features and 30+ optimizations. The main improvements include multi-tenant management, support for multiple Apache Flink versions, data synchronization in Dashboard, data preview support, and optimization of ultra-long log processing logic. After the release of 1.8.0, Apache InLong has completed the layout of full-scenario data integration around data access, data synchronization, and data subscription, combined with multi-tenant management, multi-cluster management, approval flow management, and full-link audit/metrics.]]></summary>
        <content type="html"><![CDATA[<p>Apache InLong recently released version 1.8.0, which closed about 190+ issues, including 6+ major features and 30+ optimizations. The main improvements include multi-tenant management, support for multiple Apache Flink versions, data synchronization in Dashboard, data preview support, and optimization of ultra-long log processing logic. After the release of 1.8.0, Apache InLong has completed the layout of full-scenario data integration around data access, data synchronization, and data subscription, combined with multi-tenant management, multi-cluster management, approval flow management, and full-link audit/metrics.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-apache-inlong">About Apache InLong<a href="#about-apache-inlong" class="hash-link" aria-label="Direct link to About Apache InLong" title="Direct link to About Apache InLong">​</a></h2><p>As the industry's first one-stop, full-scenario, open-source massive data integration framework, Apache InLong provides automatic, safe, reliable, and high-performance data transmission capabilities to facilitate businesses to build stream-based data analysis, modeling, and applications quickly. At present, InLong is widely used in various industries such as advertising, payment, social networking, games, artificial intelligence, etc., serving thousands of businesses, among which the scale of high-performance scene data exceeds 1 trillion lines per day, and the scale of high-reliability scene data exceeds 10 trillion lines per day.</p><p>The core keywords of InLong project positioning are "one-stop" and "massive data". For "one-stop", we hope to shield technical details, provide complete data integration and support services, and implement out-of-the-box; With its advantages, such as multi-cluster management, it can stably support larger-scale data volumes based on trillions of lines per day.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview-of-version-180">Overview of version 1.8.0<a href="#overview-of-version-180" class="hash-link" aria-label="Direct link to Overview of version 1.8.0" title="Direct link to Overview of version 1.8.0">​</a></h2><p>Apache InLong recently released version 1.8.0, which closed about 190+ issues, including 6+ major features and 30+ optimizations, mainly completing multi-tenant management, support for multiple Apache Flink versions, data synchronization in Dashboard, data preview support, and optimization of ultra-long log processing logic. After the release of 1.8.0, Apache InLong has completed the layout of full-scenario data integration around data access, data synchronization, and data subscription, combined with multi-tenant management, multi-cluster management, approval flow management, and full-link audit/metrics. Apache InLong has built a comprehensive data integration solution, achieving out-of-the-box usability:</p><ul><li>Data access: Data access is the process of aggregating data from data sources to the same storage service for further data querying and analysis;</li><li>Data synchronization: Data synchronization is the process of establishing consistency between data sources and target data storage, which can coordinate data over time;</li><li>Data subscription: Data subscription provides subscribers with the data they are authorized to access;</li></ul><p>In version 1.8.0 of Apache InLong, many other features have also been completed, mainly including:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-module">Agent module<a href="#agent-module" class="hash-link" aria-label="Direct link to Agent module" title="Direct link to Agent module">​</a></h3><ul><li>Optimized ultra-long log processing logic, improving file collection efficiency and stability</li><li>Fixed the issue of thread leakage caused by task termination</li><li>Adopted flow control to solve the OOM problem caused by file number growth</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dataproxy-module">DataProxy module<a href="#dataproxy-module" class="hash-link" aria-label="Direct link to DataProxy module" title="Direct link to DataProxy module">​</a></h3><ul><li>Support for Golang SDK</li><li>Support for configuring black and white lists based on full IP or CIDR format IP segments</li><li>Support for configuring the maximum number of write retries</li><li>Support for sending data to the default Topic when write fails</li><li>Code refactoring, unified configuration acquisition method</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-module">Sort module<a href="#sort-module" class="hash-link" aria-label="Direct link to Sort module" title="Direct link to Sort module">​</a></h3><ul><li>Enhanced DDL parsing capability, improving the stability of DDL-aware scenarios</li><li>Support for multiple Flink versions</li><li>Support for Decimal precision recognition in whole-library scenarios</li><li>Hive supports whole-library migration, with the implementation consistent with MySQL whole-library migration</li><li>Iceberg supports automatic column updates and column deletions, greatly enriching Schema change capabilities</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-module">Manager module<a href="#manager-module" class="hash-link" aria-label="Direct link to Manager module" title="Direct link to Manager module">​</a></h3><ul><li>Support for Pulsar, TubeMQ data preview</li><li>Support for dynamic configuration of audit data sources</li><li>Support for querying audit delay information</li><li>Support for multi-tenant management</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dashboard-module">Dashboard module<a href="#dashboard-module" class="hash-link" aria-label="Direct link to Dashboard module" title="Direct link to Dashboard module">​</a></h3><ul><li>Support for data flow preview</li><li>Support for viewing InLongGroup resource details</li><li>Support for tenant management and tenant switching</li><li>Support for data synchronization</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="others">Others<a href="#others" class="hash-link" aria-label="Direct link to Others" title="Direct link to Others">​</a></h3><ul><li>Remove conflicting Jsqlparser versions</li><li>Upgrade Spring-Boot-Autoconfigure version to 2.6.15</li><li>Upgrade the Snappy-Java version to 1.1.10.1</li><li>Fix syntax errors in Workflow configuration files</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="180-feature-introduction">1.8.0 Feature Introduction<a href="#180-feature-introduction" class="hash-link" aria-label="Direct link to 1.8.0 Feature Introduction" title="Direct link to 1.8.0 Feature Introduction">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-optimizes-ultra-long-log-processing-logic-improving-file-collection-efficiency-and-stability">Agent optimizes ultra-long log processing logic, improving file collection efficiency and stability<a href="#agent-optimizes-ultra-long-log-processing-logic-improving-file-collection-efficiency-and-stability" class="hash-link" aria-label="Direct link to Agent optimizes ultra-long log processing logic, improving file collection efficiency and stability" title="Direct link to Agent optimizes ultra-long log processing logic, improving file collection efficiency and stability">​</a></h3><p>In actual use, due to improper use by users or bugs in data production programs, occasionally, a single data length reaches MB or even GB level. For Agents deployed in low-profile environments, this type of data greatly affects the performance of sending. The lower version Agent reads this type of data directly into memory based on the newline character and then discards it, but is limited by the hardware configuration of the Agent deployment environment, a single ultra-long data is extremely likely to cause OOM exceptions. In version 1.8.0, Agent optimized the processing logic of ultra-long logs, ensuring that data loading does not exceed memory limits through segmented collection and segmented discarding. Thanks to @justinhuang's contribution, see INLONG-8180 for details.
<img loading="lazy" alt="1.8.0-agent-under-1.8.0.png" src="/assets/images/1.8.0-agent-under-1.8.0-9125fe37a0229a983fe1e6f5a6606d31.png" width="2544" height="1356" class="img_ev3q">
<img loading="lazy" alt="1.8.0-agent-1.8.0.png" src="/assets/images/1.8.0-agent-1.8.0-5605736e061b6d7909be88085fe7e328.png" width="3334" height="2143" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-adopts-global-flow-control-solving-the-oom-problem-caused-by-the-growth-of-file-numbers">Agent adopts global flow control, solving the OOM problem caused by the growth of file numbers<a href="#agent-adopts-global-flow-control-solving-the-oom-problem-caused-by-the-growth-of-file-numbers" class="hash-link" aria-label="Direct link to Agent adopts global flow control, solving the OOM problem caused by the growth of file numbers" title="Direct link to Agent adopts global flow control, solving the OOM problem caused by the growth of file numbers">​</a></h3><p>In previous versions, each file was collected and sent by its thread. Although we limited the maximum collection buffer of each file, with the growth of user traffic, the increase in the number of files is inevitable, leading to the simultaneous collection of too many files, causing OOM exceptions due to memory overflow. InLong supports the feature of Agent configuring global flow control in version 1.8.0. With this feature, Agent can effectively avoid frequent OOM problems caused by the growth of file numbers or the use of small quota servers. Thanks to @justinhuang's contribution, see INLONG-8251 for details. If you need to use this feature, you can add the corresponding configuration in agent.properties.
<img loading="lazy" alt="1.8.0-agent-flow-controll.png" src="/assets/images/1.8.0-agent-flow-controll-586153f2b9ee526b805dd01af8db56f1.png" width="3356" height="1213" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-multiple-flink-versions">Support multiple Flink versions<a href="#support-multiple-flink-versions" class="hash-link" aria-label="Direct link to Support multiple Flink versions" title="Direct link to Support multiple Flink versions">​</a></h3><p>As community users go deeper into using InLong, the scenarios InLong faces become more diverse and complex. To support the needs of users in different Flink environments, InLong has added support for multiple Flink versions in the current version. Users can choose the Flink version to start in the plugins/flink-sort-plugin.properties configuration file in InLong-Manager.</p><p>When changing the Flink version required to run the Sort component, you also need to replace the connectors in the InLong-Sort/connector directory with the corresponding version of the jar package. For details, see the InLong official website documentation. Thanks to @Emsnap, @GanfengTan, and @haifxu for their contributions to this capability.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"># inLong-manager/plugins/flink-sort-plugin.properties</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Flink version, support [1.13|1.15]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">flink.version=1.13</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-multi-tenant-management">Support multi-tenant management<a href="#support-multi-tenant-management" class="hash-link" aria-label="Direct link to Support multi-tenant management" title="Direct link to Support multi-tenant management">​</a></h3><p>To address the requirements for permission and resource isolation in multi-user scenarios, InLong introduces a multi-tenant architecture in the current version. The multi-tenant architecture ensures that data and permissions do not interfere with each other among different users within the same group of services. Thanks to @vernedeng and @bluewang for their contributions to this feature, see INLONG-7914 for feature details. The following is the core process:
<img loading="lazy" alt="1.8.0-multi-tenant-management.png" src="/assets/images/1.8.0-multi-tenant-management-c3af4313025cdb26d7601e45b2274420.png" width="2597" height="3891" class="img_ev3q"></p><p>Tenants are transparent to core logic developers. At the entrance of the request, tenant authentication is added, and requests without access to the tenant's permissions are directly rejected; before accessing the Database, the corresponding tenant filter conditions are added to ensure that the scope of data query and modification is limited within the tenant.</p><p>Users can create tenants and assign tenant roles on the Dashboard.
<img loading="lazy" alt="1.8.0-create-tenant.png" src="/assets/images/1.8.0-create-tenant-369c08949975ccaa2f136783327f102d.png" width="1726" height="863" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-real-time-synchronization">Support real-time synchronization<a href="#support-real-time-synchronization" class="hash-link" aria-label="Direct link to Support real-time synchronization" title="Direct link to Support real-time synchronization">​</a></h3><p>The new version supports real-time data synchronization. The main difference between real-time synchronization and data access is that it does not require the support of intermediate MQ storage. The Sort component directly stores the source data, greatly enriching the user's usage scenarios.</p><p>As shown in the figure below, the Tab page adds a "Data Synchronization" label. After the user configures the basic Group information, they only need to enter the "Data Source" and "Data Target" information, and after submitting the task, the data can be synchronized in real-time.</p><p>Thanks to @fuwen11, @bluewang, @Emsnap, and @haifxu for their contributions to this feature.
<img loading="lazy" alt="1.8.0-realtime-sync.png" src="/assets/images/1.8.0-realtime-sync-238be2fc9f89289a181513e67f552bb2.png" width="1726" height="961" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-data-preview">Support data preview<a href="#support-data-preview" class="hash-link" aria-label="Direct link to Support data preview" title="Direct link to Support data preview">​</a></h3><p>For businesses just accessing InLong, data preview can help users quickly confirm the accuracy of reported data and locate problems. In this version, the InLong front supports previewing users' real-time reported data. Thanks to @fuwen11 and @bluewang's contributions, users can choose data preview in the operation bar under the data stream after successfully creating a data stream and reporting data.
<img loading="lazy" alt="1.8.0-data-preview.png" src="/assets/images/1.8.0-data-preview-e8f81f356cbbae7dc5df7a84eba3d15b.png" width="1714" height="913" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-querying-transmission-delay">Support querying transmission delay<a href="#support-querying-transmission-delay" class="hash-link" aria-label="Direct link to Support querying transmission delay" title="Direct link to Support querying transmission delay">​</a></h3><p>Transmission delay is crucial for some real-time consumption scenarios. In this version, InLong audit supports frontend viewing of average transmission delay indicators. Thanks to @fuwen11 and @bluewang's contributions, users can query link transmission delay after successfully creating a data stream and reporting data.
<img loading="lazy" alt="1.8.0-trans-delay.png" src="/assets/images/1.8.0-trans-delay-7486369cefa16ed1b894dcd0ce7b7fe1.png" width="1711" height="964" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="follow-up-planning">Follow-up planning<a href="#follow-up-planning" class="hash-link" aria-label="Direct link to Follow-up planning" title="Direct link to Follow-up planning">​</a></h2><p>In version 1.8.0, the community also restructured the DataProxy code, unified the configuration pull interface, supported complete IP and CIDR-based IP segment configuration of black and white list features, and improved module performance and stability. Sort has improved stability in DDL sensing scenarios and supports whole library migration of Hive, Iceberg automatic column update, and column storage features. In subsequent versions, InLong will refactor DataProxy C++ SDK, enrich Flink 1.15 Connector, and improve data synchronization functions, looking forward to more developers participating in the contribution.</p>]]></content>
        <author>
            <name>Verne Deng</name>
            <uri>https://github.com/vernedeng</uri>
        </author>
        <category label="Apache InLong" term="Apache InLong"/>
        <category label="Version" term="Version"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Release 1.7.0]]></title>
        <id>https://inlong.apache.org/blog/2023/05/19/release-1.7.0</id>
        <link href="https://inlong.apache.org/blog/2023/05/19/release-1.7.0"/>
        <updated>2023-05-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache InLong recently released version 1.7.0, which closed about 150+ issues, including 3+ major features and 40+ optimizations. The main features include support for sending data directly to Kafka, MySQL all-database migration with schema change support, GH-OST awareness for MySQL all-database migration, the addition of 4 batch import modes (CSV, SQL, JSON, and Excel), simplification of command line tool for creating data stream configurations, and refactoring of the Dashboard layout.]]></summary>
        <content type="html"><![CDATA[<p>Apache InLong recently released version 1.7.0, which closed about 150+ issues, including 3+ major features and 40+ optimizations. The main features include support for sending data directly to Kafka, MySQL all-database migration with schema change support, GH-OST awareness for MySQL all-database migration, the addition of 4 batch import modes (CSV, SQL, JSON, and Excel), simplification of command line tool for creating data stream configurations, and refactoring of the Dashboard layout.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-apache-inlong">About Apache InLong<a href="#about-apache-inlong" class="hash-link" aria-label="Direct link to About Apache InLong" title="Direct link to About Apache InLong">​</a></h2><p>As the industry's first one-stop open-source massive data integration framework, Apache InLong provides automatic, safe, reliable, and high-performance data transmission capabilities to facilitate businesses to build stream-based data analysis, modeling, and applications quickly. At present, InLong is widely used in various industries such as advertising, payment, social networking, games, artificial intelligence, etc., serving thousands of businesses, among which the scale of high-performance scene data exceeds 1 trillion lines per day, and the scale of high-reliability scene data exceeds 10 trillion lines per day.</p><p>The core keywords of InLong project positioning are "one-stop" and "massive data". For "one-stop", we hope to shield technical details, provide complete data integration and support services, and implement out-of-the-box; With its advantages, such as multi-cluster management, it can stably support larger-scale data volumes based on trillions lines per day.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="170-overview">1.7.0 Overview<a href="#170-overview" class="hash-link" aria-label="Direct link to 1.7.0 Overview" title="Direct link to 1.7.0 Overview">​</a></h2><p>Apache InLong recently released version 1.7.0, which closed about 150+ issues, including 3+ major features and 40+ optimizations. The main features include support for sending data directly to Kafka, MySQL all-database migration with schema change support, GH-OST awareness for MySQL all-database migration, the addition of 4 batch import modes (CSV, SQL, JSON, and Excel), simplification of command line tool for creating data stream configurations, and refactoring of the Dashboard layout. This version also includes a large number of other features, mainly consisting of:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-module">Agent Module<a href="#agent-module" class="hash-link" aria-label="Direct link to Agent Module" title="Direct link to Agent Module">​</a></h3><ul><li>Support for sending data directly to Kafka, bypassing DataProxy</li><li>Agent optimization, improving file collection and transmission performance</li><li>Fixed the issue of Reader creation failure during MySQL data collection</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dataproxy-module">DataProxy Module<a href="#dataproxy-module" class="hash-link" aria-label="Direct link to DataProxy Module" title="Direct link to DataProxy Module">​</a></h3><ul><li>Simplify common configuration and related control logic</li><li>Code optimization, cleaning up invalid configurations in ConfigManager</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-module">Manager Module<a href="#manager-module" class="hash-link" aria-label="Direct link to Manager Module" title="Direct link to Manager Module">​</a></h3><ul><li>Add PostgreSQL and Redis data node management</li><li>Add heartbeat timeout status for data sources</li><li>Add 4 batch import modes: CSV, SQL, JSON, and Excel</li><li>Simplify command-line tools, including data stream creation logic</li><li>Support restarting and stopping data source tasks in data streams</li><li>Add connectivity tests for Redis and Kudu</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-module">Sort Module<a href="#sort-module" class="hash-link" aria-label="Direct link to Sort Module" title="Direct link to Sort Module">​</a></h3><ul><li>Mask sensitive information of Flink SQL-related data sources/targets in logs</li><li>Optimize the logic for calculating object byte size and related metrics</li><li>Support extracting DDL and operations from raw data</li><li>Add rate control when writing to Iceberg</li><li>Support schema changes in MySQL full-database migration</li><li>In full-database migration, MySQL Connector supports detecting GH-OST DDLs</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dashboard-module">Dashboard Module<a href="#dashboard-module" class="hash-link" aria-label="Direct link to Dashboard Module" title="Direct link to Dashboard Module">​</a></h3><ul><li>Add CSV, SQL, JSON, and Excel batch import pages for 4 import modes</li><li>Optimize Clickhouse flow configuration, supporting ttl/engine and other configurations</li><li>Refactor Dashboard layout, improving display experience</li><li>Optimize the creation process for data sources and data targets</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="other">Other<a href="#other" class="hash-link" aria-label="Direct link to Other" title="Direct link to Other">​</a></h3><ul><li>Fix multiple MySQL-related security vulnerabilities</li><li>TubeMQ Golang SDK supports production, completing the first phase of development</li><li>Optimize InLong development tool support for MacOS and Linux</li><li>Optimize Pulsar Client dependency to reduce installation package size</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="170-feature-introduction">1.7.0 Feature Introduction<a href="#170-feature-introduction" class="hash-link" aria-label="Direct link to 1.7.0 Feature Introduction" title="Direct link to 1.7.0 Feature Introduction">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-for-sending-data-directly-to-kafka">Support for sending data directly to Kafka<a href="#support-for-sending-data-directly-to-kafka" class="hash-link" aria-label="Direct link to Support for sending data directly to Kafka" title="Direct link to Support for sending data directly to Kafka">​</a></h3><p>In previous versions, InLong supported sending data directly from Agent to Pulsar without going through DataProxy. With this design, users with simple data scenarios and who want to ensure data integrity as much as possible can reduce their dependency on DataProxy. For users accustomed to using Kafka, version 1.7.0 supports the feature of sending data directly from Agent to Kafka. Thanks to @wangpeix for the complete contribution, and details can be found in INLONG-7783. If you want to experience this feature, you can choose "Send to MQ, and respond after MQ receives" during the data stream approval process.</p><p><img loading="lazy" alt="1.7.0-kafka-stream" src="/assets/images/1.7.0-kafka-stream-881ad8ffdf3ab2db4ad849fed1c9deb0.png" width="796" height="306" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="mysql-all-database-migration-with-schema-change-support">MySQL all-database migration with schema change support<a href="#mysql-all-database-migration-with-schema-change-support" class="hash-link" aria-label="Direct link to MySQL all-database migration with schema change support" title="Direct link to MySQL all-database migration with schema change support">​</a></h3><p>As community users delve deeper into using InLong, the drawbacks of not supporting schema changes become increasingly apparent. After the source end changes the DDL, the task needs to modify the configuration and restart, significantly increasing operational costs. In the current version, InLong supports automatic schema change capabilities. Upstream data sources can perceive Create, Alter, Drop, Truncate, and Rename DDL operations and synchronize these operations downstream. Meanwhile, downstream data sources can respond to upstream DDL changes and process them accordingly, supporting different processing strategies. For more details, refer to INLONG-7553. Thanks to @Emsnap, @yunqingmoswu, and @lordcheng10 for contributing to this feature. The following diagram illustrates the core process:</p><p><img loading="lazy" alt="1.7.0-mysql-schema" src="/assets/images/1.7.0-mysql-schema-dcac4a64b00ceca7b15bda71124bb92a.png" width="2118" height="598" class="img_ev3q"></p><p>In the database, DDL messages are perceived by Debezium in the CDC. At this point, the data obtained is a single DDL statement, such as "DROP TABLE A". This statement is a field in the Debezium JSON. The DDL statement is then parsed into a DDL model by JSQLParser. This model parses common DDL messages and processes them into a JSON format that is easy for the program to handle. The DDL model will be sent to the Sink Operator in Flink as data, and the Operator will process the DDL model.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="gh-ost-awareness-for-mysql-all-database-migration">GH-OST awareness for MySQL all-database migration<a href="#gh-ost-awareness-for-mysql-all-database-migration" class="hash-link" aria-label="Direct link to GH-OST awareness for MySQL all-database migration" title="Direct link to GH-OST awareness for MySQL all-database migration">​</a></h3><p>GH-OST (GitHub Online Schema Migration) is a trigger-free online schema migration solution for MySQL released by GitHub. It is testable and provides pause, dynamic control/reconfiguration, auditing, and many operational privileges. Throughout the migration process, it generates minimal workload on the primary server and is separated from the existing work on the migrated tables. By supporting GH-OST-aware DDL, the MySQL Connector can correctly handle table structure changes caused by GH-OST while capturing data changes. Thanks to @e-mhui for the complete contribution, and details of this feature can be found in INLONG-7554. The following diagram illustrates the core process:</p><p><img loading="lazy" alt="1.7.0-mysql-ghost" src="/assets/images/1.7.0-mysql-ghost-c636b0dc81e758f18e5b73e00701bbe8.png" width="1500" height="1099" class="img_ev3q"></p><p>First, after enabling the automatic DDL response for MySQL CDC, the ghc, gho, and del tables generated by GH-OST are also captured. Second, when perceiving the changes made by GH-OST to the gho table, the gho table in the DDL statement is replaced with the source table and stored in the state. Finally, after GH-OST completes the entire change process for the source table, the DDL statements previously stored in the state are sent downstream.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="addition-of-4-batch-import-modes-csv-sql-json-and-excel">Addition of 4 batch import modes (CSV, SQL, JSON, and Excel)<a href="#addition-of-4-batch-import-modes-csv-sql-json-and-excel" class="hash-link" aria-label="Direct link to Addition of 4 batch import modes (CSV, SQL, JSON, and Excel)" title="Direct link to Addition of 4 batch import modes (CSV, SQL, JSON, and Excel)">​</a></h3><p>When creating data stream input metadata fields, we need to enter information such as name, type, and description in sequence. If we need to input hundreds or thousands of field information, this processing method is extremely inefficient. In version 1.7.0, InLong has added four batch import modes for CSV/SQL/JSON/Excel formats. Users only need to refer to the template for each format and fill in the selected information to achieve one-time import. Many thanks to @featzhang and @fuweng11 for their participation in the development of this feature. The four batch import modes are now supported by both front-end and back-end, and you can download the latest version for direct use.</p><p><img loading="lazy" alt="1.7.0-batch-add" src="/assets/images/1.7.0-batch-add-cb20760ddfd10a324740d7cab2eb6037.png" width="634" height="873" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="simplification-of-command-line-tool-for-creating-data-stream-configurations">Simplification of command line tool for creating data stream configurations<a href="#simplification-of-command-line-tool-for-creating-data-stream-configurations" class="hash-link" aria-label="Direct link to Simplification of command line tool for creating data stream configurations" title="Direct link to Simplification of command line tool for creating data stream configurations">​</a></h3><p>In the before version, when creating a data stream using the command line, the JSON file content required was complex, and the file structure was not clear enough, making the threshold for users to create data streams through the command line very high. Additionally, when users wanted to reuse the file to create a new data stream, they had to modify many repetitive fields, such as inlongGroupID and inlongStreamID. In version 1.7.0, InLong has optimized the data stream configuration JSON structure and field configuration. Users can simply add Source/Sink content according to their data stream requirements, making creating a data stream much simpler than before. For more details, see INLONG-7778, and many thanks to @haifxu for the contribution.
The following example is a template for creating a File -&gt; Pulsar -&gt; Clickhouse in the new version:</p><div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token property">"groupInfo"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token property">"inlongGroupId"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"test_group_ctl"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token property">"inlongClusterTag"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"default_cluster"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token property">"mqType"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"PULSAR"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token property">"streamInfo"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token property">"inlongStreamId"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"test_stream_ctl"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token property">"fieldList"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token property">"fieldName"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"name"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token property">"fieldType"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"string"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token property">"sourceList"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token property">"sourceType"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"FILE"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token property">"sourceName"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"test_source_ctl"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token property">"agentIp"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"127.0.0.1"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token property">"pattern"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"/data/test.txt"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token property">"sinkList"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token property">"sinkType"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"CLICKHOUSE"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token property">"sinkName"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"test_sink_ctl"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token property">"dataNodeName"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"test_clickhouse"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token property">"dbName"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"db_test"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token property">"tableName"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"table_test"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token property">"flushInterval"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token property">"flushRecord"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token number">1000</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token property">"retryTimes"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token property">"engine"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"Log"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token property">"isDistributed"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token property">"sinkFieldList"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                        </span><span class="token property">"sourceFieldName"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"name"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                        </span><span class="token property">"sourceFieldType"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"string"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                        </span><span class="token property">"fieldName"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"name"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                        </span><span class="token property">"fieldType"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"string"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                </span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="refactoring-of-the-dashboard-layout">Refactoring of the Dashboard layout<a href="#refactoring-of-the-dashboard-layout" class="hash-link" aria-label="Direct link to Refactoring of the Dashboard layout" title="Direct link to Refactoring of the Dashboard layout">​</a></h3><p>In version 1.7.0, the community has refactored the overall layout of the Dashboard, including adjusting the top-bottom layout to a left-right layout (moving the navigation bar to the left), adding a dark theme, adding icons to the main menu, adjusting the data source selection display and process, etc. This adjustment has improved the user experience of the Dashboard. Special thanks to @leezng and @bluewang for their contributions. For more details, see INLONG-7734.</p><p><img loading="lazy" alt="1.7.0-dashboard-refactor" src="/assets/images/1.7.0-dashboard-refactor-db0bc2de4ad3b4d7b958dc9011e41222.png" width="2483" height="623" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="follow-up-planning">Follow-up planning<a href="#follow-up-planning" class="hash-link" aria-label="Direct link to Follow-up planning" title="Direct link to Follow-up planning">​</a></h2><p>In version 1.7.0, the community has also improved the performance and stability of Agent file collection, while TubeMQ has completed the first phase of the Golang SDK production. Additionally, Sort can now consume using the subscription groups allocated by the Manager. In subsequent versions, InLong will support multiple Apache Flink versions, including not only the current Flink 1.13 but also Flink 1.15. Furthermore, tenant management will be added to unify the models of InLong projects, users, and resources. We look forward to more developers participating and contributing.</p>]]></content>
        <author>
            <name>Charles Zhang</name>
            <uri>https://github.com/dockerzhang</uri>
        </author>
        <category label="Apache InLong" term="Apache InLong"/>
        <category label="Version" term="Version"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Release 1.6.0]]></title>
        <id>https://inlong.apache.org/blog/2023/03/23/release-1.6.0</id>
        <link href="https://inlong.apache.org/blog/2023/03/23/release-1.6.0"/>
        <updated>2023-03-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache InLong recently released version 1.6.0, which closed about 202+ issues, including 11+ major features and 80+ optimizations. Mainly completed the addition of Kudu data stream, improvement of Redis data stream, the addition of MQ cache cluster selector strategy, optimization of Audit ID allocation rules, the addition of data node connection testing, optimization of Sort Audit reconciliation benchmark time, and expansion of Audit support for using Kafka to cache audit data.]]></summary>
        <content type="html"><![CDATA[<p>Apache InLong recently released version 1.6.0, which closed about 202+ issues, including 11+ major features and 80+ optimizations. Mainly completed the addition of Kudu data stream, improvement of Redis data stream, the addition of MQ cache cluster selector strategy, optimization of Audit ID allocation rules, the addition of data node connection testing, optimization of Sort Audit reconciliation benchmark time, and expansion of Audit support for using Kafka to cache audit data.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-apache-inlong">About Apache InLong<a href="#about-apache-inlong" class="hash-link" aria-label="Direct link to About Apache InLong" title="Direct link to About Apache InLong">​</a></h2><p>As the industry's first one-stop open-source massive data integration framework, Apache InLong provides automatic, safe, reliable, and high-performance data transmission capabilities to facilitate businesses to build stream-based data analysis, modeling, and applications quickly. At present, InLong is widely used in various industries such as advertising, payment, social networking, games, artificial intelligence, etc., serving thousands of businesses, among which the scale of high-performance scene data exceeds 1 trillion lines per day, and the scale of high-reliability scene data exceeds 10 trillion lines per day.</p><p>The core keywords of InLong project positioning are "one-stop" and "massive data". For "one-stop", we hope to shield technical details, provide complete data integration and support services, and implement out-of-the-box; With its advantages, such as multi-cluster management, it can stably support larger-scale data volumes based on trillions lines per day.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="160-overview">1.6.0 Overview<a href="#160-overview" class="hash-link" aria-label="Direct link to 1.6.0 Overview" title="Direct link to 1.6.0 Overview">​</a></h2><p>Apache InLong recently released version 1.6.0, which closed about 202+ issues, including 11+ major features and 80+ optimizations. Mainly completed the addition of Kudu data stream, improvement of Redis data stream, the addition of MQ cache cluster selector strategy, optimization of Audit ID allocation rules, the addition of data node connection testing, optimization of Sort Audit reconciliation benchmark time, and expansion of Audit support for using Kafka to cache audit data, mainly including the following:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-module">Agent Module<a href="#agent-module" class="hash-link" aria-label="Direct link to Agent Module" title="Direct link to Agent Module">​</a></h3><ul><li>Improved the file collection stability and fixed multiple collection bugs</li><li>Fixed multiple bugs such as MQTT, MongoDB</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dataproxy-module">DataProxy Module<a href="#dataproxy-module" class="hash-link" aria-label="Direct link to DataProxy Module" title="Direct link to DataProxy Module">​</a></h3><ul><li>Added MQ cluster Selector strategy to reduce the number of producers</li><li>Added Audit report for the new MQ Sink</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-module">Manager Module<a href="#manager-module" class="hash-link" aria-label="Direct link to Manager Module" title="Direct link to Manager Module">​</a></h3><ul><li>Optimized the Audit ID distribution rules and supported multiple Load Data Node audit</li><li>Optimized Clickhouse data node metadata configuration and management</li><li>Added connection test for new Data Node, check the availability of nodes</li><li>Added Pulsar Multi-Cluster Topic and Subscription Management</li><li>Fixed multiple data stream management and status management bugs</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-module">Sort Module<a href="#sort-module" class="hash-link" aria-label="Direct link to Sort Module" title="Direct link to Sort Module">​</a></h3><ul><li>Added data Audit for Kafka Source connector</li><li>Added new CSV format and dirty data archive for Doris connector</li><li>Supported Array, Map, Struct, and other complex types</li><li>Optimized Pulsar Connector to solve the issue of data loss</li><li>Fixed the writing in chaos for Canal-JSON metadata fields</li><li>Optimized reconciliation benchmark time for Sort Audit</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dashboard-module">Dashboard Module<a href="#dashboard-module" class="hash-link" aria-label="Direct link to Dashboard Module" title="Direct link to Dashboard Module">​</a></h3><ul><li>Continue to optimize the Dashboard experience and reduce the threshold for first users</li><li>Added node management for Redis, Kudu, and other data nodes</li><li>Optimized data node parameters such as PostgreSQL, Kafka, Redis, etc.</li><li>Simplified the Agent node IP selection strategy</li><li>Added connection test pages for data nodes</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="other">Other<a href="#other" class="hash-link" aria-label="Direct link to Other" title="Direct link to Other">​</a></h3><ul><li>Supported using Kafka as cache MQ for Audit</li><li>Audit uniformly obtains the MQ cluster from Manager</li><li>Optimized deployment steps such as Standalone, Docker-Compose, Kubernetes, and other deployment steps</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="160-feature-introduction">1.6.0 Feature Introduction<a href="#160-feature-introduction" class="hash-link" aria-label="Direct link to 1.6.0 Feature Introduction" title="Direct link to 1.6.0 Feature Introduction">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="supported-kudu-data-stream">Supported Kudu data stream<a href="#supported-kudu-data-stream" class="hash-link" aria-label="Direct link to Supported Kudu data stream" title="Direct link to Supported Kudu data stream">​</a></h3><p>Apache Kudu is an open-source storage engine by Cloudera, which can provide low-delayed random read and write and efficient data analysis capabilities simultaneously. In version 1.6.0, InLong supports the Kudu data stream, including adding Kudu Connector, metadata management, metrics, etc. Kudu data stream contributed by @featzhang independently. Interested users can make an installation and experience.
<img loading="lazy" alt="1.6.0-create-kudu" src="/assets/images/1.6.0-create-kudu-cb72fe296daffb2196841d6ca42e8146.png" width="1488" height="1063" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="improved-the-redis-data-stream">Improved the Redis data stream<a href="#improved-the-redis-data-stream" class="hash-link" aria-label="Direct link to Improved the Redis data stream" title="Direct link to Improved the Redis data stream">​</a></h3><p>Redis is a viral open-source memory database with high performance and rich data structure. In version 1.6.0, InLong perfects the Redis data stream, adding SinkFunction, metadata management, indicators, and Dashboard pages in Redis Connector. Supported data formats like Redis's PLAIN, Hash, and Bitmap and realized Redis Schema conversion through the SCHEMAMAPPING mechanism. Redis data streams through the SCHEMA mapping mode, and SCHEMA can be converted into different <!-- -->[Redis Data-Type]<!-- --> (<a href="https://redis.io/docs/data-types/tutorial/" target="_blank" rel="noopener noreferrer">https://redis.io/docs/data-types/tutorial/</a>). Redis data stream is mainly contributed to and fulfilled by @featzhang independently. For details, please refer to <!-- -->[INLONG-7060]<!-- --> (<a href="https://github.com/apache/inlong/issues/7060" target="_blank" rel="noopener noreferrer">https://github.com/apache/inlong/issues/7060</a>).
<img loading="lazy" alt="1.6.0-update-redis" src="/assets/images/1.6.0-update-redis-017cd2d29c513ca867bfef88f9bf8ea6.png" width="755" height="792" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="added-mq-cluster-selector-strategy">Added MQ cluster Selector strategy<a href="#added-mq-cluster-selector-strategy" class="hash-link" aria-label="Direct link to Added MQ cluster Selector strategy" title="Direct link to Added MQ cluster Selector strategy">​</a></h3><p>In the multi-MQ cluster scene, if DataProxy is connected to all MQ clusters simultaneously, the number of producers in the MQ cluster will surge. At the same time, the amount of metadata of ZooKeeeper is excess, which will cause OutOfMemory. In version 1.6.0, InLong increased the selector strategy of the MQ cache cluster level (mainly for Apache Pulsar). As a result, the DataProxy node can only choose some MQ clusters under the same tag for production, thereby reducing the number of producer connections and Zookeeper metadata. @Luchunliang mainly develops this feature. For details, please refer to <!-- -->[INLONG-7231]<!-- --> (<a href="https://github.com/apache/inlong/pull/7236" target="_blank" rel="noopener noreferrer">https://github.com/apache/inlong/pull/7236</a>).
<img loading="lazy" alt="1.6.0-mq-selector" src="/assets/images/1.6.0-mq-selector-80659d1793454422d4f75feaa31eaed7.png" width="1468" height="447" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimized-the-audit-id-distribution-rules">Optimized the Audit ID distribution rules<a href="#optimized-the-audit-id-distribution-rules" class="hash-link" aria-label="Direct link to Optimized the Audit ID distribution rules" title="Direct link to Optimized the Audit ID distribution rules">​</a></h3><p>In the original design of InLong Audit, the receiving and sending number of each module is an independent audit ID, which is used to record the receiving and sending number for each module. There is a defect in this scheme. If InLong Sort sorted data to multiple targets at the same time (such as writing Hive and Clickhouse), the audit ID cannot distinguish different data streams for the Sort. In version 1.6.0, the Audit ID distribution rules are optimized, the different data stream has a different audit ID, achieving data audit of multiple sorting targets for the same data stream. This feature also involves changes in Manager and Sort module. It is developed and implemented by @FuWeng11 and @EMSNAP. For details, please refer to <!-- -->[INLONG-7389]<!-- --> (<a href="https://github.com/apache/inlong/pull/7390" target="_blank" rel="noopener noreferrer">https://github.com/apache/inlong/pull/7390</a>), <!-- -->[INLONG-7232]<!-- --> (<a href="https://github.com/apache/inlong/pull/7233" target="_blank" rel="noopener noreferrer">https://github.com/apache/inlong/pull/7233</a>), and <!-- -->[INLONG-7503]<!-- --> (<a href="https://github.com/apache/inlong/pull/7552" target="_blank" rel="noopener noreferrer">https://github.com/apache/inlong/pull/7552</a>).</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'audit_sort_hive_input'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'HIVE'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'7'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'audit_sort_hive_output'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'HIVE'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'8'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'audit_sort_clickhouse_input'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'CLICKHOUSE'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'9'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'audit_sort_clickhouse_output'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'CLICKHOUSE'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'10'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'audit_sort_es_input'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'ELASTICSEARCH'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'11'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'audit_sort_es_output'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'ELASTICSEARCH'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'12'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'audit_sort_starrocks_input'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'STARROCKS'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'13'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'audit_sort_starrocks_output'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'STARROCKS'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'14'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'audit_sort_hudi_input'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'HUDI'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'15'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'audit_sort_hudi_output'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'HUDI'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'16'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'audit_sort_iceberg_input'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'ICEBERG'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'17'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'audit_sort_iceberg_output'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'ICEBERG'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'18'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'audit_sort_hbase_input'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'HBASE'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'19'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'audit_sort_hbase_output'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'HBASE'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'20'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'audit_sort_doris_input'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'DORIS'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'21'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'audit_sort_doris_output'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'DORIS'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'22'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="added-connection-test-for-new-data-node">Added connection test for new Data Node<a href="#added-connection-test-for-new-data-node" class="hash-link" aria-label="Direct link to Added connection test for new Data Node" title="Direct link to Added connection test for new Data Node">​</a></h3><p>In the previous version, InLong added data nodes and registered MQ clusters, and did not judge the availability of the cluster. In the new version, InLong adds a connection test for the main data node and InLong system component registration, which enhances the data stream creation. This feature is mainly participated in development by @leosanqing, @Bluewang, and @Fuweng11.
<img loading="lazy" alt="1.6.0-connection-test" src="/assets/images/1.6.0-connection-test-ef3deff138b5d316c137aa4e027550a9.png" width="889" height="794" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimized-reconciliation-benchmark-time-for-sort-audit">Optimized reconciliation benchmark time for Sort Audit<a href="#optimized-reconciliation-benchmark-time-for-sort-audit" class="hash-link" aria-label="Direct link to Optimized reconciliation benchmark time for Sort Audit" title="Direct link to Optimized reconciliation benchmark time for Sort Audit">​</a></h3><p>Before version 1.6.0, the Sort reconciliation benchmark time is the machine time for data processing. Using this time will cause the full-link reconciliation data to be inaccurate. In this version, the Sort audit reconciliation referred to the design of TimestampedCollector in Apache Flink, replaced the Simple Collector in the original Pulsar Connector, and aligned the audit index. The implementation principle is to set the Timestamp field at the Collector. When obtaining the InLongMsg data transmitted by DataProxy, the reset data time is used as the Audit time record metric when sending the message downstream. The optimized Sort audit index can be aligned with other modules, such as DataProxy, this feature was mainly developed by @Emsnap.
<img loading="lazy" alt="1.6.0-sort-audit-time" src="/assets/images/1.6.0-sort-audit-time-357db69618d9a1ff04731d1effd0ecfb.png" width="1904" height="947" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="supported-using-kafka-as-cache-mq-for-audit">Supported using Kafka as cache MQ for Audit<a href="#supported-using-kafka-as-cache-mq-for-audit" class="hash-link" aria-label="Direct link to Supported using Kafka as cache MQ for Audit" title="Direct link to Supported using Kafka as cache MQ for Audit">​</a></h3><p>InLong Audit is an independent subsystem that conducts real-time audit reconciliation of the InLong system's Agent, DataProxy, and Sort module's inflow, and outflow. The current amount of audits is minute, hour, and day. In the previous version, InLong Audit only supports the use of Pulsar to cache audit data, and this will increase the cost of user deployment when they chose Kafka. In the entire InLong Audit design, the choice of MQ type should be consistent with the data stream to avoid different use of different use MQ types. In version 1.6.0, in order to achieve the use of the same type of MQ service in Audit modules and data stream, Audit supports the use of Kafka cache audit data to achieve the unity of MQ service selection. This feature is mainly completed by @haifxu and @dockerzhang.
<img loading="lazy" alt="1.6.0-audit-kafka" src="/assets/images/1.6.0-audit-kafka-120f70dcf6449309dd9824c5e4d7faa1.png" width="843" height="732" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="follow-up-planning">Follow-up planning<a href="#follow-up-planning" class="hash-link" aria-label="Direct link to Follow-up planning" title="Direct link to Follow-up planning">​</a></h2><p>In 1.6.0, the Sort module also fixes multiple bugs such as dirty data archives, metrics, and Connectors. The Dashboard continues to optimize display and approval processes and other experience problems. For more details, please refer to the Release <!-- -->[Changelog]<!-- --> (https: // github. com/Apache/InLong/Blob/Master/Changes.md). In the subsequent version, Apache InLong will add Schema dynamic change, Schema batch import, agent installation, adding more data nodes, looking forward to more developers participating in contributions.</p>]]></content>
        <author>
            <name>Charles Zhang</name>
            <uri>https://github.com/dockerzhang</uri>
        </author>
        <category label="Apache InLong" term="Apache InLong"/>
        <category label="Version" term="Version"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Release 1.5.0]]></title>
        <id>https://inlong.apache.org/blog/2023/01/13/release-1.5.0</id>
        <link href="https://inlong.apache.org/blog/2023/01/13/release-1.5.0"/>
        <updated>2023-01-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache InLong recently released version 1.5.0, which closed about 296+ issues, including 12+ major features and 110+ optimizations. Mainly completed the addition of StarRocks, Hudi, Doris, Elasticsearch, and other sinks, optimization of the Dashboard experience, refactor the MQ management model, support dirty data processing, full-link Apache Kafka support, and TubeMQ C++/Python SDK support for production, etc.]]></summary>
        <content type="html"><![CDATA[<p>Apache InLong recently released version 1.5.0, which closed about 296+ issues, including 12+ major features and 110+ optimizations. Mainly completed the addition of StarRocks, Hudi, Doris, Elasticsearch, and other sinks, optimization of the Dashboard experience, refactor the MQ management model, support dirty data processing, full-link Apache Kafka support, and TubeMQ C++/Python SDK support for production, etc.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-apache-inlong">About Apache InLong<a href="#about-apache-inlong" class="hash-link" aria-label="Direct link to About Apache InLong" title="Direct link to About Apache InLong">​</a></h2><p>As the industry's first one-stop open-source massive data integration framework, Apache InLong provides automatic, safe, reliable, and high-performance data transmission capabilities to facilitate businesses to build stream-based data analysis, modeling, and applications quickly. At present, InLong is widely used in various industries such as advertising, payment, social networking, games, artificial intelligence, etc., serving thousands of businesses, among which the scale of high-performance scene data exceeds 1 trillion/day, and the scale of high-reliability scene data exceeds 10 trillion/day sky.</p><p>The core keywords of InLong project positioning are "one-stop" and actual "massive data". For "one-stop", we hope to shield technical details, provide complete data integration and support services, and realize out-of-the-box; With its advantages, such as multi-cluster management, it can stably support larger-scale data volumes on the basis of trillions/day.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="150-overview">1.5.0 Overview<a href="#150-overview" class="hash-link" aria-label="Direct link to 1.5.0 Overview" title="Direct link to 1.5.0 Overview">​</a></h2><p>Apache InLong recently released version 1.5.0, which closed about 296+ issues, including 12+ major features and 110+ optimizations. Mainly completed the addition of StarRocks, Hudi, Doris, Elasticsearch, and other data stream sinks, optimization of the Dashboard experience, reconstruction of the MQ management model, addition of dirty data processing, full-link Apache Kafka support, and TubeMQ C++/Python SDK support for production, etc. This version has also completed a large number of other features, mainly including the following:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-module">Agent Module<a href="#agent-module" class="hash-link" aria-label="Direct link to Agent Module" title="Direct link to Agent Module">​</a></h3><ul><li>Support log collection in CVM scenarios</li><li>Added direct sending Pulsar, sending DataProxy synchronous and asynchronous strategies</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dataproxy-module">DataProxy Module<a href="#dataproxy-module" class="hash-link" aria-label="Direct link to DataProxy Module" title="Direct link to DataProxy Module">​</a></h3><ul><li>Refactor the MQ management model to support the rapid expansion of new MQ types</li><li>Optimized caching layer to support Apache Kafka message queue</li><li>Added support for BufferQueueChannel</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tubemq-module">TubeMQ Module<a href="#tubemq-module" class="hash-link" aria-label="Direct link to TubeMQ Module" title="Direct link to TubeMQ Module">​</a></h3><ul><li>Increase data sending and receiving delay statistics</li><li>TubeMQ C++ SDK supports the produce</li><li>TubeMQ Python SDK supports the produce</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-module">Manager Module<a href="#manager-module" class="hash-link" aria-label="Direct link to Manager Module" title="Direct link to Manager Module">​</a></h3><ul><li>Added Hudi data node and data stream management</li><li>Added StarRocks data node and data stream management</li><li>Optimize Elasticsearch data node and data stream management</li><li>Added data conversion management in Manager Client</li><li>Optimize Apache Kafka message queue management</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-module">Sort Module<a href="#sort-module" class="hash-link" aria-label="Direct link to Sort Module" title="Direct link to Sort Module">​</a></h3><ul><li>The MySQL Load node inventory phase supports concurrent reading of tables without primary keys</li><li>Added StarRocks, Hudi, Doris, Elasticsearch 5.x data flow support</li><li>Add dirty data processing for Doris, PostgreSQL, Hive, HBase, Elasticsearch, etc.</li><li>Upgraded Iceberg to version 1.1.0</li><li>StarRocks, PostgreSQL, Doris, Hudi and other flows support table-level indicators</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dashboard-module">Dashboard Module<a href="#dashboard-module" class="hash-link" aria-label="Direct link to Dashboard Module" title="Direct link to Dashboard Module">​</a></h3><ul><li>Experience optimization with more than 50 optimization points</li><li>Add JSON, Key-Value, and AVRO formats</li><li>Support ClickHouse, Iceberg, Elasticsearch, MySQL, and other data node management pages</li><li>Added SQLServer, Oracle, MongoDB, and MQTT data source pages</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="other">Other<a href="#other" class="hash-link" aria-label="Direct link to Other" title="Direct link to Other">​</a></h3><ul><li>Add Spotless code formatting plugin and response pipeline</li><li>Docker-compose comes with Apache Flink environment</li><li>Added Grafana indicator display templates for Agent and DataProxy</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="150-feature-introduction">1.5.0 Feature Introduction<a href="#150-feature-introduction" class="hash-link" aria-label="Direct link to 1.5.0 Feature Introduction" title="Direct link to 1.5.0 Feature Introduction">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-starrocks-hudi-doris-elasticsearch-sinks">Support StarRocks, Hudi, Doris, Elasticsearch Sinks<a href="#support-starrocks-hudi-doris-elasticsearch-sinks" class="hash-link" aria-label="Direct link to Support StarRocks, Hudi, Doris, Elasticsearch Sinks" title="Direct link to Support StarRocks, Hudi, Doris, Elasticsearch Sinks">​</a></h3><p>In version 1.5.0, InLong expanded the new data node Connector, supported StarRocks, Hudi, Doris, Elasticsearch, and other flow directions for community user scenarios, and expanded the data warehouse and lake scenarios. These new data nodes are mainly contributed by @liaorui, @featzhang, @kuansix, @LvJiancheng, and other developers.
<img loading="lazy" alt="1.5.0-create-hudi-source" src="/assets/images/1.5.0-create-hudi-source-d1860467d7a91d8c068aab055f4028c5.png" width="1332" height="1228" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimization-of-the-dashboard-experience">Optimization of the Dashboard Experience<a href="#optimization-of-the-dashboard-experience" class="hash-link" aria-label="Direct link to Optimization of the Dashboard Experience" title="Direct link to Optimization of the Dashboard Experience">​</a></h3><p>Compared with traditional data integration projects, InLong has added concepts such as Group, Stream, and data nodes. Community users using Dashboard for the first time will be confused about the whole process. To reduce the cost of using Dashboard users, InLong has made a lot of optimizations for the Dashboard front-end page, with more than 50 optimization points, and adjusted the concept, process, and display. The figure below shows the process of creating a Stream in 1.5.0, which is more simplified than the previous version. Special thanks to @leezng, @bluewang, @kinfuy for optimizing the Dashboard and @Charles Zhang for the modification suggestions.
<img loading="lazy" alt="1.5.0-create-dashboard-stream" src="/assets/images/1.5.0-create-dashboard-stream-9a3f955dc1e9854da4f2d81442587665.png" width="1955" height="553" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="refactor-the-mq-management-model">Refactor the MQ Management Model<a href="#refactor-the-mq-management-model" class="hash-link" aria-label="Direct link to Refactor the MQ Management Model" title="Direct link to Refactor the MQ Management Model">​</a></h3><p>To quickly support new message queue services (such as RocketMQ) to implement plug-ins, and unify the existing support for Pulsar, Kafka, and TubeMQ, in version 1.5.0, InLong DataProxy refactored the MQ management model, and all MQ types are based on <code>MessageQueueHandler </code> Implement the corresponding <code>Handler</code>. Thanks to @woofyzhao and @luchunliang for the implementation of this feature. If you need to develop a new MQ type, you can refer to the DataProxy plug-in guide.
<img loading="lazy" alt="1.5.0-mq-handler" src="/assets/images/1.5.0-mq-handler-de6522ff228c5a35d74e661da04dee25.png" width="959" height="511" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-dirty-data-processing">Support Dirty Data Processing<a href="#support-dirty-data-processing" class="hash-link" aria-label="Direct link to Support Dirty Data Processing" title="Direct link to Support Dirty Data Processing">​</a></h3><p>If there is dirty data that does not meet the data specifications (such as field range exceeding, missing data fields, etc.) when entering the lake into the warehouse, it may cause the user task to fail to write and restart continuously. In version 1.5.0, InLong supports storing unrecoverable dirty data in external storage, including S3 and local logs. At the same time, users can customize the output port of dirty data and can configure "whether to enable dirty data archiving" and "whether to ignore write entry error", as follows to design a UML diagram for dirty data archiving. The realization of this feature is thanks to the support of @yunqingmoswu and @Yizhou-Yang.
<img loading="lazy" alt="1.5.0-dirty-data" src="/assets/images/1.5.0-dirty-data-f0d44e031631c4128e14293d7fa0ec01.png" width="1500" height="577" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-apache-kafka-full-link">Support Apache Kafka Full-link<a href="#support-apache-kafka-full-link" class="hash-link" aria-label="Direct link to Support Apache Kafka Full-link" title="Direct link to Support Apache Kafka Full-link">​</a></h3><p>In version 1.5.0, the DataProxy, Manager, Sort, and Dashboard modules have completed the full-link support for Apache Kafka. The support for Kafka has gone through two versions, and it is available for production in 1.5.0. When users create data streams Just choose Kafka. The implementation of this feature is thanks to @woofyzhao, @fuweng11, @haifxu for their support.
<img loading="lazy" alt="1.5.0-support-kafka" src="/assets/images/1.5.0-support-kafka-3b9ddf99ca3d5779906e72865fdfe18a.png" width="1869" height="573" class="img_ev3q"></p><p>For more details on the 1.5.0 release, please refer to the release notes, which detail the features, enhancements, and bug fixes for this release.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="follow-up-planning">Follow-up planning<a href="#follow-up-planning" class="hash-link" aria-label="Direct link to Follow-up planning" title="Direct link to Follow-up planning">​</a></h2><p>In the following versions, Apache InLong will add multi-tenant management, standardize data flow, resources, and permissions of projects, clusters, and users, and optimize the performance and stability of various data sources, Agent management, etc., expect more developers to participate and contribute.</p>]]></content>
        <author>
            <name>Charles Zhang</name>
            <uri>https://github.com/dockerzhang</uri>
        </author>
        <category label="Apache InLong" term="Apache InLong"/>
        <category label="Version" term="Version"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Release 1.4.0]]></title>
        <id>https://inlong.apache.org/blog/2022/11/16/release-1.4.0</id>
        <link href="https://inlong.apache.org/blog/2022/11/16/release-1.4.0"/>
        <updated>2022-11-16T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache InLong is a one-stop integration framework for massive data that provides automatic, secure and reliable data transmission capabilities.]]></summary>
        <content type="html"><![CDATA[<p>Apache InLong is a one-stop integration framework for massive data that provides automatic, secure and reliable data transmission capabilities.
InLong offers great power to build data analysis, modeling and other real-time applications based on streaming data.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="140-version-overview">1.4.0 version overview<a href="#140-version-overview" class="hash-link" aria-label="Direct link to 1.4.0 version overview" title="Direct link to 1.4.0 version overview">​</a></h2><p>Apache InLong recently released version 1.4.0, which closed about 364+ issues with 16+ features and 120+ optimizations. It mainly completes the real-time synchronization of the entire database to Apache Doris and the real-time synchronization of the entire database to Apache Iceberg, the standard architecture supports HTTP reporting, and the standard architecture adds MongoDB and other collection nodes. This release also completes a number of other features, including:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-module">Agent Module<a href="#agent-module" class="hash-link" aria-label="Direct link to Agent Module" title="Direct link to Agent Module">​</a></h3><ul><li>Refactored sink-sending metrics</li><li>Audit report increases data size</li><li>Support Redis, MQTT, SQLServer, Oracle, and MongoDB data sources</li><li>Enhanced Kubernetes environment file collection capability</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dataproxy-module">DataProxy module<a href="#dataproxy-module" class="hash-link" aria-label="Direct link to DataProxy module" title="Direct link to DataProxy module">​</a></h3><ul><li>Heartbeat reporting adds service status and supports authentication</li><li>Added proxy-send mode to send data</li><li>Optimized data link buried point indicators</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tubemq-module">TubeMQ module<a href="#tubemq-module" class="hash-link" aria-label="Direct link to TubeMQ module" title="Direct link to TubeMQ module">​</a></h3><ul><li>Added client load balancing consumer group control API</li><li>C++ SDK fixes multiple bugs</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-module">Manager module<a href="#manager-module" class="hash-link" aria-label="Direct link to Manager module" title="Direct link to Manager module">​</a></h3><ul><li>Data stream Group and Stream support extended parameters</li><li>Client supports updating and deleting data flow through Key</li><li>Refactored the way to obtain Sort cluster configuration information</li><li>Optimized state management</li><li>Client supports cluster addition, deletion, modification, and query</li><li>Cluster nodes report new protocol types</li><li>Cache layer usage supports using Kafka</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-module">Sort module<a href="#sort-module" class="hash-link" aria-label="Direct link to Sort module" title="Direct link to Sort module">​</a></h3><ul><li>Support debezium-json format</li><li>Kafka data nodes support topic dynamic awareness</li><li>Connectors such as Hive/Hbase/Iceberg support indicator status recovery</li><li>Elasticsearch 6/7, JDBC connector added indicator status</li><li>Iceberg sink supports schema revolution, can automatically build tables, and perceive the increase of fields</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dashboard-module">Dashboard module<a href="#dashboard-module" class="hash-link" aria-label="Direct link to Dashboard module" title="Direct link to Dashboard module">​</a></h3><ul><li>Unified data source, data flow type definition</li><li>Added Agent type for cluster management</li><li>Add data node management</li><li>Support selection of Kafka message type</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="other">Other<a href="#other" class="hash-link" aria-label="Direct link to Other" title="Direct link to Other">​</a></h3><ul><li>docker-compose deploys the built-in Flink environment</li><li>Fix multiple aarch64 mirror bugs</li><li>Fix multiple dependency security bugs</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="140-version-feature-introduction">1.4.0 version feature introduction<a href="#140-version-feature-introduction" class="hash-link" aria-label="Direct link to 1.4.0 version feature introduction" title="Direct link to 1.4.0 version feature introduction">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-adds-a-variety-of-data-sources">Agent adds a variety of data sources<a href="#agent-adds-a-variety-of-data-sources" class="hash-link" aria-label="Direct link to Agent adds a variety of data sources" title="Direct link to Agent adds a variety of data sources">​</a></h3><p>In version 1.4.0, Agent supports data sources such as Redis, MQTT, SQLServer, Oracle, MongoDB, etc., so that the collection capabilities of standard architecture and lightweight architecture are basically aligned and users have more choices in massive scenarios. The support of this part of the back-end capabilities is mainly completed by @iamsee123 and @haibo-duan, and the front-end part is completed by @bluewang.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="improve-component-metrics">Improve component metrics<a href="#improve-component-metrics" class="hash-link" aria-label="Direct link to Improve component metrics" title="Direct link to Improve component metrics">​</a></h3><p>In version 1.4.0, the Agent, DataProxy, and Sort modules all have indicators optimized and improved, including the reconstruction of the indicators sent by the Agent, increasing the dimension of the data Group/Stream indicators, and fixing the inaccuracy of the Prometheus Listener indicators. Thanks to @Keylchen, @pocozh, and others for their contributions.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimize-docker-compose-deployment">Optimize Docker-compose deployment<a href="#optimize-docker-compose-deployment" class="hash-link" aria-label="Direct link to Optimize Docker-compose deployment" title="Direct link to Optimize Docker-compose deployment">​</a></h3><p>There are many InLong service components, and there has always been a problem with high deployment thresholds. In version 1.4.0, the compatibility of docker-compose deployment is optimized, and an Apache Flink environment is built-in to help developers quickly start creating tasks. Thanks to @dockerzhang for optimizing this part.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimize-heartbeat-management">Optimize heartbeat management<a href="#optimize-heartbeat-management" class="hash-link" aria-label="Direct link to Optimize heartbeat management" title="Direct link to Optimize heartbeat management">​</a></h3><p>In version 1.4.0, a lot of optimizations have been made for the heartbeat of service components, including adding data protocol when reporting, automatic registration of Agent/DataProxy component reporting, adding heartbeat management API to Manager, and optimizing multiple heartbeat status bugs. Thanks to @gosonzhang, @GanfengTan, @pocozh, @lucaspeng12138 and @haifxu for their contributions.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-real-time-synchronization-of-the-entire-database">Support real-time synchronization of the entire database<a href="#support-real-time-synchronization-of-the-entire-database" class="hash-link" aria-label="Direct link to Support real-time synchronization of the entire database" title="Direct link to Support real-time synchronization of the entire database">​</a></h3><p>In version 1.4.0, InLong began to support real-time synchronization of the entire database to follow up on the needs of community users. Currently, it is the first to achieve real-time synchronization of the entire database to Doris and real-time synchronization of the entire database to Iceberg/Kafka/Doris. In the near future, the community will also realize the synchronization of the entire database. Share the details. Thanks to @thesumery, @EMsnap, @yunqingmoswu for their contributions.</p><p>For more details on the 1.4.0 release, please refer to the release notes, which detail the features, enhancements, and bug fixes for this release.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="follow-up-planning">Follow-up planning<a href="#follow-up-planning" class="hash-link" aria-label="Direct link to Follow-up planning" title="Direct link to Follow-up planning">​</a></h3><p>In the next version, the community will continue to add synchronization scenarios for the entire database, improve task indicators, increase system stability, and conduct stress tests on standard architectures and lightweight architectures.</p>]]></content>
        <author>
            <name>Charles Zhang</name>
            <uri>https://github.com/dockerzhang</uri>
        </author>
        <category label="Apache InLong" term="Apache InLong"/>
        <category label="Version" term="Version"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Release 1.3.0]]></title>
        <id>https://inlong.apache.org/blog/2022/09/05/release-1.3.0</id>
        <link href="https://inlong.apache.org/blog/2022/09/05/release-1.3.0"/>
        <updated>2022-09-05T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache InLong is a one-stop integration framework for massive data that provides automatic, secure and reliable data transmission capabilities.]]></summary>
        <content type="html"><![CDATA[<p>Apache InLong is a one-stop integration framework for massive data that provides automatic, secure and reliable data transmission capabilities.
InLong supports both batch and stream data processing at the same time, which offers great power to build data analysis, modeling and other real-time applications based on streaming data.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="130-features-overview">1.3.0 Features Overview<a href="#130-features-overview" class="hash-link" aria-label="Direct link to 1.3.0 Features Overview" title="Direct link to 1.3.0 Features Overview">​</a></h2><p><strong>The just-released 1.3.0 version closes about 410+ issues, contains 110+ features and 170+ optimizations.</strong>
Mainly include the following:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="enhance-management-and-control-capabilities">Enhance management and control capabilities<a href="#enhance-management-and-control-capabilities" class="hash-link" aria-label="Direct link to Enhance management and control capabilities" title="Direct link to Enhance management and control capabilities">​</a></h3><ul><li>Added permission authentication for Open Api</li><li>Added cluster heartbeat mechanism for Agent and DataProxy</li><li>Manager adapts to two roles such as user and approver</li><li>Abstract operations on Load nodes to support easy scaling of Load node resources</li><li>Supports creation of databases and tables for SQLServer, Oracle and MySQL</li><li>Enhanced functionality of the Manager client, including but not limited to user and data node management</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="extended-collection-node">Extended collection node<a href="#extended-collection-node" class="hash-link" aria-label="Direct link to Extended collection node" title="Direct link to Extended collection node">​</a></h3><ul><li>Support for collecting data in TubeMq</li><li>Support for collecting data in Redis</li><li>Support for collecting data in Doris</li><li>Support to collect data in Pulsar without AdminUrl</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimize-write-node">Optimize write node<a href="#optimize-write-node" class="hash-link" aria-label="Direct link to Optimize write node" title="Direct link to Optimize write node">​</a></h3><ul><li>Kafka Sink supports All Changelog Mode</li><li>JDBC Sink supports All Changelog Mode</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-data-conversion">Support data conversion<a href="#support-data-conversion" class="hash-link" aria-label="Direct link to Support data conversion" title="Direct link to Support data conversion">​</a></h3><ul><li>Support Union operator</li><li>Support encrypted Udf</li><li>Support Json Udf</li><li>Support Temporal Join</li><li>Support Lookup Join</li><li>Support Interval Join</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="strengthen-agent-function">Strengthen Agent function<a href="#strengthen-agent-function" class="hash-link" aria-label="Direct link to Strengthen Agent function" title="Direct link to Strengthen Agent function">​</a></h3><ul><li>Support regular expression custom line break: default "\n" line ending mark, custom regular matching line ending mark can realize multi-line merging and folding</li><li>Support K8s log collection and carry cluster information</li><li>Supports standard output, node log collection, and will carry container and cluster information for standard output</li><li>Support full and incremental collection of file content</li><li>Supports automatic heartbeat reporting and registration to Manager</li><li>Support custom IP and get IP automatically</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="other-optimizations">Other optimizations<a href="#other-optimizations" class="hash-link" aria-label="Direct link to Other optimizations" title="Direct link to Other optimizations">​</a></h3><ul><li>GitHub Action check, pipeline optimization</li><li>DataProxy improves monitoring capabilities such as auditing and indicator reporting</li><li>DataProxy adds c++ sdk data reporting capability</li><li>Sort Support metrics report and audit report </li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="130-features-details">1.3.0 Features Details<a href="#130-features-details" class="hash-link" aria-label="Direct link to 1.3.0 Features Details" title="Direct link to 1.3.0 Features Details">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="abstracting-load-node-operations">Abstracting Load node operations<a href="#abstracting-load-node-operations" class="hash-link" aria-label="Direct link to Abstracting Load node operations" title="Direct link to Abstracting Load node operations">​</a></h3><p>Manager abstracts Load nodes to support easy expansion of Load node resources and greatly reduce the development time of a Load node
This part of the feature was contributed by @ciscozhou</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="add-permission-authentication-for-manager-open-api">Add permission authentication for Manager Open Api<a href="#add-permission-authentication-for-manager-open-api" class="hash-link" aria-label="Direct link to Add permission authentication for Manager Open Api" title="Direct link to Add permission authentication for Manager Open Api">​</a></h3><p>In the old version, the Manager Open Api can be accessed anonymously, and in the new version, it is implemented using the Apache Shiro framework.
Login authentication method based on Basic Access Authentication, this part of the function was contributed by @woofyzhao</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="enhanced-collection-of-file-data-and-k8s-logs">Enhanced collection of file data and k8s logs<a href="#enhanced-collection-of-file-data-and-k8s-logs" class="hash-link" aria-label="Direct link to Enhanced collection of file data and k8s logs" title="Direct link to Enhanced collection of file data and k8s logs">​</a></h3><p>Version 1.3.0 enhances the collection of file data and k8s data, in which file collection supports regular expression custom line breaks, so that multiple lines can be merged and folded
In addition, the new version of Agent supports full and incremental collection of file content. This part of the function was contributed by @ganfengtan</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dataproxy-adds-c-sdk-capability">DataProxy adds c++ sdk capability<a href="#dataproxy-adds-c-sdk-capability" class="hash-link" aria-label="Direct link to DataProxy adds c++ sdk capability" title="Direct link to DataProxy adds c++ sdk capability">​</a></h3><p>In addition to the current java client, DataProxy has added c++ client capabilities, which are provided by @pocozh</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="supports-multiple-udf-and-join-operators">Supports multiple udf and join operators<a href="#supports-multiple-udf-and-join-operators" class="hash-link" aria-label="Direct link to Supports multiple udf and join operators" title="Direct link to Supports multiple udf and join operators">​</a></h3><p>The new version of Sort supports three kinds of Temporal Join\ Lookup Join \ Interval Join, this part of the function was contributed by @yunqingmoswu
Most community users mentioned the need for encryption and decryption and Json Udf, this part of the function was contributed by @Emsnap and @Emhui</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-connector-supports-indicator-reporting-function">Sort connector supports indicator reporting function<a href="#sort-connector-supports-indicator-reporting-function" class="hash-link" aria-label="Direct link to Sort connector supports indicator reporting function" title="Direct link to Sort connector supports indicator reporting function">​</a></h3><p>The new version of Sort Connector supports Flink built-in indicator reporting of various Connectors. External indicator systems such as Prometheus can directly obtain the number and rate of task data read and write.
In addition, the new version also supports InLong Audit Audit data reporting, which is contributed by @pacigong, @Emsnap, @thesumery @Oneal65 @yunqingmoswu</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-supports-the-creation-of-resources-in-multiple-flow-directions">Manager supports the creation of resources in multiple flow directions<a href="#manager-supports-the-creation-of-resources-in-multiple-flow-directions" class="hash-link" aria-label="Direct link to Manager supports the creation of resources in multiple flow directions" title="Direct link to Manager supports the creation of resources in multiple flow directions">​</a></h3><p>In version 1.3.0, Manager added the creation of some storage resources:</p><ul><li>Create Topic for SQLServer</li><li>Create Oracle libraries and tables</li><li>Create MySQL namespaces and tables</li></ul><p>The above are all contributed by community member @haibo-duan, thanks</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="other-features-and-bug-fixes">Other features and bug fixes<a href="#other-features-and-bug-fixes" class="hash-link" aria-label="Direct link to Other features and bug fixes" title="Direct link to Other features and bug fixes">​</a></h3><p>For related content, please refer to the <a href="https://github.com/apache/inlong/blob/master/CHANGES.md" target="_blank" rel="noopener noreferrer">Release Notes</a>, which details the features, enhancements and bug fixes of this release.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="apache-inlong-follow-up-planning">Apache InLong follow-up planning<a href="#apache-inlong-follow-up-planning" class="hash-link" aria-label="Direct link to Apache InLong follow-up planning" title="Direct link to Apache InLong follow-up planning">​</a></h2><p>In subsequent versions, we will expand more data sources and storages to cover more usage scenarios, and gradually improve the usability and robustness of the system, including:</p><ul><li>Agent adds Redis, CloudEvents, MongoDB collection types</li><li>Unified DataProxy MQ framework</li><li>Full support for Apache Kafka</li></ul>]]></content>
        <author>
            <name>EMsnap</name>
            <uri>https://github.com/EMsnap</uri>
        </author>
        <category label="Apache InLong" term="Apache InLong"/>
        <category label="Version" term="Version"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Release 1.2.0]]></title>
        <id>https://inlong.apache.org/blog/2022/06/22/release-1.2.0</id>
        <link href="https://inlong.apache.org/blog/2022/06/22/release-1.2.0"/>
        <updated>2022-06-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache InLong is a one-stop integration framework for massive data that provides automatic, secure and reliable data transmission capabilities.]]></summary>
        <content type="html"><![CDATA[<p>Apache InLong is a one-stop integration framework for massive data that provides automatic, secure and reliable data transmission capabilities.
InLong supports both batch and stream data processing at the same time, which offers great power to build data analysis, modeling and other real-time applications based on streaming data.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="120-features-overview">1.2.0 Features Overview<a href="#120-features-overview" class="hash-link" aria-label="Direct link to 1.2.0 Features Overview" title="Direct link to 1.2.0 Features Overview">​</a></h2><p><strong>The just-released 1.2.0-incubating version closes about 410+ issues, contains 30+ features and 190+ optimizations.</strong>
Mainly include the following:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="enhance-management-and-control-capabilities">Enhance management and control capabilities<a href="#enhance-management-and-control-capabilities" class="hash-link" aria-label="Direct link to Enhance management and control capabilities" title="Direct link to Enhance management and control capabilities">​</a></h3><ul><li>Dashboard and Manager add cluster management capabilities</li><li>Dashboard optimizes the flow creation process</li><li>Manager supports plug-in extension of MQ</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="extended-collection-node">Extended collection node<a href="#extended-collection-node" class="hash-link" aria-label="Direct link to Extended collection node" title="Direct link to Extended collection node">​</a></h3><ul><li>Support for collecting data in Pulsar</li><li>Support data collection in MongoDB-CDC</li><li>Support data collection in MySQL-CDC</li><li>Support data collection in Oracle-CDC</li><li>Support data collection in PostgreSQL-CDC</li><li>Support data collection in SQLServer-CDC</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="extended-write-node">Extended write node<a href="#extended-write-node" class="hash-link" aria-label="Direct link to Extended write node" title="Direct link to Extended write node">​</a></h3><ul><li>Support for writing data to Kafka</li><li>Support for writing data to HBase</li><li>Support for writing data to PostgreSQL</li><li>Support for writing data to Oracle</li><li>Supports writing data to MySQL</li><li>Support writing data to TDSQL-PostgreSQL</li><li>Support for writing data to Greenplum</li><li>Supports writing data to SQLServer</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-data-conversion">Support data conversion<a href="#support-data-conversion" class="hash-link" aria-label="Direct link to Support data conversion" title="Direct link to Support data conversion">​</a></h3><ul><li>Support String Split</li><li>Support String Regular Replace</li><li>Support String Regular Replace First Matched Value</li><li>Support Data Filter</li><li>Support Data Distinct</li><li>Support Regular Join</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="enhanced-system-monitoring-function">Enhanced system monitoring function<a href="#enhanced-system-monitoring-function" class="hash-link" aria-label="Direct link to Enhanced system monitoring function" title="Direct link to Enhanced system monitoring function">​</a></h3><ul><li>Support the reporting and management of data link heartbeat</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="other-optimizations">Other optimizations<a href="#other-optimizations" class="hash-link" aria-label="Direct link to Other optimizations" title="Direct link to Other optimizations">​</a></h3><ul><li>Supports the delivery of DataProxy multi-cluster configurations</li><li>GitHub Action check, pipeline optimization</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="120-features-details">1.2.0 Features Details<a href="#120-features-details" class="hash-link" aria-label="Direct link to 1.2.0 Features Details" title="Direct link to 1.2.0 Features Details">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-multi-cluster-management">Support multi-cluster management<a href="#support-multi-cluster-management" class="hash-link" aria-label="Direct link to Support multi-cluster management" title="Direct link to Support multi-cluster management">​</a></h3><p>Manager adds cluster management function, supports multi-cluster configuration, and solves the limitation that only one set of clusters can be defined through configuration files.
Users can create different types of clusters on Dashboard as needed.</p><p>The multi-cluster feature is mainly designed and implemented by @healchow, @luchunliang, @leezng, thanks to three contributors.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="enhanced-collection-of-file-data-and-mysql-binlog">Enhanced collection of file data and MySQL Binlog<a href="#enhanced-collection-of-file-data-and-mysql-binlog" class="hash-link" aria-label="Direct link to Enhanced collection of file data and MySQL Binlog" title="Direct link to Enhanced collection of file data and MySQL Binlog">​</a></h3><p>Version 1.2.0 supports collecting complete file data, and also supports collecting data from the specified Binlog location in MySQL. This part of the work was done by @Greedyu.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-whole-database-migration">Support whole database migration<a href="#support-whole-database-migration" class="hash-link" aria-label="Direct link to Support whole database migration" title="Direct link to Support whole database migration">​</a></h3><p>Sort supports migration of data across the entire database, contributed by @EMsnap.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="supports-writing-data-in-canal-format">Supports writing data in Canal format<a href="#supports-writing-data-in-canal-format" class="hash-link" aria-label="Direct link to Supports writing data in Canal format" title="Direct link to Supports writing data in Canal format">​</a></h3><p>Support for writing data in Canal format to Kafka, contributed by @thexiay.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimize-the-http-request-method-in-manager-client">Optimize the HTTP request method in Manager Client<a href="#optimize-the-http-request-method-in-manager-client" class="hash-link" aria-label="Direct link to Optimize the HTTP request method in Manager Client" title="Direct link to Optimize the HTTP request method in Manager Client">​</a></h3><p>Optimized the way of executing HTTP requests in Manager Client, and added unit tests for Client, which reduces maintenance costs while reducing duplication of code.
This feature was contributed by new contributor @leosanqing.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="supports-running-sql-scripts">Supports running SQL scripts<a href="#supports-running-sql-scripts" class="hash-link" aria-label="Direct link to Supports running SQL scripts" title="Direct link to Supports running SQL scripts">​</a></h3><p>Sort supports running SQL scripts, see <a href="https://github.com/apache/inlong/issues/4405" target="_blank" rel="noopener noreferrer">INLONG-4405</a>, thanks to @gong for contributing this feature.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-the-reporting-and-management-of-data-link-heartbeat">Support the reporting and management of data link heartbeat<a href="#support-the-reporting-and-management-of-data-link-heartbeat" class="hash-link" aria-label="Direct link to Support the reporting and management of data link heartbeat" title="Direct link to Support the reporting and management of data link heartbeat">​</a></h3><p>This version supports the heartbeat reporting and management of data grouping, data flow and underlying components, which is the premise of the state management of each link of the subsequent system.</p><p>This feature was primarily designed and contributed by @baomingyu, @healchow and @kipshi.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-supports-the-creation-of-resources-in-multiple-flow-directions">Manager supports the creation of resources in multiple flow directions<a href="#manager-supports-the-creation-of-resources-in-multiple-flow-directions" class="hash-link" aria-label="Direct link to Manager supports the creation of resources in multiple flow directions" title="Direct link to Manager supports the creation of resources in multiple flow directions">​</a></h3><p>In version 1.2.0, Manager added the creation of some storage resources:</p><ul><li>Create Topic for Kafka (contributed by @woofyzhao)</li><li>Create databases and tables for Iceberg (contributed by @woofyzhao)</li><li>Create namespaces and tables for HBase (contributed by @woofyzhao)</li><li>Create databases and tables for ClickHouse (contributed by @lucaspeng12138)</li><li>Create indices for Elasticsearch (contributed by @lucaspeng12138)</li><li>Create databases and tables for PostgreSQL (contributed by @baomingyu)</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-supports-lightweight-architecture">Sort supports lightweight architecture<a href="#sort-supports-lightweight-architecture" class="hash-link" aria-label="Direct link to Sort supports lightweight architecture" title="Direct link to Sort supports lightweight architecture">​</a></h3><p>Version 1.2.0 of Sort has done a lot of refactoring and improvements.
By introducing Flink-CDC, it supports a variety of Extract and Load nodes, and also supports data transformation (ie Transform).</p><p>This feature contains many sub-features. The main developers are:
@baomingyu, @EMsnap, @GanfengTan, @gong, @lucaspeng12138, @LvJiancheng, @kipshi, @thexiay, @woofyzhao, @yunqingmoswu, thank you all for your contributions.</p><p>For more information, please refer to: <a href="/blog/2022/06/16/inlong-sort-etl">Analysis of InLong Sort ETL Solution</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="other-features-and-bug-fixes">Other features and bug fixes<a href="#other-features-and-bug-fixes" class="hash-link" aria-label="Direct link to Other features and bug fixes" title="Direct link to Other features and bug fixes">​</a></h3><p>For related content, please refer to the <a href="https://github.com/apache/inlong/blob/master/CHANGES.md" target="_blank" rel="noopener noreferrer">Release Notes</a>, which details the features, enhancements and bug fixes of this release.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="apache-inlong-follow-up-planning">Apache InLong follow-up planning<a href="#apache-inlong-follow-up-planning" class="hash-link" aria-label="Direct link to Apache InLong follow-up planning" title="Direct link to Apache InLong follow-up planning">​</a></h2><p>In subsequent versions, we will expand more data sources and storages to cover more usage scenarios, and gradually improve the usability and robustness of the system, including:</p><ul><li>Heartbeat report of each component</li><li>Status management of data flow</li><li>Full link audit support for writing to ClickHouse</li><li>Expand more types of acquisition nodes and storage nodes</li></ul>]]></content>
        <author>
            <name>healchow</name>
            <uri>https://github.com/healchow</uri>
        </author>
        <category label="Apache InLong" term="Apache InLong"/>
        <category label="Version" term="Version"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of InLong Sort ETL Solution]]></title>
        <id>https://inlong.apache.org/blog/2022/06/16/inlong-sort-etl</id>
        <link href="https://inlong.apache.org/blog/2022/06/16/inlong-sort-etl"/>
        <updated>2022-06-16T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[1. Background]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-background">1. Background<a href="#1-background" class="hash-link" aria-label="Direct link to 1. Background" title="Direct link to 1. Background">​</a></h2><p>With the increasing number of users and developers of Apache InLong(incubating), the demand for richer usage scenarios and low-cost operation is getting stronger and stronger. Among them, the demand for adding Transform (T) to the whole link of InLong has received the most feedback. After the research and design of @yunqingmoswu, @EMsnap, @gong, @thexiay community developers, the InLong Sort ETL solution based on Flink SQL has been completed. This article will introduce the implementation details of the solution in detail.</p><p>Firstly, based on Apache Flink SQL, there are mainly the following considerations：</p><ul><li>Flink SQL has high scalability and flexibility brought about by its powerful expression ability. Basically, Flink SQL can support most demand scenarios in the community. When the built-in functions of Flink SQL do not meet the requirements, we can also extend them through various UDFs.</li><li>Compared with the implementation of the underlying API of Flink, the development cost of Flink SQL is lower. Only for the first time, the conversion logic of Flink SQL needs to be implemented. In the future, we can focus on the construction of the ability of Flink SQL, such as the extension connector and the UDF.</li><li>In general, Flink SQL will be more robust and run more stable. The reason is that Flink SQL shields a lot of the underlying details of Flink, has strong community support, and has been practiced by a large number of users.</li><li>For users, Flink SQL is also easier to understand, especially for users who have used SQL, the usage is simple and familiar, which helps users to land quickly.</li><li>For the migration of existing real-time tasks, if they are originally SQL-type tasks, especially Flink SQL tasks, the migration cost is extremely low, and in some cases, no changes are even required.</li></ul><p><strong>Note</strong>: For all codes of this scheme, please refer to <a href="https://github.com/apache/incubator-inlong/tree/master/inlong-sort" target="_blank" rel="noopener noreferrer">Apache InLong Sort</a>, which can be downloaded and used in the upcoming version 1.2.0.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-introduction">2. Introduction<a href="#2-introduction" class="hash-link" aria-label="Direct link to 2. Introduction" title="Direct link to 2. Introduction">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="21-requirements">2.1 Requirements<a href="#21-requirements" class="hash-link" aria-label="Direct link to 2.1 Requirements" title="Direct link to 2.1 Requirements">​</a></h3><p>The main requirements of this solution are the completed inlong sort module transform (T) capability, including:</p><table><thead><tr><th align="center">Transform</th><th align="center">Notes</th></tr></thead><tbody><tr><td align="center">Deduplication in the window</td><td align="center">Deduplicate data within a time window</td></tr><tr><td align="center">time window aggregation</td><td align="center">Aggregate data within a time window</td></tr><tr><td align="center">time format conversion</td><td align="center">Converts the value of a field to a string in the target time format</td></tr><tr><td align="center">field segmentation</td><td align="center">Split a field into multiple new fields by a delimiter</td></tr><tr><td align="center">string replacement</td><td align="center">Replace some or all of the contents of a string field</td></tr><tr><td align="center">Data filtering</td><td align="center">Discard or retain data that meets the filter conditions</td></tr><tr><td align="center">Content extraction</td><td align="center">Extract part of a field to create a new field</td></tr><tr><td align="center">Join</td><td align="center">Support two table join</td></tr><tr><td align="center">Value substitution</td><td align="center">Given a matching value, if the field's value is equal to that value, replace it with the target value</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="22-usage-scenarios">2.2 Usage Scenarios<a href="#22-usage-scenarios" class="hash-link" aria-label="Direct link to 2.2 Usage Scenarios" title="Direct link to 2.2 Usage Scenarios">​</a></h3><p>Users of big data integration have transform requirements such as data transformation, connection and filtering in many business scenarios.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="23-design-goal">2.3 Design Goal<a href="#23-design-goal" class="hash-link" aria-label="Direct link to 2.3 Design Goal" title="Direct link to 2.3 Design Goal">​</a></h3><p>This design needs to achieve the following goals:</p><ul><li>Functionality: Under InLong Sort's existing architecture and data flow model, it covers basic Transform capabilities and has the ability to expand rapidly.</li><li>Compatibility: The new InLong Sort data model is forward compatible to ensure that historical tasks can be configured and run properly.</li><li>Maintainability: The conversion of the InLong Sort data model to Flink SQL only needs to be implemented once. When there are new functional requirements later, this part does not need to be changed, even if there are changes, it can be supported with a small amount of changes.</li><li>Extensibility: When the open source Flink Connector or the built-in Flink SQL function does not meet the requirements, you can customize the Flink Connector and UDF to achieve its function expansion.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="24-basic-concepts">2.4 Basic Concepts<a href="#24-basic-concepts" class="hash-link" aria-label="Direct link to 2.4 Basic Concepts" title="Direct link to 2.4 Basic Concepts">​</a></h3><p>The core concept refers to the explanation of terms in the outline design</p><table><thead><tr><th align="center">Name</th><th align="center">Meaning</th></tr></thead><tbody><tr><td align="center">InLong Dashboard</td><td align="center">Inlong front end management interface</td></tr><tr><td align="center">InLong Manager Client</td><td align="center">Wrap the interface in the manager for external user programs to call without going through the front-end inlong dashboard</td></tr><tr><td align="center">InLong Manager Openapi</td><td align="center">Inlong manager and external system call interface</td></tr><tr><td align="center">InLong Manager metaData</td><td align="center">Inlong manager metadata management, including metadata information of group and stream dimensions</td></tr><tr><td align="center">InLong Manager task manager</td><td align="center">Inlong manager manages the data source collection task module, manages agent task distribution, instruction distribution, and heartbeat reporting</td></tr><tr><td align="center">InLong Group</td><td align="center">Data flow group, including multiple data flows, one group represents one data access</td></tr><tr><td align="center">InLong Stream</td><td align="center">Data flow: a data flow has a specific flow direction</td></tr><tr><td align="center">Stream Source</td><td align="center">There are corresponding acquisition end and sink end in the stream. This design only involves the stream source</td></tr><tr><td align="center">Stream Info</td><td align="center">Abstract of data flow in sort, including various sources, transformations, destinations, etc. of the data flow</td></tr><tr><td align="center">Group Info</td><td align="center">Encapsulation of data flow in sort. A group info can contain multiple stream infos</td></tr><tr><td align="center">Node</td><td align="center">Abstraction of data source, data transformation and data destination in data synchronization</td></tr><tr><td align="center">Extract Node</td><td align="center">Source side abstraction of data synchronization</td></tr><tr><td align="center">Load Node</td><td align="center">Destination abstraction of data synchronization</td></tr><tr><td align="center">MySQL Extract Node</td><td align="center">MySQL data source abstraction</td></tr><tr><td align="center">Kafka Load Node</td><td align="center">Kafka data destination abstraction</td></tr><tr><td align="center">Transform Node</td><td align="center">Transformation process abstraction of data synchronization</td></tr><tr><td align="center">Aggregate Transform Node</td><td align="center">Data synchronization aggregation class transformation process abstraction</td></tr><tr><td align="center">Node Relation</td><td align="center">Relationship abstraction of nodes in data synchronization</td></tr><tr><td align="center">Field Relation</td><td align="center">Abstraction of the relationship between upstream and downstream node fields in data synchronization</td></tr><tr><td align="center">Function</td><td align="center">Abstraction of the relationship between upstream and downstream node fields in data synchronization</td></tr><tr><td align="center">Substring Function</td><td align="center">Abstraction of string interception function</td></tr><tr><td align="center">Filter Function</td><td align="center">Abstraction of data filter function</td></tr><tr><td align="center">Function Param</td><td align="center">Input parameter abstraction of function</td></tr><tr><td align="center">Constant Param</td><td align="center">Constant parameters</td></tr><tr><td align="center">Field Info</td><td align="center">Node field</td></tr><tr><td align="center">Meta FieldInfo</td><td align="center">Node meta information field</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="25-domain-model">2.5 Domain Model<a href="#25-domain-model" class="hash-link" aria-label="Direct link to 2.5 Domain Model" title="Direct link to 2.5 Domain Model">​</a></h3><p>This design mainly involves the following entities: </p><p>Group, Stream, GroupInfo, StreamInfo, Node, NodeRelation, FieldRelation, Function, FilterFunction, SubstringFunction, FunctionParam, FieldInfo, MetaFieldInfo, MySQLExtractNode, KafkaLoadNode, etc.</p><p>For ease of understanding, this section will model and analyze the relationship between entities. Description of entity correspondence of domain model:</p><ul><li>One group corresponds to one group info</li><li>A group contains one or more streams</li><li>One stream corresponds to one StreamInfo</li><li>A GroupInfo contains one or more StreamInfo</li><li>A StreamInfo contains multiple nodes</li><li>A StreamInfo contains one or more NodeRelations</li><li>A NodeRelation contains one or more FieldRelations</li><li>A NodeRelation contains 0 or more FilterFunctions</li><li>A FieldRelation contains one function or one FieldInfo as the source field and one FieldInfo as the target field</li><li>A function contains one or more FunctionParams</li></ul><p>The above relationship can be represented by UML object relationship diagram as:</p><p><img loading="lazy" alt="sort_UML" src="/assets/images/sort_UML-896d751427509d769add998680df9516.png" width="2576" height="869" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="26-function-use-case-diagram">2.6 Function Use-case Diagram<a href="#26-function-use-case-diagram" class="hash-link" aria-label="Direct link to 2.6 Function Use-case Diagram" title="Direct link to 2.6 Function Use-case Diagram">​</a></h3><p><img loading="lazy" alt="sort-usecase" src="/assets/images/sort-usecase-fb8639f9724899ab3afcbf35b8a21902.png" width="606" height="356" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-system-outline-design">3. System Outline Design<a href="#3-system-outline-design" class="hash-link" aria-label="Direct link to 3. System Outline Design" title="Direct link to 3. System Outline Design">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="31-system-architecture-diagram">3.1 System Architecture Diagram<a href="#31-system-architecture-diagram" class="hash-link" aria-label="Direct link to 3.1 System Architecture Diagram" title="Direct link to 3.1 System Architecture Diagram">​</a></h3><p><img loading="lazy" alt="architecture" src="/assets/images/architecture-b4c0fb3783a6ed2f2868f534df98e74b.png" width="461" height="741" class="img_ev3q"></p><ul><li>Serialization: Serialization Implementation Module</li><li>Deserialization: Deserialization Implementation Module</li><li>Flink Source: Custom Flink source implementation module</li><li>Flink Sink: Custom Flink sink implementation module</li><li>Transformation: Custom Transform implementation module</li><li>GroupInfo: Corresponding to Inlong group</li><li>StreamInfo: Corresponding to inlong stream</li><li>Node: Abstraction of data source, data conversion and data destination in data synchronization</li><li>FlinkSQLParser: SQL parser</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="32-inlong-sort-internal-operation-flow-chart">3.2 InLong Sort Internal Operation Flow Chart<a href="#32-inlong-sort-internal-operation-flow-chart" class="hash-link" aria-label="Direct link to 3.2 InLong Sort Internal Operation Flow Chart" title="Direct link to 3.2 InLong Sort Internal Operation Flow Chart">​</a></h3><p><img loading="lazy" alt="sort-operation-flow" src="/assets/images/sort-operation-flow-77363f12a68a011beba26db9ccc6fedb.png" width="771" height="61" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="33-module-design">3.3 Module Design<a href="#33-module-design" class="hash-link" aria-label="Direct link to 3.3 Module Design" title="Direct link to 3.3 Module Design">​</a></h3><p>This design only adds Flink connector and Flink SQL generator to the original system, and modifies the data model module.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="331-module-structure">3.3.1 Module Structure<a href="#331-module-structure" class="hash-link" aria-label="Direct link to 3.3.1 Module Structure" title="Direct link to 3.3.1 Module Structure">​</a></h4><p><img loading="lazy" alt="sort-module-structure" src="/assets/images/sort-module-structure-4dd424ae93043cb912dba69c08590b33.png" width="771" height="1011" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="332-module-division">3.3.2 Module Division<a href="#332-module-division" class="hash-link" aria-label="Direct link to 3.3.2 Module Division" title="Direct link to 3.3.2 Module Division">​</a></h4><p>Description of important module division:</p><table><thead><tr><th align="center">Name</th><th align="center">Description</th></tr></thead><tbody><tr><td align="center">FlinkSQLParser</td><td align="center">Used to generate Flink SQL core classes, including references to GroupInfo</td></tr><tr><td align="center">GroupInfo</td><td align="center">The internal abstraction of sort for inlong group is used to encapsulate the synchronization related information of the entire inlong group, including the reference to list\&lt;StreamInfo<!-- -->&gt;</td></tr><tr><td align="center">StreamInfo</td><td align="center">The internal abstraction of sort to inlong stream is used to encapsulate inlong stream synchronization related information, including references to list\&lt;node<!-- -->&gt;<!-- -->, list\&lt;NodeRelation<!-- -->&gt;</td></tr><tr><td align="center">Node</td><td align="center">The top-level interface of the synchronization node. Its subclass implementation is mainly used to encapsulate the data of the synchronization data source and the transformation node</td></tr><tr><td align="center">ExtractNode</td><td align="center">Data extract node abstraction, inherited from node</td></tr><tr><td align="center">LoadNode</td><td align="center">Data load node abstraction, inherited from node</td></tr><tr><td align="center">TransformNode</td><td align="center">Data transformation node abstraction, inherited from node</td></tr><tr><td align="center">NodeRelation</td><td align="center">Define relationships between nodes</td></tr><tr><td align="center">FieldRelation</td><td align="center">Define field relationships between nodes</td></tr><tr><td align="center">Function</td><td align="center">Abstract of T-ability execution function</td></tr><tr><td align="center">FilterFunction</td><td align="center">Function abstraction for data filtering, inherited from function</td></tr><tr><td align="center">SubstringFunction</td><td align="center">Used for string interception function abstraction, inherited from function</td></tr><tr><td align="center">FunctionParam</td><td align="center">Abstraction for function parameters</td></tr><tr><td align="center">ConstantParam</td><td align="center">Encapsulation of function constant parameters, inherited from FunctionParam</td></tr><tr><td align="center">FieldInfo</td><td align="center">The encapsulation of node fields can also be used as function input parameters, inherited from FunctionParam</td></tr><tr><td align="center">MetaFieldInfo</td><td align="center">The encapsulation of built-in fields is currently mainly used in the metadata field scenario of canal JSON, which is inherited from FieldInfo</td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-detailed-system-design">4. Detailed System Design<a href="#4-detailed-system-design" class="hash-link" aria-label="Direct link to 4. Detailed System Design" title="Direct link to 4. Detailed System Design">​</a></h2><p>The following describes the principle of SQL generation by taking MySQL synchronizing data to Kafka as an example</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="41-node-described-in-sql">4.1 Node Described in SQL<a href="#41-node-described-in-sql" class="hash-link" aria-label="Direct link to 4.1 Node Described in SQL" title="Direct link to 4.1 Node Described in SQL">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="411-extractnode-described-in-sql">4.1.1 ExtractNode Described in SQL<a href="#411-extractnode-described-in-sql" class="hash-link" aria-label="Direct link to 4.1.1 ExtractNode Described in SQL" title="Direct link to 4.1.1 ExtractNode Described in SQL">​</a></h4><p>The node configuration is:</p><div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"> private Node buildMySQLExtractNode() {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        List&lt;FieldInfo&gt; fields = Arrays.asList(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                new FieldInfo("name", new StringFormatInfo()),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                new FieldInfo("age", new IntFormatInfo()));</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        return new MySqlExtractNode("1", "mysql_input", fields,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                null, null, "id",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                Collections.singletonList("tableName"), "localhost", "root", "password",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                "inlong", null, null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                null, null);</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The generated SQL is:</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">mysql_1</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">name</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> string</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">age</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">int</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">with</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'connector'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'mysql-cdc-inlong'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token string" style="color:rgb(255, 121, 198)">'hostname'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'localhost'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token string" style="color:rgb(255, 121, 198)">'username'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'root'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token string" style="color:rgb(255, 121, 198)">'password'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'password'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token string" style="color:rgb(255, 121, 198)">'database-name'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'inlong'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token string" style="color:rgb(255, 121, 198)">'table-name'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'tableName'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="412-transformnode--described-in-sql">4.1.2 TransformNode  Described in SQL<a href="#412-transformnode--described-in-sql" class="hash-link" aria-label="Direct link to 4.1.2 TransformNode  Described in SQL" title="Direct link to 4.1.2 TransformNode  Described in SQL">​</a></h4><p>The node configuration is:</p><div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"> List&lt;FilterFunction&gt; filters = Arrays.asList(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                new SingleValueFilterFunction(EmptyOperator.getInstance(),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                        new FieldInfo("age", new IntFormatInfo()),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                        LessThanOperator.getInstance(), new ConstantParam(25)),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                new SingleValueFilterFunction(AndOperator.getInstance(),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                        new FieldInfo("age", new IntFormatInfo()),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                        MoreThanOrEqualOperator.getInstance(), new ConstantParam(18))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        );</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The generated SQL is:</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">SELECT</span><span class="token plain"> </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">name</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">AS</span><span class="token plain"> </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">name</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">age</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">AS</span><span class="token plain"> </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">age</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">FROM</span><span class="token plain"> </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">mysql_1</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">WHERE</span><span class="token plain"> </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">age</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token operator">&lt;</span><span class="token plain"> </span><span class="token number">25</span><span class="token plain"> </span><span class="token operator">AND</span><span class="token plain"> </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">age</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token operator">&gt;=</span><span class="token plain"> </span><span class="token number">18</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="413-loadnode-described-in-sql">4.1.3 LoadNode Described in SQL<a href="#413-loadnode-described-in-sql" class="hash-link" aria-label="Direct link to 4.1.3 LoadNode Described in SQL" title="Direct link to 4.1.3 LoadNode Described in SQL">​</a></h4><p>The node configuration is:</p><div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"> private Node buildKafkaLoadNode(FilterStrategy filterStrategy) {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        List&lt;FieldInfo&gt; fields = Arrays.asList(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                new FieldInfo("name", new StringFormatInfo()),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                new FieldInfo("age", new IntFormatInfo())</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        );</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        List&lt;FieldRelation&gt; relations = Arrays</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                .asList(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                        new FieldRelation(new FieldInfo("name", new StringFormatInfo()),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                                new FieldInfo("name", new StringFormatInfo())),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                        new FieldRelation(new FieldInfo("age", new IntFormatInfo()),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                                new FieldInfo("age", new IntFormatInfo()))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                );</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        List&lt;FilterFunction&gt; filters = Arrays.asList(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                new SingleValueFilterFunction(EmptyOperator.getInstance(),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                        new FieldInfo("age", new IntFormatInfo()),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                        LessThanOperator.getInstance(), new ConstantParam(25)),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                new SingleValueFilterFunction(AndOperator.getInstance(),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                        new FieldInfo("age", new IntFormatInfo()),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                        MoreThanOrEqualOperator.getInstance(), new ConstantParam(18))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        );</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        return new KafkaLoadNode("2", "kafka_output", fields, relations, filters,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                filterStrategy, "topic1", "localhost:9092",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                new CanalJsonFormat(), null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                null, "id");</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The generated SQL is:</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">kafka_3</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">name</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> string</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">age</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">int</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">with</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token string" style="color:rgb(255, 121, 198)">'connector'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'kafka-inlong'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token string" style="color:rgb(255, 121, 198)">'topic'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'topic1'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token string" style="color:rgb(255, 121, 198)">'properties.bootstrap.servers'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'localhost:9092'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token string" style="color:rgb(255, 121, 198)">'format'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'canal-json-inlong'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token string" style="color:rgb(255, 121, 198)">'canal-json-inlong.ignore-parse-errors'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'true'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token string" style="color:rgb(255, 121, 198)">'canal-json-inlong.map-null-key.mode'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'DROP'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token string" style="color:rgb(255, 121, 198)">'canal-json-inlong.encode.decimal-as-plain-number'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'true'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token string" style="color:rgb(255, 121, 198)">'canal-json-inlong.timestamp-format.standard'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'SQL'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token string" style="color:rgb(255, 121, 198)">'canal-json-inlong.map-null-key.literal'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'null'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="42-field-t-described-in-sql">4.2 Field T Described in SQL<a href="#42-field-t-described-in-sql" class="hash-link" aria-label="Direct link to 4.2 Field T Described in SQL" title="Direct link to 4.2 Field T Described in SQL">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="421-filter-operator">4.2.1 Filter operator<a href="#421-filter-operator" class="hash-link" aria-label="Direct link to 4.2.1 Filter operator" title="Direct link to 4.2.1 Filter operator">​</a></h4><p>See 4.1 node configuration for relevant configurations</p><p>The generated SQL is:</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INSERT</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INTO</span><span class="token plain"> </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">kafka_3</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">SELECT</span><span class="token plain"> </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">name</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">AS</span><span class="token plain"> </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">name</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">age</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">AS</span><span class="token plain"> </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">age</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">FROM</span><span class="token plain"> </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">mysql_1</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">WHERE</span><span class="token plain"> </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">age</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token operator">&lt;</span><span class="token plain"> </span><span class="token number">25</span><span class="token plain"> </span><span class="token operator">AND</span><span class="token plain"> </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">age</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token operator">&gt;=</span><span class="token plain"> </span><span class="token number">18</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="422-watermark">4.2.2 Watermark<a href="#422-watermark" class="hash-link" aria-label="Direct link to 4.2.2 Watermark" title="Direct link to 4.2.2 Watermark">​</a></h4><p>The complete configuration of GroupInfo is as follows:</p><div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">private Node buildMySqlExtractNode() {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        List&lt;FieldInfo&gt; fields = Arrays.asList(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                new FieldInfo("name", new StringFormatInfo()),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                new FieldInfo("age", new IntFormatInfo()),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                new FieldInfo("ts", new TimestampFormatInfo()));</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        WatermarkField wk = new WatermarkField(new FieldInfo("ts", new TimestampFormatInfo()),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                new StringConstantParam("1"),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                new TimeUnitConstantParam(TimeUnit.MINUTE));</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        return new MySqlExtractNode("1", "mysql_input", fields,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                wk, null, "id",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                Collections.singletonList("tableName"), "localhost", "root", "password",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                "inlong", null, null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                null, null);</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    private Node buildKafkaNode() {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        List&lt;FieldInfo&gt; fields = Arrays.asList(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                new FieldInfo("name", new StringFormatInfo()),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                new FieldInfo("age", new IntFormatInfo()),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                new FieldInfo("ts", new TimestampFormatInfo()));</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        List&lt;FieldRelation&gt; relations = Arrays</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                .asList(new FieldRelation(new FieldInfo("name", new StringFormatInfo()),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                                new FieldInfo("name", new StringFormatInfo())),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                        new FieldRelation(new FieldInfo("age", new IntFormatInfo()),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                                new FieldInfo("age", new IntFormatInfo()))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                );</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        return new KafkaLoadNode("2", "kafka_output", fields, relations, null, null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                "topic", "localhost:9092", new JsonFormat(),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                1, null, "id");</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    private NodeRelation buildNodeRelation(List&lt;Node&gt; inputs, List&lt;Node&gt; outputs) {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        List&lt;String&gt; inputIds = inputs.stream().map(Node::getId).collect(Collectors.toList());</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        List&lt;String&gt; outputIds = outputs.stream().map(Node::getId).collect(Collectors.toList());</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        return new NodeRelation(inputIds, outputIds);</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    @Override</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    public GroupInfo getTestObject() {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        Node input = buildMySqlExtractNode();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        Node output = buildKafkaNode();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        StreamInfo streamInfo = new StreamInfo("1", Arrays.asList(input, output), Collections.singletonList(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                buildNodeRelation(Collections.singletonList(input), Collections.singletonList(output))));</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        return new GroupInfo("1", Collections.singletonList(streamInfo));</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>]]></content>
        <author>
            <name>Oneal65</name>
            <uri>https://github.com/Oneal65</uri>
        </author>
        <category label="Apache InLong" term="Apache InLong"/>
        <category label="Sort" term="Sort"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Release 1.1.0]]></title>
        <id>https://inlong.apache.org/blog/2022/04/25/release-1.1.0</id>
        <link href="https://inlong.apache.org/blog/2022/04/25/release-1.1.0"/>
        <updated>2022-04-25T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache InLong is a one-stop integration framework for massive data that provides automatic, secure and reliable data transmission capabilities. InLong supports both batch and stream data processing at the same time, which offers great power to build data analysis, modeling and other real-time applications based on streaming data.]]></summary>
        <content type="html"><![CDATA[<p>Apache InLong is a one-stop integration framework for massive data that provides automatic, secure and reliable data transmission capabilities. InLong supports both batch and stream data processing at the same time, which offers great power to build data analysis, modeling and other real-time applications based on streaming data.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="110-features-overview">1.1.0 Features Overview<a href="#110-features-overview" class="hash-link" aria-label="Direct link to 1.1.0 Features Overview" title="Direct link to 1.1.0 Features Overview">​</a></h2><p>The 1.1.0-incubating just released mainly includes the following:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="enhanced-management-capabilities-for-manager">Enhanced management capabilities for manager<a href="#enhanced-management-capabilities-for-manager" class="hash-link" aria-label="Direct link to Enhanced management capabilities for manager" title="Direct link to Enhanced management capabilities for manager">​</a></h3><ul><li>Added Manager Client to support the integration of InLong for creating data streams</li><li>Added ManagerCtl command-line tool to support viewing and creating data streams</li><li>Manager supports dispatching Agent tasks</li><li>Manager supports dispatching Sort Flink tasks</li><li>Manger data streams management, supports start, restart, pause operations</li><li>Manager plugin support</li><li>Manager metadata management supports using MySQL</li><li>The first phase of cluster management, unified cluster information registration</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="rich-data-nodes">Rich data nodes<a href="#rich-data-nodes" class="hash-link" aria-label="Direct link to Rich data nodes" title="Direct link to Rich data nodes">​</a></h3><ul><li>Support Iceberg</li><li>Support ClickHouse</li><li>Support MySQL Binlog collection</li><li>Support Kafka ingestion</li><li>Sort Standalone supports multiple type streams</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tools-construction">Tools construction<a href="#tools-construction" class="hash-link" aria-label="Direct link to Tools construction" title="Direct link to Tools construction">​</a></h3><ul><li>Dashboard plugin support</li><li>Kubernetes deployment optimization</li><li>Standalone deployment refactoring</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="system-upgrade">System Upgrade<a href="#system-upgrade" class="hash-link" aria-label="Direct link to System Upgrade" title="Direct link to System Upgrade">​</a></h3><ul><li>Protocol Buffers upgrade</li><li>Unified version Maven version dependencies</li><li>Fixed a bunch of dependency CVEs</li><li>DataProxy supports PB compression protocol</li></ul><p>This version closed about 642+ issues, including four 23 features and 180 improvements.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="110-features-details">1.1.0 Features Details<a href="#110-features-details" class="hash-link" aria-label="Direct link to 1.1.0 Features Details" title="Direct link to 1.1.0 Features Details">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="add-manager-client">Add Manager Client<a href="#add-manager-client" class="hash-link" aria-label="Direct link to Add Manager Client" title="Direct link to Add Manager Client">​</a></h3><p>The newly added Manager Client defines common InLong Group/Stream operation interfaces, including task creation, viewing and deletion. Through Manager Client, users can build InLong into any third-party platform to achieve unified customized platform construction. The Manager Client is mainly designed and implemented by @kipshi, @gong, @ciscozhou, thanks to three contributors.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="add-managerctl-command-line-tool">Add ManagerCtl command line tool<a href="#add-managerctl-command-line-tool" class="hash-link" aria-label="Direct link to Add ManagerCtl command line tool" title="Direct link to Add ManagerCtl command line tool">​</a></h3><p>ManagerCtl is developed based on Manager Client and is a command-line tool for manipulating InLong resources, which can further simplify the use of developers. ManagerCtl was contributed independently by @haifxu and includes guidelines including:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Usage: managerctl [options] [command] [command options]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Options:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-h, --help</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Get all command about managerctl.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Commands:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">create      Create resource by json file</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Usage: create [options]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">​</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">describe      Display details of one or more resources</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Usage: describe [options]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">​</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">list      Displays main information for one or more resources</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Usage: list [options]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-supports-issuing-sort-tasks">Manager supports issuing Sort tasks<a href="#manager-supports-issuing-sort-tasks" class="hash-link" aria-label="Direct link to Manager supports issuing Sort tasks" title="Direct link to Manager supports issuing Sort tasks">​</a></h3><p>In previous versions, after the user created the data group/stream task, Sort needed to submit it to the Flink cluster through the command line, and then perform data sorting. In version 1.1.0, we unified the start, stop, and suspend operations of Sort tasks to the Manager to complete. Users only need to configure the correct Flink cluster when the Manager is deployed. When the data stream is approved, Sort will be automatically started.
This part of the work is mainly contributed by @LvJiancheng.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="manager-metadata-is-saved-to-zookeeper">Manager metadata is saved to ZooKeeper<a href="#manager-metadata-is-saved-to-zookeeper" class="hash-link" aria-label="Direct link to Manager metadata is saved to ZooKeeper" title="Direct link to Manager metadata is saved to ZooKeeper">​</a></h3><p>InLong uses ZooKeeper to save data stream metadata, which increases the deployment threshold and operation and maintenance difficulty for developers and users.
In version 1.1.0, we save data stream metadata in DB by default, and ZooKeeper is only an optional solution in large-scale scenarios. Thanks to @kipshi @yunqingmoswu for contributing a PR to ZooKeeper.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="support-mysql-binlog-collection">Support MySQL Binlog collection<a href="#support-mysql-binlog-collection" class="hash-link" aria-label="Direct link to Support MySQL Binlog collection" title="Direct link to Support MySQL Binlog collection">​</a></h3><p>Version 1.1.0 fully supports the collection of data from MySQL, and supports both incremental and full strategies. Users can collect MySQL data with simple configuration in InLong. This part of the work was contributed by @EMsnap.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-added-iceberg-clickhouse-kafka">Sort Added Iceberg, ClickHouse, Kafka<a href="#sort-added-iceberg-clickhouse-kafka" class="hash-link" aria-label="Direct link to Sort Added Iceberg, ClickHouse, Kafka" title="Direct link to Sort Added Iceberg, ClickHouse, Kafka">​</a></h3><p>In version 1.1.0, the storage of data nodes for various scenarios has been added, including Iceberg, ClickHouse, and Kafka. The support of these three streams enriches the usage scenarios of InLong. New stream support, mainly contributed by @chantccc @KevinWen007 @healchow.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-standalone-supports-hive-elasticsearch-kafka">Sort Standalone supports Hive, Elasticsearch, Kafka<a href="#sort-standalone-supports-hive-elasticsearch-kafka" class="hash-link" aria-label="Direct link to Sort Standalone supports Hive, Elasticsearch, Kafka" title="Direct link to Sort Standalone supports Hive, Elasticsearch, Kafka">​</a></h3><p>As mentioned in the previous version, for non-Flink environments, we can sort data streams through Sort Standalone. In version 1.1.0, we added support for Hive, ElasticSearch, Kafka, and expanded the usage scenarios of Sort Standalone. Sort Standalone is mainly contributed by @vernedeng @luchunliang.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="protocol-buffers-upgrade">Protocol Buffers upgrade<a href="#protocol-buffers-upgrade" class="hash-link" aria-label="Direct link to Protocol Buffers upgrade" title="Direct link to Protocol Buffers upgrade">​</a></h3><p>All InLong components Protocol Buffers dependencies have been upgraded from 2.5.0 to 3.19.4. Thanks to @gosonzhang @doleyzi for their contributions, a lot of compatibility and testing work for Protocol Buffers upgrades.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="inlong-on-kubernetes-optimization">InLong on Kubernetes optimization<a href="#inlong-on-kubernetes-optimization" class="hash-link" aria-label="Direct link to InLong on Kubernetes optimization" title="Direct link to InLong on Kubernetes optimization">​</a></h3><p>The optimization work of InLong on Kubernetes mainly includes adding Audit, combing configuration, optimizing the use of message queues, optimizing the use of documents, etc., to facilitate the use of InLong on the cloud. Thanks to @shink for contributing to these optimizations.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dashboard-plugin">Dashboard plugin<a href="#dashboard-plugin" class="hash-link" aria-label="Direct link to Dashboard plugin" title="Direct link to Dashboard plugin">​</a></h3><p>In order to facilitate users to quickly build new data stream on Dashboard, Dashboard is support plugin in version 1.1.0. JavaScript developers who understand the plugin development guidelines can quickly expand new data stream. Thanks for this part of the work @leezng</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="other-features-and-bug-fixes">Other features and bug fixes<a href="#other-features-and-bug-fixes" class="hash-link" aria-label="Direct link to Other features and bug fixes" title="Direct link to Other features and bug fixes">​</a></h3><p>For related content, please refer to the version <a href="https://github.com/apache/incubator-inlong/blob/master/CHANGES.md" target="_blank" rel="noopener noreferrer">release notes</a>, which list the features, enhancements and bug fixes of this version in detail, as well as specific contributors.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="apache-inlongincubating-follow-up-planning">Apache InLong(incubating) follow-up planning<a href="#apache-inlongincubating-follow-up-planning" class="hash-link" aria-label="Direct link to Apache InLong(incubating) follow-up planning" title="Direct link to Apache InLong(incubating) follow-up planning">​</a></h2><p>In subsequent versions, we will support lightweight Sort, and expand more data sources and targets to cover more usage scenarios, including:</p><ul><li>Flink SQL support</li><li>Elasticsearch, HBase support</li></ul>]]></content>
        <author>
            <name>dockerzhang</name>
            <uri>https://github.com/dockerzhang</uri>
        </author>
        <category label="Apache InLong" term="Apache InLong"/>
        <category label="Version" term="Version"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Release 0.12.0]]></title>
        <id>https://inlong.apache.org/blog/2022/01/04/release-0.12.0</id>
        <link href="https://inlong.apache.org/blog/2022/01/04/release-0.12.0"/>
        <updated>2022-01-04T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[InLong: the sacred animal in Chinese myths stories, draws rivers into the sea, as a metaphor for the InLong system to provide data access capabilities.]]></summary>
        <content type="html"><![CDATA[<p>InLong: the sacred animal in Chinese myths stories, draws rivers into the sea, as a metaphor for the InLong system to provide data access capabilities.</p><p>Apache InLong is a one-stop integration framework for massive data that provides automatic, secure and reliable data transmission capabilities. InLong supports both batch and stream data processing at the same time, which offers great power to build data analysis, modeling and other real-time applications based on streaming data.
The 0.12.0-incubating just released mainly includes the following:</p><ul><li>Provide automatic, safe, reliable and high-performance data transmission capabilities, while supporting batch and streaming</li><li>Refactor the document structure of the official website to facilitate users to consult related documents based on the main line</li><li>The whole process supports Pulsar data flow, and completes the whole process coverage of high-performance and high-reliability scenarios</li><li>Full-process support indicators, including JMX and Prometheus output</li><li>The first phase of data audit and reconciliation, write audit data to MySQL</li></ul><p>This version closed about 120+ issues, including four major features and 35 improvements.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="apache-inlongincubating-introduction">Apache InLong(incubating) Introduction<a href="#apache-inlongincubating-introduction" class="hash-link" aria-label="Direct link to Apache InLong(incubating) Introduction" title="Direct link to Apache InLong(incubating) Introduction">​</a></h3><p><a href="https://inlong.apache.org" target="_blank" rel="noopener noreferrer">Apache InLong</a> is a one-stop integration framework for massive data donated by Tencent to the Apache community.  It provides automatic,  safe,  reliable,  and high-performance data transmission capabilities to facilitate the construction of streaming-based data analysis,  modeling,  and applications.<br>
<!-- -->The Apache InLong project was originally called TubeMQ,  focusing on high-performance,  low-cost message queuing services.  In order to further release the surrounding ecological capabilities of TubeMQ,  we upgraded the project to InLong,  focusing on creating a one-stop integration framework for massive data.</p><p>Apache InLong uses TDBank internally used by Tencent as the prototype,  and relies on trillion-level data access and processing capabilities to integrate the entire process of data collection,  aggregation,  storage,  and sorting data processing.  It is simple to use,  flexible to expand,  stable and reliable characteristic.</p><img loading="lazy" src="/img/inlong-structure-en.png" align="center" alt="Apache InLong" class="img_ev3q"><p>Apache InLong serves the entire life cycle from data collection to landing,  and provides different processing modules according to different stages of data,  including the next modules:</p><ul><li><strong>inlong-agent</strong>,  data collection agent, supports reading regular logs from specified directories or files and reporting data one by one.  In the future,  DB collection and HTTP reporting capabilities will also be expanded.</li><li><strong>inlong-dataproxy</strong>,  a Proxy component based on Flume-ng,  supports data transmission blocking,  placing retransmission, and has the ability to forward received data to different MQ (message queues).</li><li><strong>inlong-tubemq</strong>,  Tencent's self-developed message queuing service,  focuses on high-performance storage and transmission of massive data in big data scenarios and has a relatively good core advantage in mass practice and low cost.</li><li><strong>inlong-sort</strong>,  after consuming data from different MQ services,  perform ETL processing,  and then aggregate and write the data into Apache Hive, ClickHouse,  Hbase,  IceBerg,  etc.</li><li><strong>inlong-manager</strong>, provides complete data service management and control capabilities,  including metadata,  OpenAPI,  task flow,  authority,  etc.</li><li><strong>inlong-website</strong>, a front-end page for managing data access,  simplifying the use of the entire InLong control platform.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="whats-new-in-apache-inlongincubating-0120">What’s New in Apache InLong(incubating) 0.12.0<a href="#whats-new-in-apache-inlongincubating-0120" class="hash-link" aria-label="Direct link to What’s New in Apache InLong(incubating) 0.12.0" title="Direct link to What’s New in Apache InLong(incubating) 0.12.0">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="1-support-apache-pulsar-data-cache">1. Support Apache Pulsar data cache<a href="#1-support-apache-pulsar-data-cache" class="hash-link" aria-label="Direct link to 1. Support Apache Pulsar data cache" title="Direct link to 1. Support Apache Pulsar data cache">​</a></h4><p>In version 0.12.0, we have completed the data reporting capability of FileAgent→DataProxy→Pulsar→Sort. So far, InLong supports high-performance and high-reliability data access scenarios: Compared with the high-throughput TubeMQ, Apache Pulsar can provide better data consistency and is more suitable for scenarios that require extremely high data reliability. For example, finance and billing.</p><img loading="lazy" src="/img/pulsar-arch-en.png" align="center" alt="Report via Pulsar" class="img_ev3q"><p>Thanks to @healchow, @baomingyu, @leezng, @bruceneenhl, @ifndef-SleePy and others for their contributions to this feature. For more information, please refer to <a href="https://github.com/apache/" target="_blank" rel="noopener noreferrer">INLONG-1310</a>incubator-inlong/issues/1310), please refer to <!-- -->[Pulsar usage example]<!-- -->(<a href="https://inlong.apache." target="_blank" rel="noopener noreferrer">https://inlong.apache.</a> org/zh -CN/docs/next/quick_start/pulsar_example/) to get the usage guide.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="2-support-jmx-and-prometheus-metrics">2. Support JMX and Prometheus metrics<a href="#2-support-jmx-and-prometheus-metrics" class="hash-link" aria-label="Direct link to 2. Support JMX and Prometheus metrics" title="Direct link to 2. Support JMX and Prometheus metrics">​</a></h4><p>In addition to the existing file output metrics, the various components of InLong began to gradually support the output of JMX and Prometheus metrics to improve the visibility of the entire system. Currently, modules including Agent, DataProxy, TubeMQ, Sort-Standalone, etc. already support the above metrics, and the documentation of metrics output by each module is being sorted out.</p><p>Thanks to @shink, @luchunliang, @EMsnap, @gosonzhang and others for their contributions. For related PRs, please see <a href="https://github.com/apache/incubator-inlong/issues/1712" target="_blank" rel="noopener noreferrer">INLONG-1712</a>, <!-- -->[INLONG-1786]<!-- --> (<a href="https://github.com/apache/incubator-inlong/issues/1786" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-inlong/issues/1786</a>), <a href="https://github.com/apache/incubator-inlong/issues/1796" target="_blank" rel="noopener noreferrer">INLONG-1796</a>, <!-- -->[INLONG-1827]<!-- --> (<a href="https://github.com/apache/incubator-inlong/issues/1827" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-inlong/issues/1827</a>), <a href="https://github.com/apache/incubator-inlong/issues/1851" target="_blank" rel="noopener noreferrer">INLONG-1851</a>, <!-- -->[INLONG-1926]<!-- --> (<a href="https://github.com/apache/incubator-inlong/issues/1926" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-inlong/issues/1926</a>).</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="3-function-extension-of-the-modules">3. Function extension of the modules<a href="#3-function-extension-of-the-modules" class="hash-link" aria-label="Direct link to 3. Function extension of the modules" title="Direct link to 3. Function extension of the modules">​</a></h4><p>The Sort module adds support for Apache Doris storage and realizes the ability to load sorted data into Apache Doris, giving InLong one more storage option. In addition, in order to enrich the functions of the entire data access process, the Audit and Sort-Standalone modules have been added:</p><ul><li>The Audit module provides the ability to reconcile the entire process of data flow, and monitor the service quality of the system through data flow indicators;</li><li>Sort-Standalone module is a non-Flink-based data sorting module. It adds lightweight data sorting capabilities to the system, facilitating the rapid construction and use of businesses.</li></ul><p>The Audit and Sort-Standalone modules are still under development and will be released when the version is stable.</p><p>Thanks to @huzk8, @doleyzi, @luchunliang and others for their contributions, please refer to <a href="https://github.com/apache/incubator-inlong/issues/1821" target="_blank" rel="noopener noreferrer">INLONG-1821</a>, <!-- -->[INLONG-1738]<!-- -->( https: / /github.com/apache/incubator-inlong/issues/1738), <a href="https://github.com/apache/incubator-inlong/issues/1889" target="_blank" rel="noopener noreferrer">INLONG-1889</a>.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="4-official-website-document-directory-reconstruction">4. Official website document directory reconstruction<a href="#4-official-website-document-directory-reconstruction" class="hash-link" aria-label="Direct link to 4. Official website document directory reconstruction" title="Direct link to 4. Official website document directory reconstruction">​</a></h4><p>In addition to the improvement of functional modules in version 0.12.0, the official website structure and the use of documents have also been deeply adjusted, including the reconstruction of the document directory structure, supplementing and improving the function introduction of each module, adding document translation, and further improving the documentation of the InLong official website. Readability, to achieve quick search and easy reading. You can check the official website to understand this content. The document is still under construction. We welcome your valuable comments or suggestions.</p><p>Thanks to @bluewang, @dockerzhang, @healchow and others for their contributions, please refer to <a href="https://github.com/apache/incubator-inlong/issues/1711" target="_blank" rel="noopener noreferrer">INLONG-1711</a>, <!-- -->[INLONG-1740]<!-- -->(https: //github.com/apache/incubator-inlong/issues/1740), <a href="https://github.com/apache/incubator-inlong/issues/1802" target="_blank" rel="noopener noreferrer">INLONG-1802</a>, <!-- -->[INLONG-1809]<!-- -->(https: //github.com/apache/incubator-inlong/issues/1809), <a href="https://github.com/apache/incubator-inlong/issues/1810" target="_blank" rel="noopener noreferrer">INLONG-1810</a>, <!-- -->[INLONG-1815]<!-- -->(https: //github.com/apache/incubator-inlong/issues/1815), <a href="https://github.com/apache/incubator-inlong/issues/1822" target="_blank" rel="noopener noreferrer">INLONG-1822</a>, <!-- -->[INLONG-1840]<!-- -->(https: //github.com/apache/incubator-inlong/issues/1840), <a href="https://github.com/apache/incubator-inlong/issues/1856" target="_blank" rel="noopener noreferrer">INLONG-1856</a>, <!-- -->[INLONG-1861]<!-- -->(https: //github.com/apache/incubator-inlong/issues/1861), <a href="https://github.com/apache/incubator-inlong/issues/1867" target="_blank" rel="noopener noreferrer">INLONG-1867</a>, <!-- -->[INLONG-1878]<!-- -->(https: //github.com/apache/incubator-inlong/issues/1878), <a href="https://github.com/apache/incubator-inlong/issues/1901" target="_blank" rel="noopener noreferrer">INLONG-1901</a>, <!-- -->[INLONG-1939]<!-- -->(https: //gith ub.com/apache/incubator-inlong/issues/1939).</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="5-other-features-and-bug-fixes">5. Other features and bug fixes<a href="#5-other-features-and-bug-fixes" class="hash-link" aria-label="Direct link to 5. Other features and bug fixes" title="Direct link to 5. Other features and bug fixes">​</a></h4><p>For related content, please refer to <a href="https://github.com/apache/incubator-inlong/blob/0.12.0-incubating-RC0/CHANGES.md" target="_blank" rel="noopener noreferrer">Version Release Notes</a>, which lists the detailed features of this version, Improvements, bug fixes, and specific contributors.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="apache-inlongincubating-follow-up-planning">Apache InLong(incubating) follow-up planning<a href="#apache-inlongincubating-follow-up-planning" class="hash-link" aria-label="Direct link to Apache InLong(incubating) follow-up planning" title="Direct link to Apache InLong(incubating) follow-up planning">​</a></h3><p>In subsequent versions, we will further enhance the capabilities of InLong to cover more usage scenarios, including:</p><ul><li>Support link for data access ClickHouse</li><li>Support DB data collection</li><li>The second stage full link indicator audit function</li></ul>]]></content>
        <author>
            <name>gosonzhang</name>
            <uri>https://github.com/gosonzhang</uri>
        </author>
        <category label="Apache InLong" term="Apache InLong"/>
        <category label="Version" term="Version"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Release 0.11.0]]></title>
        <id>https://inlong.apache.org/blog/2021/11/10/release-0.11.0</id>
        <link href="https://inlong.apache.org/blog/2021/11/10/release-0.11.0"/>
        <updated>2021-11-10T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache InLong (incubating)  has been renamed from the original Apache TubeMQ (incubating) from 0.9.0.  With the name change,  InLong has also been upgraded from a single message queue to a one-stop integration framework for massive data.  InLong supports data collection,  aggregation,  caching,  and sorting,  users can import data from the data source to the real-time computing engine or land to offline storage with a simple configuration.]]></summary>
        <content type="html"><![CDATA[<p>Apache InLong (incubating)  has been renamed from the original Apache TubeMQ (incubating) from 0.9.0.  With the name change,  InLong has also been upgraded from a single message queue to a one-stop integration framework for massive data.  InLong supports data collection,  aggregation,  caching,  and sorting,  users can import data from the data source to the real-time computing engine or land to offline storage with a simple configuration.</p><p>The just-released version <code>0.11.0-incubating</code> is the third version after the name changed.  This version includes next features:</p><ul><li>Lower the user's threshold for use further.  Support all modules of InLong to be deployed on Kubernetes,  and refactor the official website,  so that users can more easily access related documents.</li><li>Support more usage scenarios,  increase the data flow direction of <code>Dataproxy -&gt; Pulsar</code>,  and cover scenarios with higher requirements for data integrity and consistency.</li><li>Supports SDKs in more languages for TubeMQ.  This version opens the production-level TubeMQ Go SDK, which is convenient for users who use the Go language to access</li></ul><p>This version closed more than 80 issues, including four significant features and 35 improvements.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="apache-inlongincubating-introduction">Apache InLong(incubating) Introduction<a href="#apache-inlongincubating-introduction" class="hash-link" aria-label="Direct link to Apache InLong(incubating) Introduction" title="Direct link to Apache InLong(incubating) Introduction">​</a></h3><p><a href="https://inlong.apache.org" target="_blank" rel="noopener noreferrer">Apache InLong</a> is a one-stop integration framework for massive data donated by Tencent to the Apache community.  It provides automatic,  safe,  reliable,  and high-performance data transmission capabilities to facilitate the construction of streaming-based data analysis,  modeling,  and applications.<br>
<!-- -->The Apache InLong project was originally called TubeMQ,  focusing on high-performance,  low-cost message queuing services.  In order to further release the surrounding ecological capabilities of TubeMQ,  we upgraded the project to InLong,  focusing on creating a one-stop integration framework for massive data.</p><p>Apache InLong uses TDBank internally used by Tencent as the prototype,  and relies on trillion-level data access and processing capabilities to integrate the entire process of data collection,  aggregation,  storage,  and sorting data processing.  It is simple to use,  flexible to expand,  stable and reliable characteristic.</p><img loading="lazy" src="/img/inlong-structure-en.png" align="center" alt="Apache InLong" class="img_ev3q"><p>Apache InLong serves the entire life cycle from data collection to landing,  and provides different processing modules according to different stages of data,  including the next modules:</p><ul><li><strong>inlong-agent</strong>,  data collection agent, supports reading regular logs from specified directories or files and reporting data one by one.  In the future,  DB collection and HTTP reporting capabilities will also be expanded.</li><li><strong>inlong-dataproxy</strong>,  a Proxy component based on Flume-ng,  supports data transmission blocking,  placing retransmission, and has the ability to forward received data to different MQ (message queues).</li><li><strong>inlong-tubemq</strong>,  Tencent's self-developed message queuing service,  focuses on high-performance storage and transmission of massive data in big data scenarios and has a relatively good core advantage in mass practice and low cost.</li><li><strong>inlong-sort</strong>,  after consuming data from different MQ services,  perform ETL processing,  and then aggregate and write the data into Apache Hive, ClickHouse,  Hbase,  IceBerg,  etc.</li><li><strong>inlong-manager</strong>, provides complete data service management and control capabilities,  including metadata,  OpenAPI,  task flow,  authority,  etc.</li><li><strong>inlong-website</strong>, a front-end page for managing data access,  simplifying the use of the entire InLong control platform.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="whats-new-in-apache-inlongincubating-0110">What’s New in Apache InLong(incubating) 0.11.0<a href="#whats-new-in-apache-inlongincubating-0110" class="hash-link" aria-label="Direct link to What’s New in Apache InLong(incubating) 0.11.0" title="Direct link to What’s New in Apache InLong(incubating) 0.11.0">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="inlong-on-kubernetes">InLong on Kubernetes<a href="#inlong-on-kubernetes" class="hash-link" aria-label="Direct link to InLong on Kubernetes" title="Direct link to InLong on Kubernetes">​</a></h4><p>Apache InLong includes multiple components such as data collection,  data aggregation,  data caching,  data sorting,  and cluster management and control.  In order to facilitate users to use and support cloud-native features,  all components currently support deployment in Kubernetes.
Thanks to @shink for contributing to this feature.  For specific details,  please refer to <a href="https://github.com/apache/incubator-inlong/issues/1308" target="_blank" rel="noopener noreferrer">INLONG-1308</a>.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="open-up-dataproxy-pulsar-data-flow">Open up dataproxy-&gt;pulsar data flow<a href="#open-up-dataproxy-pulsar-data-flow" class="hash-link" aria-label="Direct link to Open up dataproxy->pulsar data flow" title="Direct link to Open up dataproxy->pulsar data flow">​</a></h4><p>Before version 0.11.0,  InLong's data caching layer could only support TubeMQ.  TubeMQ is very suitable for scenarios with huge data volumes,  but in extreme scenarios,  there may be a small amount of data loss. To provide data reliability, the Inlong added support for Apache Pulsar in version 0.11.0.  Now InLong backend can support data flow <code>agent -&gt; dataProxy -&gt; tubeMQ/pulsar -&gt; sort.</code> The introduction of Pulsar makes the application scenarios covered by InLong more abundant,  which could meet the needs of more users.
Thanks to @baomingyu for his contribution to this feature.  For more details,  please refer to <a href="https://github.com/apache/incubator-inlong/issues/1330" target="_blank" rel="noopener noreferrer">INLONG-1330</a>.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="go-sdk-for-inlong-tubemq">Go SDK for InLong TubeMQ<a href="#go-sdk-for-inlong-tubemq" class="hash-link" aria-label="Direct link to Go SDK for InLong TubeMQ" title="Direct link to Go SDK for InLong TubeMQ">​</a></h4><p>Before version 0.11.0,  InLong TubeMQ supports SDKs in three languages:  Java,  C++,  and Python.  With more and more applications of Go language,  the demand for Go language SDK in the community is also increasing. Version 0.11.0 was officially introduced to the Go language SDK of TubeMQ.  The multilingual SDK is enriched,  and the difficulty of access and use for Go language users is also reduced.
Thanks to @TszKitLo40 for contributing to this feature. For more details, please refer to:</p><ul><li><a href="https://github.com/apache/incubator-inlong/issues/624" target="_blank" rel="noopener noreferrer">INLONG-624</a></li><li><a href="https://github.com/apache/incubator-inlong/issues/1570" target="_blank" rel="noopener noreferrer">INLONG-1578</a></li><li><a href="https://github.com/apache/incubator-inlong/issues/1584" target="_blank" rel="noopener noreferrer">INLONG-1584</a></li><li><a href="https://github.com/apache/incubator-inlong/issues/1666" target="_blank" rel="noopener noreferrer">INLONG-1666</a></li><li><a href="https://github.com/apache/incubator-inlong/issues/1682" target="_blank" rel="noopener noreferrer">INLONG-1682</a></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="refactor-the-official-website">refactor the official website<a href="#refactor-the-official-website" class="hash-link" aria-label="Direct link to refactor the official website" title="Direct link to refactor the official website">​</a></h4><p>In version 0.11.0,  InLong uses Docusaurus to refactor the <a href="https://inlong.apache.org/" target="_blank" rel="noopener noreferrer">official website</a> to provide a more concise and intuitive document management and display.
Thanks to @leezng for his contribution to this feature. For more details,  please refer to:</p><ul><li><a href="https://github.com/apache/incubator-inlong/issues/1631" target="_blank" rel="noopener noreferrer">INLONG-1631</a></li><li><a href="https://github.com/apache/incubator-inlong/issues/1632" target="_blank" rel="noopener noreferrer">INLONG-1632</a></li><li><a href="https://github.com/apache/incubator-inlong/issues/1633" target="_blank" rel="noopener noreferrer">INLONG-1633</a></li><li><a href="https://github.com/apache/incubator-inlong/issues/1634" target="_blank" rel="noopener noreferrer">INLONG-1634</a></li><li><a href="https://github.com/apache/incubator-inlong/issues/1635" target="_blank" rel="noopener noreferrer">INLONG-1635</a></li><li><a href="https://github.com/apache/incubator-inlong/issues/1636" target="_blank" rel="noopener noreferrer">INLONG-1636</a></li><li><a href="https://github.com/apache/incubator-inlong/issues/1637" target="_blank" rel="noopener noreferrer">INLONG-1637</a></li><li><a href="https://github.com/apache/incubator-inlong/issues/1638" target="_blank" rel="noopener noreferrer">INLONG-1638</a></li></ul><p>In addition to the above major features,  InLong 0.11.0 version has other key improvements,  including but not limited to:</p><ul><li>Added contribution guidelines in the main Repo,  <a href="https://github.com/apache/incubator-inlong/issues/1623" target="_blank" rel="noopener noreferrer">INLONG-1623</a></li><li>Add Inlong-Manager to DataProxy configuration management, <a href="https://github.com/apache/incubator-inlong/issues/1595" target="_blank" rel="noopener noreferrer">INLONG-1595</a></li><li>Optimized the GitHub issue template, <a href="https://github.com/apache/incubator-inlong/issues/1585" target="_blank" rel="noopener noreferrer">INLONG-1585</a></li><li>Code Checkstyle optimization, <a href="https://github.com/apache/incubator-inlong/issues/1571" target="_blank" rel="noopener noreferrer">INLONG-1571</a>, <a href="https://github.com/apache/incubator-inlong/issues/1593" target="_blank" rel="noopener noreferrer">INLONG-1593</a>, <a href="https://github.com/apache/incubator-inlong/issues/1593" target="_blank" rel="noopener noreferrer">INLONG-1593</a>, <a href="https://github.com/apache/incubator-inlong/issues/1662" target="_blank" rel="noopener noreferrer">INLONG-1662</a></li><li>Agent introduces message filter, <a href="https://github.com/apache/incubator-inlong/issues/1641" target="_blank" rel="noopener noreferrer">INLONG-1641</a></li></ul><p>The 0.11.0 version also fixes ~45 bugs. Thanks to all the contributions who have contributed to the Inlong community (in no particular order):</p><ul><li>shink</li><li>baomingyu</li><li>TszKitLo40</li><li>leezng</li><li>ruanwenjun</li><li>leo65535</li><li>luchunliang</li><li>pierre94</li><li>EMsnap</li><li>dockerzhang</li><li>gosonzhang</li><li>healchow</li><li>guangxuCheng</li><li>yuanboliu</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="apache-inlongincubating-follow-up-planning">Apache InLong(incubating) follow-up planning<a href="#apache-inlongincubating-follow-up-planning" class="hash-link" aria-label="Direct link to Apache InLong(incubating) follow-up planning" title="Direct link to Apache InLong(incubating) follow-up planning">​</a></h3><p>In the subsequent version planning of InLong, we will further release the capabilities of InLong to cover more usage scenarios, mainly including</p><ul><li>Support Apache Pulsar full link data access capabilities, including back-end processing and front-end management capabilities.</li><li>Support data flow such as ClickHouse,  Apache Iceberg,  Apache HBase, etc.</li></ul>]]></content>
        <author>
            <name>dockerzhang</name>
            <uri>https://github.com/dockerzhang</uri>
        </author>
        <category label="Apache InLong" term="Apache InLong"/>
        <category label="Version" term="Version"/>
    </entry>
</feed>