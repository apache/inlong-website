"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[4023],{3905:function(e,n,t){t.d(n,{Zo:function(){return p},kt:function(){return d}});var a=t(67294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var s=a.createContext({}),u=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},p=function(e){var n=u(e.components);return a.createElement(s.Provider,{value:n},e.children)},c={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),m=u(t),d=r,f=m["".concat(s,".").concat(d)]||m[d]||c[d]||o;return t?a.createElement(f,l(l({ref:n},p),{},{components:t})):a.createElement(f,l({ref:n},p))}));function d(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,l=new Array(o);l[0]=m;var i={};for(var s in n)hasOwnProperty.call(n,s)&&(i[s]=n[s]);i.originalType=e,i.mdxType="string"==typeof e?e:r,l[1]=i;for(var u=2;u<o;u++)l[u]=t[u];return a.createElement.apply(null,l)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},89128:function(e,n,t){t.r(n),t.d(n,{assets:function(){return p},contentTitle:function(){return s},default:function(){return d},frontMatter:function(){return i},metadata:function(){return u},toc:function(){return c}});var a=t(87462),r=t(63366),o=(t(67294),t(3905)),l=["components"],i={title:"DataStream Example",sidebar_position:3},s=void 0,u={unversionedId:"modules/sort/example",id:"modules/sort/example",title:"DataStream Example",description:"Overview",source:"@site/docs/modules/sort/example.md",sourceDirName:"modules/sort",slug:"/modules/sort/example",permalink:"/docs/next/modules/sort/example",draft:!1,editUrl:"https://github.com/apache/inlong-website/edit/master/docs/modules/sort/example.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{title:"DataStream Example",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Deployment",permalink:"/docs/next/modules/sort/quick_start"},next:{title:"Overview",permalink:"/docs/next/modules/manager/overview"}},p={},c=[{value:"Overview",id:"overview",level:2},{value:"Environment Requirements",id:"environment-requirements",level:2},{value:"Prepare InLong Sort And Connectors",id:"prepare-inlong-sort-and-connectors",level:2},{value:"Usage for SQL API",id:"usage-for-sql-api",level:2},{value:"MySQL to Kafka",id:"mysql-to-kafka",level:3},{value:"Kafka to Hive",id:"kafka-to-hive",level:3},{value:"Usage for Dashboard",id:"usage-for-dashboard",level:2},{value:"Usage for Manager Client Tools",id:"usage-for-manager-client-tools",level:2}],m={toc:c};function d(e){var n=e.components,t=(0,r.Z)(e,l);return(0,o.kt)("wrapper",(0,a.Z)({},m,t,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"overview"},"Overview"),(0,o.kt)("p",null,"To make it easier for you to create InLong-Sort jobs, here we list some data stream configuration examples.\nThe following will introduce SQL, Dashboard, Manager Client Tools methods to use Inlong Sort."),(0,o.kt)("h2",{id:"environment-requirements"},"Environment Requirements"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"JDK 1.8.x"),(0,o.kt)("li",{parentName:"ul"},"Flink 1.13.5"),(0,o.kt)("li",{parentName:"ul"},"MySQL"),(0,o.kt)("li",{parentName:"ul"},"Kafka"),(0,o.kt)("li",{parentName:"ul"},"Hadoop"),(0,o.kt)("li",{parentName:"ul"},"Hive 3.x")),(0,o.kt)("h2",{id:"prepare-inlong-sort-and-connectors"},"Prepare InLong Sort And Connectors"),(0,o.kt)("p",null,"You can prepare InLong Sort and Data Node Connectors by referring to ",(0,o.kt)("a",{parentName:"p",href:"/docs/next/modules/sort/quick_start"},"Deployment Guide"),"."),(0,o.kt)("h2",{id:"usage-for-sql-api"},"Usage for SQL API"),(0,o.kt)("p",null,"This example defines the data flow for a single table(mysql--\x3ekafka--\x3ehive). "),(0,o.kt)("h3",{id:"mysql-to-kafka"},"MySQL to Kafka"),(0,o.kt)("p",null,"Single table sync example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"./bin/flink run -c org.apache.inlong.sort.Entrance FLINK_HOME/lib/sort-dist-[version].jar \\\n--sql.script.file /YOUR_SQL_SCRIPT_DIR/mysql-to-kafka.sql\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"mysql-to-kafka.sql")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE `table_1`(\n    PRIMARY KEY (`id`) NOT ENFORCED,\n    `id` BIGINT,\n    `name` STRING,\n    `age` INT,\n    `salary` FLOAT,\n    `ts` TIMESTAMP(2),\n    `event_type` STRING)\n    WITH (\n    'append-mode' = 'true',\n    'connector' = 'mysql-cdc-inlong',\n    'hostname' = 'localhost',\n    'username' = 'root',\n    'password' = 'password',\n    'database-name' = 'dbName',\n    'table-name' = 'tableName'\n);\n\nCREATE TABLE `table_2`(\n    `id` BIGINT,\n    `name` STRING,\n    `age` INT,\n    `salary` FLOAT,\n    `ts` TIMESTAMP(2))\n    WITH (\n    'topic' = 'topicName',-- Your kafka topic\n    'properties.bootstrap.servers' = 'localhost:9092',\n    'connector' = 'kafka',\n    'json.timestamp-format.standard' = 'SQL',\n    'json.encode.decimal-as-plain-number' = 'true',\n    'json.map-null-key.literal' = 'null',\n    'json.ignore-parse-errors' = 'true',\n    'json.map-null-key.mode' = 'DROP',\n    'format' = 'json',\n    'json.fail-on-missing-field' = 'false'\n);\n\nINSERT INTO `table_2` \n    SELECT \n    `id` AS `id`,\n    `name` AS `name`,\n    `age` AS `age`,\n    CAST(NULL as FLOAT) AS `salary`,\n    `ts` AS `ts`\n    FROM `table_1`;\n\n")),(0,o.kt)("h3",{id:"kafka-to-hive"},"Kafka to Hive"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Note:"),"  First you need to create user table in Hive."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"./bin/flink run -c org.apache.inlong.sort.Entrance FLINK_HOME/lib/sort-dist-[version].jar \\\n--sql.script.file /YOUR_SQL_SCRIPT_DIR/kafka-to-hive.sql\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"kafka-to-hive.sql")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE `table_1`(\n    `id` BIGINT,\n    `name` STRING,\n    `age` INT,\n    `salary` FLOAT,\n    `ts` TIMESTAMP(2)\n    WITH (\n    'topic' = 'topicName',-- Your kafka topic\n    'properties.bootstrap.servers' = 'localhost:9092',\n    'connector' = 'kafka',\n    'scan.startup.mode' = 'earliest-offset',\n    'json.timestamp-format.standard' = 'SQL',\n    'json.encode.decimal-as-plain-number' = 'true',\n    'json.map-null-key.literal' = 'null',\n    'json.ignore-parse-errors' = 'true',\n    'json.map-null-key.mode' = 'DROP',\n    'format' = 'json',\n    'json.fail-on-missing-field' = 'false',\n    'properties.group.id' = 'groupId'-- Your group id\n);\n\nCREATE TABLE `user`(\n    `id` BIGINT,\n    `name` STRING,\n    `age` INT,\n    `salary` FLOAT,\n    `ts` TIMESTAMP(9))\n    WITH (\n    'connector' = 'hive',\n    'default-database' = 'default',\n    'hive-version' = '3.1.2',\n    'hive-conf-dir' = 'hdfs://ip:9000/.../hive-site.xml' -- Put your hive-site.xml into HDFS\n);\n\nINSERT INTO `user` \n    SELECT \n    `id` AS `id`,\n    `name` AS `name`,\n    `age` AS `age`,\n    CAST(NULL as FLOAT) AS `salary`,\n    `ts` AS `ts`\n    FROM `table_1`;\n\n")),(0,o.kt)("p",null,"Note: Of course you can also put all the SQL in one file."),(0,o.kt)("h2",{id:"usage-for-dashboard"},"Usage for Dashboard"),(0,o.kt)("p",null,"The underlying capabilities are already available and will complement the Dashboard capabilities in the future."),(0,o.kt)("h2",{id:"usage-for-manager-client-tools"},"Usage for Manager Client Tools"),(0,o.kt)("p",null,"TODO: It will be supported in the future."))}d.isMDXComponent=!0}}]);