"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[1477],{30010:function(n){n.exports=JSON.parse('{"blogPosts":[{"id":"/InLong_Sort_ETL_en","metadata":{"permalink":"/blog/InLong_Sort_ETL_en","editUrl":"https://github.com/apache/incubator-inlong-website/edit/master/blog/blog/InLong_Sort_ETL_en.md","source":"@site/blog/InLong_Sort_ETL_en.md","title":"Analysis of InLong Sort ETL Solution Based on Apache Flink SQL","description":"With the increasing number of users and developers of Apache InLong(incubating), the demand for richer usage scenarios and low-cost operation is getting stronger and stronger. Among them, the demand for adding Transform (T) to the whole link of InLong has received the most feedback. After the research and design of @yunqingmoswu, @EMsnap, @gong, @thexiay community developers, the InLong Sort ETL solution based on Flink SQL has been completed. This article will introduce the implementation details of the solution in detail.","date":"2022-06-16T04:52:05.000Z","formattedDate":"June 16, 2022","tags":[],"readingTime":10.47,"truncated":false,"authors":[],"frontMatter":{"title":"Analysis of InLong Sort ETL Solution Based on Apache Flink SQL","sidebar_position":4},"nextItem":{"title":"Release InLong 0.11.0","permalink":"/blog/apache-inlong-0.11.0"}},"content":"# 1. Background\\n\\nWith the increasing number of users and developers of Apache InLong(incubating), the demand for richer usage scenarios and low-cost operation is getting stronger and stronger. Among them, the demand for adding Transform (T) to the whole link of InLong has received the most feedback. After the research and design of @yunqingmoswu, @EMsnap, @gong, @thexiay community developers, the InLong Sort ETL solution based on Flink SQL has been completed. This article will introduce the implementation details of the solution in detail.\\n\\nFirst of all, based on Apache Flink SQL, there are mainly the following considerations\uff1a\\n\\n-  Flink SQL has high scalability and flexibility brought about by its powerful expression ability. Basically, Flink SQL can support most demand scenarios in the community. When the built-in functions of Flink SQL do not meet the requirements, we can also extend them through various UDFs.\\n-  Compared with the implementation of the underlying API of Flink, the development cost of Flink SQL is lower. Only for the first time, the conversion logic of Flink SQL needs to be implemented. In the future, we can focus on the construction of the ability of Flink SQL, such as the extension connector and the UDF.\\n- In general, Flink SQL will be more robust and run more stable. The reason is that Flink SQL shields a lot of the underlying details of Flink, has strong community support, and has been practiced by a large number of users.\\n- For users, Flink SQL is also easier to understand, especially for users who have used SQL, the usage is simple and familiar, which helps users to land quickly.\\n- For the migration of existing real-time tasks, if they are originally SQL-type tasks, especially Flink SQL tasks, the migration cost is extremely low, and in some cases, no changes are even required.\\n\\n**Note**:  for all codes of this scheme, please refer to [Apache inlong sort]\uff08 https://github.com/apache/incubator-inlong/tree/master/inlong-sort \uff09Module, which can be downloaded and used in the upcoming version 1.2.0.\\n\\n# 2. Introduction\\n\\n## 2.1 Requirements\\n\\nThe main requirements of this solution are the completed inlong sort module transform (T) capability, including:\\n\\n|          Transform          |                            Notes                             |\\n| :-------------------------: | :----------------------------------------------------------: |\\n| Deduplication in the window |            Deduplicate data within a time window             |\\n|   time window aggregation   |             Aggregate data within a time window              |\\n|   time format conversion    | Converts the value of a field to a string in the target time format |\\n|     field segmentation      |    Split a field into multiple new fields by a delimiter     |\\n|     string replacement      |    Replace some or all of the contents of a string field     |\\n|       Data filtering        |   Discard or retain data that meets the filter conditions    |\\n|     Content extraction      |        Extract part of a field to create a new field         |\\n|            Join             |                    Support two table join                    |\\n|     Value substitution      | Given a matching value, if the field\'s value is equal to that value, replace it with the target value |\\n\\n## 2.2 Usage Scenarios\\n\\nUsers of big data integration have transform requirements such as data transformation, connection and filtering in many business scenarios.\\n\\n## 2.3 Design Goal\\n\\nThis design needs to achieve the following goals:\\n\\n- Functionality: Under InLong Sort\'s existing architecture and data flow model, it covers basic Transform capabilities and has the ability to expand rapidly.\\n- Compatibility: The new InLong Sort data model is forward compatible to ensure that historical tasks can be configured and run properly.\\n- Maintainability: The conversion of the InLong Sort data model to Flink SQL only needs to be implemented once. When there are new functional requirements later, this part does not need to be changed, even if there are changes, it can be supported with a small amount of changes.\\n- Extensibility: When the open source Flink Connector or the built-in Flink SQL function does not meet the requirements, you can customize the Flink Connector and UDF to achieve its function expansion.\\n\\n## 2.4 Basic Concepts\\n\\nThe core concept refers to the explanation of terms in the outline design\\n\\n|            Name             |                           Meaning                            |\\n| :-------------------------: | :----------------------------------------------------------: |\\n|      InLong Dashborad       |            Inlong front end management interface             |\\n|    InLong Manager Client    | Wrap the interface in the manager for external user programs to call without going through the front-end inlong dashboard |\\n|   InLong Manager Openapi    |      Inlong manager and external system call interface       |\\n|   InLong Manager metaData   | Inlong manager metadata management, including metadata information of group and stream dimensions |\\n| InLong Manager task manager | Inlong manager manages the data source collection task module, manages agent task distribution, instruction distribution, and heartbeat reporting |\\n|        InLong Group         | Data flow group, including multiple data flows, one group represents one data access |\\n|        InLong Stream        |     Data flow: a data flow has a specific flow direction     |\\n|        Stream Source        | There are corresponding acquisition end and sink end in the stream. This design only involves the stream source |\\n|         Stream Info         | Abstract of data flow in sort, including various sources, transformations, destinations, etc. of the data flow |\\n|         Group Info          | Encapsulation of data flow in sort. A groupinfo can contain multiple stream infos |\\n|            Node             | Abstraction of data source, data transformation and data destination in data synchronization |\\n|        Extract Node         |       Source side abstraction of data synchronization        |\\n|          Load Node          |       Destination abstraction of data synchronization        |\\n|     MySQL Extract Node      |                MySQL data source abstraction                 |\\n|       Kafka Load Node       |              Kafka data destination abstraction              |\\n|       Transform Node        |  Transformation process abstraction of data synchronization  |\\n|  Aggregate Transform Node   | Data synchronization aggregation class transformation process abstraction |\\n|        Node Relation        |  Relationship abstraction of nodes in data synchronization   |\\n|       Field Relation        | Abstraction of the relationship between upstream and downstream node fields in data synchronization |\\n|          Function           | Abstraction of the relationship between upstream and downstream node fields in data synchronization |\\n|     Substring Function      |         Abstraction of string interception function          |\\n|       Filter Function       |             Abstraction of data filter function              |\\n|       Function Param        |           Input parameter abstraction of function            |\\n|       Constant Param        |                     Constant parameters                      |\\n|         Field Info          |                          Node field                          |\\n|       Meta FieldInfo        |                 Node meta information field                  |\\n\\n\\n\\n## 2.5 Domain Model\\n\\nThis design mainly involves the following entities: \\n\\nGroup\u3001Stream\u3001GroupInfo\u3001StreamInfo\u3001Node\u3001NodeRelation\u3001FieldRelation\u3001Function\u3001FilterFunction\u3001SubstringFunction\u3001FunctionParam\u3001FieldInfo\u3001MetaFieldInfo\u3001MySQLExtractNode\u3001KafkaLoadNode and etc.\\n\\nFor ease of understanding, this section will model and analyze the relationship between entities. Description of entity correspondence of domain model:\\n\\n- One group corresponds to one groupinfo\\n- A group contains one or more streams\\n- One stream corresponds to one streaminfo\\n- A groupinfo contains one or more streaminfo\\n- A streaminfo contains multiple nodes\\n- A streaminfo contains one or more NodeRelations\\n- A noderelation contains one or more fieldrelations\\n- A NodeRelation contains 0 or more filterfunctions\\n- A fieldrelation contains one function or one fieldinfo as the source field and one fieldinfo as the target field\\n- A function contains one or more FunctionParams\\n\\nThe above relationship can be represented by UML object relationship diagram as:\\n\\n![sort_UML](./img/sort_UML.png)\\n\\n## 2.6 Function Use-case Diagram\\n\\n![sort-usecase](./img/sort-usecase.png)\\n\\n# 3. System Outline Design\\n\\n## 3.1 System Architecture Diagram\\n\\n![architecture](./img/architecture.png)\\n\\n- Serialization: Serialization Implementation Module\\n- Deserialization: Deserialization Implementation Module\\n- Flink Source: Custom Flink source implementation module\\n- Flink Sink: Custom Flink sink implementation module\\n- Transformation: Custom Transform implementation module\\n- GroupInfo: Corresponding to Inlong group\\n- StreamInfo: Corresponding to inlong stream\\n- Node: Abstraction of data source, data conversion and data destination in data synchronization\\n- FlinkSQLParser: SQL parser\\n\\n## 3.2 InLong Sort Internal Operation Flow Chart\\n\\n![sort-operation-flow](./img/sort-operation-flow.png)\\n\\n## 3.3 Module Design\\n\\nThis design only adds Flink connector and flinksql generator to the original system, and modifies the data model module.\\n\\n### 3.3.1 Module Structure\\n\\n![sort-module-structure](./img/sort-module-structure.png)\\n\\n### 3.3.2 Module Division\\n\\nDescription of important module division:\\n\\n|       Name        |                         Description                          |\\n| :---------------: | :----------------------------------------------------------: |\\n|  FlinkSQLParser   | Used to generate flinksql core classes, including references to groupinfo |\\n|     GroupInfo     | The internal abstraction of sort for inlong group is used to encapsulate the synchronization related information of the entire inlong group, including the reference to list\\\\<streaminfo\\\\> |\\n|    StreamInfo     | The internal abstraction of sort to inlong stream is used to encapsulate inlong stream synchronization related information, including references to list\\\\<node\\\\>, list\\\\<noderelation\\\\> |\\n|       Node        | The top-level interface of the synchronization node. Its subclass implementation is mainly used to encapsulate the data of the synchronization data source and the transformation node |\\n|    ExtractNode    |      Data extract node abstraction, inherited from node      |\\n|     LoadNode      |       Data load node abstraction, inherited from node        |\\n|   TransformNode   |  Data transformation node abstraction, inherited from node   |\\n|   NodeRelation    |              Define relationships between nodes              |\\n|   FieldRelation   |           Define field relationships between nodes           |\\n|     Function      |           Abstract of T-ability execution function           |\\n|  FilterFunction   | Function abstraction for data filtering, inherited from function |\\n| SubstringFunction | Used for string interception function abstraction, inherited from function |\\n|   FunctionParam   |             Abstraction for function parameters              |\\n|   ConstantParam   | Encapsulation of function constant parameters, inherited from FunctionParam |\\n|     FieldInfo     | The encapsulation of node fields can also be used as function input parameters, inherited from functionparam |\\n|   MetaFieldInfo   | The encapsulation of built-in fields is currently mainly used in the metadata field scenario of canal JSON, which is inherited from fieldinfo |\\n\\n# 4. Detailed System Design\\n\\nThe following describes the principle of SQL generation by taking MySQL synchronizing data to Kafka as an example\\n\\n## 4.1 Node Described in SQL\\n\\n### 4.1.1 ExtractNode Described in SQL\\n\\nThe node configuration is:\\n\\n**nodeconfig1**\\n\\n```java\\n private Node buildMySQLExtractNode() {\\n        List<FieldInfo> fields = Arrays.asList(\\n                new FieldInfo(\\"name\\", new StringFormatInfo()),\\n                new FieldInfo(\\"age\\", new IntFormatInfo()));\\n        return new MySqlExtractNode(\\"1\\", \\"mysql_input\\", fields,\\n                null, null, \\"id\\",\\n                Collections.singletonList(\\"tableName\\"), \\"localhost\\", \\"root\\", \\"password\\",\\n                \\"inlong\\", null, null,\\n                null, null);\\n    }\\n```\\n\\nThe generated SQL is:\\n\\n**ss**\\n\\n```sql\\nCREATE TABLE `mysql_1` (`name` string,`age` int) \\nwith \\n(\'connector\' = \'mysql-cdc-inlong\',\\n\'hostname\' = \'localhost\',\\n\'username\' = \'root\',\\n\'password\' = \'password\',\\n\'database-name\' = \'inlong\',\\n\'table-name\' = \'tableName\')\\n\\n```\\n\\n\\n\\n### 4.1.2 TransformNode  Described in SQL\\n\\nThe node configuration is:\\n\\n**nodeconfig2**\\n\\n```java\\n List<FilterFunction> filters = Arrays.asList(\\n                new SingleValueFilterFunction(EmptyOperator.getInstance(),\\n                        new FieldInfo(\\"age\\", new IntFormatInfo()),\\n                        LessThanOperator.getInstance(), new ConstantParam(25)),\\n                new SingleValueFilterFunction(AndOperator.getInstance(),\\n                        new FieldInfo(\\"age\\", new IntFormatInfo()),\\n                        MoreThanOrEqualOperator.getInstance(), new ConstantParam(18))\\n        );\\n\\n```\\n\\nThe generated SQL is:\\n\\n**ss2**\\n\\n```sql\\nSELECT `name` AS `name`,`age` AS `age` FROM `mysql_1` WHERE `age` < 25 AND `age` >= 18\\n\\n```\\n\\n\\n\\n### 4.1.3 LoadNode Described in SQL\\n\\nThe node configuration is:\\n\\n**nodeconfig3**\\n\\n```java\\n private Node buildKafkaLoadNode(FilterStrategy filterStrategy) {\\n        List<FieldInfo> fields = Arrays.asList(\\n                new FieldInfo(\\"name\\", new StringFormatInfo()),\\n                new FieldInfo(\\"age\\", new IntFormatInfo())\\n        );\\n        List<FieldRelation> relations = Arrays\\n                .asList(\\n                        new FieldRelation(new FieldInfo(\\"name\\", new StringFormatInfo()),\\n                                new FieldInfo(\\"name\\", new StringFormatInfo())),\\n                        new FieldRelation(new FieldInfo(\\"age\\", new IntFormatInfo()),\\n                                new FieldInfo(\\"age\\", new IntFormatInfo()))\\n                );\\n        List<FilterFunction> filters = Arrays.asList(\\n                new SingleValueFilterFunction(EmptyOperator.getInstance(),\\n                        new FieldInfo(\\"age\\", new IntFormatInfo()),\\n                        LessThanOperator.getInstance(), new ConstantParam(25)),\\n                new SingleValueFilterFunction(AndOperator.getInstance(),\\n                        new FieldInfo(\\"age\\", new IntFormatInfo()),\\n                        MoreThanOrEqualOperator.getInstance(), new ConstantParam(18))\\n        );\\n        return new KafkaLoadNode(\\"2\\", \\"kafka_output\\", fields, relations, filters,\\n                filterStrategy, \\"topic1\\", \\"localhost:9092\\",\\n                new CanalJsonFormat(), null,\\n                null, \\"id\\");\\n    }\\n\\n```\\n\\nThe generated SQL is:\\n\\n**ss3**\\n\\n```sql\\nCREATE TABLE `kafka_3` (`name` string,`age` int) \\nwith (\\n\'connector\' = \'kafka-inlong\',\\n\'topic\' = \'topic1\',\\n\'properties.bootstrap.servers\' = \'localhost:9092\',\\n\'format\' = \'canal-json-inlong\',\\n\'canal-json-inlong.ignore-parse-errors\' = \'true\',\\n\'canal-json-inlong.map-null-key.mode\' = \'DROP\',\\n\'canal-json-inlong.encode.decimal-as-plain-number\' = \'true\',\\n\'canal-json-inlong.timestamp-format.standard\' = \'SQL\',\\n\'canal-json-inlong.map-null-key.literal\' = \'null\'\\n)\\n\\n```\\n\\n\\n\\n## 4.2 Field T Described in SQL\\n\\n### 4.2.1 Filter operator\\n\\nSee 4.1 node configuration for relevant configurations\\n\\nThe generated SQL is:\\n\\n**ss4**\\n\\n```sql\\nINSERT INTO `kafka_3` SELECT `name` AS `name`,`age` AS `age` FROM `mysql_1` WHERE `age` < 25 AND `age` >= 18\\n\\n```\\n\\n### 4.2.2 Watermark\\n\\nThe complete configuration of groupinfo is as follows:\\n\\n**nodeconfig3**\\n\\n```java\\nprivate Node buildMySqlExtractNode() {\\n        List<FieldInfo> fields = Arrays.asList(\\n                new FieldInfo(\\"name\\", new StringFormatInfo()),\\n                new FieldInfo(\\"age\\", new IntFormatInfo()),\\n                new FieldInfo(\\"ts\\", new TimestampFormatInfo()));\\n        WatermarkField wk = new WatermarkField(new FieldInfo(\\"ts\\", new TimestampFormatInfo()),\\n                new StringConstantParam(\\"1\\"),\\n                new TimeUnitConstantParam(TimeUnit.MINUTE));\\n        return new MySqlExtractNode(\\"1\\", \\"mysql_input\\", fields,\\n                wk, null, \\"id\\",\\n                Collections.singletonList(\\"tableName\\"), \\"localhost\\", \\"root\\", \\"password\\",\\n                \\"inlong\\", null, null,\\n                null, null);\\n    }\\n\\n    private Node buildKafkaNode() {\\n        List<FieldInfo> fields = Arrays.asList(\\n                new FieldInfo(\\"name\\", new StringFormatInfo()),\\n                new FieldInfo(\\"age\\", new IntFormatInfo()),\\n                new FieldInfo(\\"ts\\", new TimestampFormatInfo()));\\n        List<FieldRelation> relations = Arrays\\n                .asList(new FieldRelation(new FieldInfo(\\"name\\", new StringFormatInfo()),\\n                                new FieldInfo(\\"name\\", new StringFormatInfo())),\\n                        new FieldRelation(new FieldInfo(\\"age\\", new IntFormatInfo()),\\n                                new FieldInfo(\\"age\\", new IntFormatInfo()))\\n                );\\n        return new KafkaLoadNode(\\"2\\", \\"kafka_output\\", fields, relations, null, null,\\n                \\"topic\\", \\"localhost:9092\\", new JsonFormat(),\\n                1, null, \\"id\\");\\n    }\\n\\n    private NodeRelation buildNodeRelation(List<Node> inputs, List<Node> outputs) {\\n        List<String> inputIds = inputs.stream().map(Node::getId).collect(Collectors.toList());\\n        List<String> outputIds = outputs.stream().map(Node::getId).collect(Collectors.toList());\\n        return new NodeRelation(inputIds, outputIds);\\n    }\\n\\n    @Override\\n    public GroupInfo getTestObject() {\\n        Node input = buildMySqlExtractNode();\\n        Node output = buildKafkaNode();\\n        StreamInfo streamInfo = new StreamInfo(\\"1\\", Arrays.asList(input, output), Collections.singletonList(\\n                buildNodeRelation(Collections.singletonList(input), Collections.singletonList(output))));\\n        return new GroupInfo(\\"1\\", Collections.singletonList(streamInfo));\\n    }\\n```"},{"id":"/apache-inlong-0.11.0","metadata":{"permalink":"/blog/apache-inlong-0.11.0","editUrl":"https://github.com/apache/incubator-inlong-website/edit/master/blog/blog/apache-inlong-0.11.0.md","source":"@site/blog/apache-inlong-0.11.0.md","title":"Release InLong 0.11.0","description":"Apache InLong (incubating)  has been renamed from the original Apache TubeMQ (incubating) from 0.9.0.  With the name change,  InLong has also been upgraded from a single message queue to a one-stop integration framework for massive data.  InLong supports data collection,  aggregation,  caching,  and sorting,  users can import data from the data source to the real-time computing engine or land to offline storage with a simple configuration.","date":"2022-06-16T04:52:05.000Z","formattedDate":"June 16, 2022","tags":[],"readingTime":4.915,"truncated":false,"authors":[],"frontMatter":{"title":"Release InLong 0.11.0","sidebar_position":3},"prevItem":{"title":"Analysis of InLong Sort ETL Solution Based on Apache Flink SQL","permalink":"/blog/InLong_Sort_ETL_en"},"nextItem":{"title":"Release InLong 0.12.0","permalink":"/blog/apache-inlong-0.12.0"}},"content":"Apache InLong (incubating)  has been renamed from the original Apache TubeMQ (incubating) from 0.9.0.  With the name change,  InLong has also been upgraded from a single message queue to a one-stop integration framework for massive data.  InLong supports data collection,  aggregation,  caching,  and sorting,  users can import data from the data source to the real-time computing engine or land to offline storage with a simple configuration.\\nThe just-released version `0.11.0-incubating` is the third version after the name changed.  This version includes next features:\\n- Lower the user\'s threshold for use further.  Support all modules of InLong to be deployed on Kubernetes,  and refactor the official website,  so that users can more easily access related documents.\\n- Support more usage scenarios,  increase the data flow direction of `Dataproxy -> Pulsar`,  and cover scenarios with higher requirements for data integrity and consistency.\\n- Supports SDKs in more languages for TubeMQ.  This version opens the production-level TubeMQ Go SDK, which is convenient for users who use the Go language to access\\n\\nThis version closed more than 80 issues, including four significant features and 35 improvements.\\n\\n### Apache InLong(incubating) Introduction\\n[Apache InLong](https://inlong.apache.org) is a one-stop integration framework for massive data donated by Tencent to the Apache community.  It provides automatic,  safe,  reliable,  and high-performance data transmission capabilities to facilitate the construction of streaming-based data analysis,  modeling,  and applications.  \\nThe Apache InLong project was originally called TubeMQ,  focusing on high-performance,  low-cost message queuing services.  In order to further release the surrounding ecological capabilities of TubeMQ,  we upgraded the project to InLong,  focusing on creating a one-stop integration framework for massive data.\\n\\nApache InLong uses TDBank internally used by Tencent as the prototype,  and relies on trillion-level data access and processing capabilities to integrate the entire process of data collection,  aggregation,  storage,  and sorting data processing.  It is simple to use,  flexible to expand,  stable and reliable characteristic.\\n<img src=\\"/img/inlong-structure-en.png\\" align=\\"center\\" alt=\\"Apache InLong\\"/>\\n\\nApache InLong serves the entire life cycle from data collection to landing,  and provides different processing modules according to different stages of data,  including the next modules:\\n- **inlong-agent**,  data collection agent, supports reading regular logs from specified directories or files and reporting data one by one.  In the future,  DB collection and HTTP reporting capabilities will also be expanded.\\n- **inlong-dataproxy**,  a Proxy component based on Flume-ng,  supports data transmission blocking,  placing retransmission, and has the ability to forward received data to different MQ (message queues).\\n- **inlong-tubemq**,  Tencent\'s self-developed message queuing service,  focuses on high-performance storage and transmission of massive data in big data scenarios and has a relatively good core advantage in mass practice and low cost.\\n- **inlong-sort**,  after consuming data from different MQ services,  perform ETL processing,  and then aggregate and write the data into Apache Hive, ClickHouse,  Hbase,  IceBerg,  etc.\\n- **inlong-manager**, provides complete data service management and control capabilities,  including metadata,  OpenAPI,  task flow,  authority,  etc.\\n- **inlong-website**, a front-end page for managing data access,  simplifying the use of the entire InLong control platform.\\n\\n### What\u2019s New in Apache InLong(incubating) 0.11.0\\n#### InLong on Kubernetes \\nApache InLong includes multiple components such as data collection,  data aggregation,  data caching,  data sorting,  and cluster management and control.  In order to facilitate users to use and support cloud-native features,  all components currently support deployment in Kubernetes.\\nThanks to @shink for contributing to this feature.  For specific details,  please refer to [INLONG-1308](https://github.com/apache/incubator-inlong/issues/1308).\\n\\n#### Open up dataproxy->pulsar data flow\\nBefore version 0.11.0,  InLong\'s data caching layer could only support TubeMQ.  TubeMQ is very suitable for scenarios with huge data volumes,  but in extreme scenarios,  there may be a small amount of data loss. To provide data reliability, the Inlong added support for Apache Pulsar in version 0.11.0.  Now InLong backend can support data flow `agent -> dataProxy -> tubeMQ/pulsar -> sort.` The introduction of Pulsar makes the application scenarios covered by InLong more abundant,  which could meet the needs of more users.\\nThanks to @baomingyu for his contribution to this feature.  For more details,  please refer to [INLONG-1330](https://github.com/apache/incubator-inlong/issues/1330).\\n\\n#### Go SDK for InLong TubeMQ\\nBefore version 0.11.0,  InLong TubeMQ supports SDKs in three languages:  Java,  C++,  and Python.  With more and more applications of Go language,  the demand for Go language SDK in the community is also increasing. Version 0.11.0 was officially introduced to the Go language SDK of TubeMQ.  The multilingual SDK is enriched,  and the difficulty of access and use for Go language users is also reduced.\\nThanks to @TszKitLo40 for contributing to this feature. For more details, please refer to:\\n- [INLONG-624](https://github.com/apache/incubator-inlong/issues/624)\\n- [INLONG-1578](https://github.com/apache/incubator-inlong/issues/1570)\\n- [INLONG-1584](https://github.com/apache/incubator-inlong/issues/1584)\\n- [INLONG-1666](https://github.com/apache/incubator-inlong/issues/1666)\\n- [INLONG-1682](https://github.com/apache/incubator-inlong/issues/1682)\\n\\n#### refactor the official website\\nIn version 0.11.0,  InLong uses Docusaurus to refactor the [official website](https://inlong.apache.org/) to provide a more concise and intuitive document management and display.\\nThanks to @leezng for his contribution to this feature. For more details,  please refer to:\\n- [INLONG-1631](https://github.com/apache/incubator-inlong/issues/1631)\\n- [INLONG-1632](https://github.com/apache/incubator-inlong/issues/1632)\\n- [INLONG-1633](https://github.com/apache/incubator-inlong/issues/1633)\\n- [INLONG-1634](https://github.com/apache/incubator-inlong/issues/1634)\\n- [INLONG-1635](https://github.com/apache/incubator-inlong/issues/1635)\\n- [INLONG-1636](https://github.com/apache/incubator-inlong/issues/1636)\\n- [INLONG-1637](https://github.com/apache/incubator-inlong/issues/1637)\\n- [INLONG-1638](https://github.com/apache/incubator-inlong/issues/1638)\\n\\nIn addition to the above major features,  InLong 0.11.0 version has other key improvements,  including but not limited to:\\n- Added contribution guidelines in the main Repo,  [INLONG-1623](https://github.com/apache/incubator-inlong/issues/1623)\\n- Add Inlong-Manager to DataProxy configuration management, [INLONG-1595](https://github.com/apache/incubator-inlong/issues/1595)\\n- Optimized the GitHub issue template, [INLONG-1585](https://github.com/apache/incubator-inlong/issues/1585)\\n- Code Checkstyle optimization, [INLONG-1571](https://github.com/apache/incubator-inlong/issues/1571), [INLONG-1593](https://github.com/apache/incubator-inlong/issues/1593), [INLONG-1593](https://github.com/apache/incubator-inlong/issues/1593), [INLONG-1662](https://github.com/apache/incubator-inlong/issues/1662)\\n- Agent introduces message filter, [INLONG-1641](https://github.com/apache/incubator-inlong/issues/1641)\\n\\nThe 0.11.0 version also fixes ~45 bugs. Thanks to all the contributions who have contributed to the Inlong community (in no particular order):\\n- shink\\n- baomingyu\\n- TszKitLo40\\n- leezng\\n- ruanwenjun\\n- leo65535\\n- luchunliang\\n- pierre94\\n- EMsnap\\n- dockerzhang\\n- gosonzhang\\n- healchow\\n- guangxuCheng\\n- yuanboliu\\n\\n### Apache InLong(incubating) follow-up planning\\nIn the subsequent version planning of InLong, we will further release the capabilities of InLong to cover more usage scenarios, mainly including\\n- Support Apache Pulsar full link data access capabilities, including back-end processing and front-end management capabilities.\\n- Support data flow such as ClickHouse,  Apache Iceberg,  Apache HBase, etc."},{"id":"/apache-inlong-0.12.0","metadata":{"permalink":"/blog/apache-inlong-0.12.0","editUrl":"https://github.com/apache/incubator-inlong-website/edit/master/blog/blog/apache-inlong-0.12.0.md","source":"@site/blog/apache-inlong-0.12.0.md","title":"Release InLong 0.12.0","description":"InLong: the sacred animal in Chinese myths stories, draws rivers into the sea, as a metaphor for the InLong system to provide data access capabilities.","date":"2022-06-16T04:52:05.000Z","formattedDate":"June 16, 2022","tags":[],"readingTime":5.15,"truncated":false,"authors":[],"frontMatter":{"title":"Release InLong 0.12.0","sidebar_position":2},"prevItem":{"title":"Release InLong 0.11.0","permalink":"/blog/apache-inlong-0.11.0"},"nextItem":{"title":"Release InLong 1.1.0","permalink":"/blog/apache-inlong-1.1.0"}},"content":"InLong: the sacred animal in Chinese myths stories, draws rivers into the sea, as a metaphor for the InLong system to provide data access capabilities.\\n\\nApache InLong is a one-stop integration framework for massive data that provides automatic, secure and reliable data transmission capabilities. InLong supports both batch and stream data processing at the same time, which offers great power to build data analysis, modeling and other real-time applications based on streaming data.\\nThe 0.12.0-incubating just released mainly includes the following:\\n- Provide automatic, safe, reliable and high-performance data transmission capabilities, while supporting batch and streaming\\n- Refactor the document structure of the official website to facilitate users to consult related documents based on the main line\\n- The whole process supports Pulsar data flow, and completes the whole process coverage of high-performance and high-reliability scenarios\\n- Full-process support indicators, including JMX and Prometheus output\\n- The first phase of data audit and reconciliation, write audit data to MySQL\\n\\nThis version closed about 120+ issues, including four major features and 35 improvements.\\n\\n\\n### Apache InLong(incubating) Introduction\\n[Apache InLong](https://inlong.apache.org) is a one-stop integration framework for massive data donated by Tencent to the Apache community.  It provides automatic,  safe,  reliable,  and high-performance data transmission capabilities to facilitate the construction of streaming-based data analysis,  modeling,  and applications.  \\nThe Apache InLong project was originally called TubeMQ,  focusing on high-performance,  low-cost message queuing services.  In order to further release the surrounding ecological capabilities of TubeMQ,  we upgraded the project to InLong,  focusing on creating a one-stop integration framework for massive data.\\n\\nApache InLong uses TDBank internally used by Tencent as the prototype,  and relies on trillion-level data access and processing capabilities to integrate the entire process of data collection,  aggregation,  storage,  and sorting data processing.  It is simple to use,  flexible to expand,  stable and reliable characteristic.\\n<img src=\\"/img/inlong-structure-en.png\\" align=\\"center\\" alt=\\"Apache InLong\\"/>\\n\\nApache InLong serves the entire life cycle from data collection to landing,  and provides different processing modules according to different stages of data,  including the next modules:\\n- **inlong-agent**,  data collection agent, supports reading regular logs from specified directories or files and reporting data one by one.  In the future,  DB collection and HTTP reporting capabilities will also be expanded.\\n- **inlong-dataproxy**,  a Proxy component based on Flume-ng,  supports data transmission blocking,  placing retransmission, and has the ability to forward received data to different MQ (message queues).\\n- **inlong-tubemq**,  Tencent\'s self-developed message queuing service,  focuses on high-performance storage and transmission of massive data in big data scenarios and has a relatively good core advantage in mass practice and low cost.\\n- **inlong-sort**,  after consuming data from different MQ services,  perform ETL processing,  and then aggregate and write the data into Apache Hive, ClickHouse,  Hbase,  IceBerg,  etc.\\n- **inlong-manager**, provides complete data service management and control capabilities,  including metadata,  OpenAPI,  task flow,  authority,  etc.\\n- **inlong-website**, a front-end page for managing data access,  simplifying the use of the entire InLong control platform.\\n\\n### What\u2019s New in Apache InLong(incubating) 0.12.0\\n#### 1. Support Apache Pulsar data cache\\nIn version 0.12.0, we have completed the data reporting capability of FileAgent\u2192DataProxy\u2192Pulsar\u2192Sort. So far, InLong supports high-performance and high-reliability data access scenarios: Compared with the high-throughput TubeMQ, Apache Pulsar can provide better data consistency and is more suitable for scenarios that require extremely high data reliability. For example, finance and billing.\\n<img src=\\"/img/pulsar-arch-en.png\\" align=\\"center\\" alt=\\"Report via Pulsar\\"/>\\n\\nThanks to @healzhou, @baomingyu, @leezng, @bruceneenhl, @ifndef-SleePy and others for their contributions to this feature. For more information, please refer to [INLONG-1310](https://github.com/apache/)incubator-inlong/issues/1310), please refer to [Pulsar usage example](https://inlong.apache. org/zh -CN/docs/next/quick_start/pulsar_example/) to get the usage guide.\\n\\n#### 2. Support JMX and Prometheus metrics\\nIn addition to the existing file output metrics, the various components of InLong began to gradually support the output of JMX and Prometheus metrics to improve the visibility of the entire system. Currently, modules including Agent, DataProxy, TubeMQ, Sort-Standalone, etc. already support the above metrics, and the documentation of metrics output by each module is being sorted out.\\n\\nThanks to @shink, @luchunliang, @EMsnap, @gosonzhang and others for their contributions. For related PRs, please see [INLONG-1712](https://github.com/apache/incubator-inlong/issues/1712), [INLONG-1786] (https://github.com/apache/incubator-inlong/issues/1786), [INLONG-1796](https://github.com/apache/incubator-inlong/issues/1796), [INLONG-1827] (https://github.com/apache/incubator-inlong/issues/1827), [INLONG-1851](https://github.com/apache/incubator-inlong/issues/1851), [INLONG-1926] (https://github.com/apache/incubator-inlong/issues/1926).\\n\\n#### 3. Function extension of the modules\\nThe Sort module adds support for Apache Doris storage and realizes the ability to load sorted data into Apache Doris, giving InLong one more storage option. In addition, in order to enrich the functions of the entire data access process, the Audit and Sort-Standalone modules have been added:\\n- The Audit module provides the ability to reconcile the entire process of data flow, and monitor the service quality of the system through data flow indicators;\\n- Sort-Standalone module is a non-Flink-based data sorting module. It adds lightweight data sorting capabilities to the system, facilitating the rapid construction and use of businesses.\\n\\nThe Audit and Sort-Standalone modules are still under development and will be released when the version is stable.\\n\\nThanks to @huzk8, @doleyzi, @luchunliang and others for their contributions, please refer to [INLONG-1821](https://github.com/apache/incubator-inlong/issues/1821), [INLONG-1738]( https: / /github.com/apache/incubator-inlong/issues/1738), [INLONG-1889](https://github.com/apache/incubator-inlong/issues/1889).\\n\\n#### 4. Official website document directory reconstruction\\nIn addition to the improvement of functional modules in version 0.12.0, the official website structure and the use of documents have also been deeply adjusted, including the reconstruction of the document directory structure, supplementing and improving the function introduction of each module, adding document translation, and further improving the documentation of the InLong official website. Readability, to achieve quick search and easy reading. You can check the official website to understand this content. The document is still under construction. We welcome your valuable comments or suggestions.\\n\\nThanks to @bluewang, @dockerzhang, @healchow and others for their contributions, please refer to [INLONG-1711](https://github.com/apache/incubator-inlong/issues/1711), [INLONG-1740](https: //github.com/apache/incubator-inlong/issues/1740), [INLONG-1802](https://github.com/apache/incubator-inlong/issues/1802), [INLONG-1809](https: //github.com/apache/incubator-inlong/issues/1809), [INLONG-1810](https://github.com/apache/incubator-inlong/issues/1810), [INLONG-1815](https: //github.com/apache/incubator-inlong/issues/1815), [INLONG-1822](https://github.com/apache/incubator-inlong/issues/1822), [INLONG-1840](https: //github.com/apache/incubator-inlong/issues/1840), [INLONG-1856](https://github.com/apache/incubator-inlong/issues/1856), [INLONG-1861](https: //github.com/apache/incubator-inlong/issues/1861), [INLONG-1867](https://github.com/apache/incubator-inlong/issues/1867), [INLONG-1878](https: //github.com/apache/incubator-inlong/issues/1878), [INLONG-1901](https://github.com/apache/incubator-inlong/issues/1901), [INLONG-1939](https: //gith ub.com/apache/incubator-inlong/issues/1939).\\n\\n#### 5. Other features and bug fixes\\nFor related content, please refer to [Version Release Notes](https://github.com/apache/incubator-inlong/blob/0.12.0-incubating-RC0/CHANGES.md), which lists the detailed features of this version, Improvements, bug fixes, and specific contributors.\\n\\n\\n### Apache InLong(incubating) follow-up planning\\nIn subsequent versions, we will further enhance the capabilities of InLong to cover more usage scenarios, including:\\n- Support link for data access ClickHouse\\n- Support DB data collection\\n- The second stage full link indicator audit function"},{"id":"/apache-inlong-1.1.0","metadata":{"permalink":"/blog/apache-inlong-1.1.0","editUrl":"https://github.com/apache/incubator-inlong-website/edit/master/blog/blog/apache-inlong-1.1.0.md","source":"@site/blog/apache-inlong-1.1.0.md","title":"Release InLong 1.1.0","description":"Apache InLong is a one-stop integration framework for massive data that provides automatic, secure and reliable data transmission capabilities. InLong supports both batch and stream data processing at the same time, which offers great power to build data analysis, modeling and other real-time applications based on streaming data.","date":"2022-06-16T04:52:05.000Z","formattedDate":"June 16, 2022","tags":[],"readingTime":4.24,"truncated":false,"authors":[],"frontMatter":{"title":"Release InLong 1.1.0","sidebar_position":1},"prevItem":{"title":"Release InLong 0.12.0","permalink":"/blog/apache-inlong-0.12.0"}},"content":"Apache InLong is a one-stop integration framework for massive data that provides automatic, secure and reliable data transmission capabilities. InLong supports both batch and stream data processing at the same time, which offers great power to build data analysis, modeling and other real-time applications based on streaming data.\\n\\n## 1.1.0 Features Overview\\nThe 1.1.0-incubating just released mainly includes the following:\\n\\n### Enhanced management capabilities for manager\\n- Added Manager Client to support the integration of InLong for creating data streams\\n- Added ManagerCtl command-line tool to support viewing and creating data streams\\n- Manager supports dispatching Agent tasks\\n- Manager supports dispatching Sort Flink tasks\\n- Manger data streams management, supports start, restart, pause operations\\n- Manager plugin support\\n- Manager metadata management supports using MySQL\\n- The first phase of cluster management, unified cluster information registration\\n\\n### Rich data nodes\\n- Support Iceberg\\n- Support ClickHouse\\n- Support MySQL Binlog collection\\n- Support Kafka ingestion\\n- Sort Standalone supports multiple type streams\\n\\n### Tools construction\\n- Dashboard plugin support\\n- Kubernetes deployment optimization\\n- Standalone deployment refactoring\\n\\n### System Upgrade\\n- Protocol Buffers upgrade\\n- Unified version Maven version dependencies\\n- Fixed a bunch of dependency CVEs\\n- DataProxy supports PB compression protocol\\n\\nThis version closed about 642+ issues, including four 23 features and 180 improvements.\\n\\n## 1.1.0 Features Details\\n### Add Manager Client\\nThe newly added Manager Client defines common InLong Group/Stream operation interfaces, including task creation, viewing and deletion. Through Manager Client, users can build InLong into any third-party platform to achieve unified customized platform construction. The Manager Client is mainly designed and implemented by @kipshi, @gong, @ciscozhou, thanks to three contributors.\\n\\n### Add ManagerCtl command line tool\\nManagerCtl is developed based on Manager Client and is a command-line tool for manipulating InLong resources, which can further simplify the use of developers. ManagerCtl was contributed independently by @haifxu and includes guidelines including:\\n```\\nUsage: managerctl [options] [command] [command options]\\nOptions:\\n-h, --help\\nGet all command about managerctl.\\nCommands:\\ncreate      Create resource by json file\\nUsage: create [options]\\n\u200b\\ndescribe      Display details of one or more resources\\nUsage: describe [options]\\n\u200b\\nlist      Displays main information for one or more resources\\nUsage: list [options]\\n```\\n\\n### Manager supports issuing Sort tasks\\nIn previous versions, after the user created the data group/stream task, Sort needed to submit it to the Flink cluster through the command line, and then perform data sorting. In version 1.1.0, we unified the start, stop, and suspend operations of Sort tasks to the Manager to complete. Users only need to configure the correct Flink cluster when the Manager is deployed. When the data stream is approved, Sort will be automatically started. \\nThis part of the work is mainly contributed by @LvJiancheng.\\n\\n### Manager metadata is saved to ZooKeeper\\nInLong uses ZooKeeper to save data stream metadata, which increases the deployment threshold and operation and maintenance difficulty for developers and users. \\nIn version 1.1.0, we save data stream metadata in DB by default, and ZooKeeper is only an optional solution in large-scale scenarios. Thanks to @kipshi @yunqingmoswu for contributing a PR to ZooKeeper.\\n\\n### Support MySQL Binlog collection\\nVersion 1.1.0 fully supports the collection of data from MySQL, and supports both incremental and full strategies. Users can collect MySQL data with simple configuration in InLong. This part of the work was contributed by @EMsnap.\\n\\n### Sort Added Iceberg, ClickHouse, Kafka\\nIn version 1.1.0, the storage of data nodes for various scenarios has been added, including Iceberg, ClickHouse, and Kafka. The support of these three streams enriches the usage scenarios of InLong. New stream support, mainly contributed by @chantccc @KevinWen007 @healchow.\\n\\n### Sort Standalone supports Hive, Elasticsearch, Kafka\\nAs mentioned in the previous version, for non-Flink environments, we can sort data streams through Sort Standalone. In version 1.1.0, we added support for Hive, ElasticSearch, Kafka, and expanded the usage scenarios of Sort Standalone. Sort Standalone is mainly contributed by @vernedeng @luchunliang.\\n\\n### Protocol Buffers upgrade\\nAll InLong components Protocol Buffers dependencies have been upgraded from 2.5.0 to 3.19.4. Thanks to @gosonzhang @doleyzi for their contributions, a lot of compatibility and testing work for Protocol Buffers upgrades.\\n\\n### InLong on Kubernetes optimization\\nThe optimization work of InLong on Kubernetes mainly includes adding Audit, combing configuration, optimizing the use of message queues, optimizing the use of documents, etc., to facilitate the use of InLong on the cloud. Thanks to @shink for contributing to these optimizations.\\n\\n### Dashboard plugin\\nIn order to facilitate users to quickly build new data stream on Dashboard, Dashboard is support plugin in version 1.1.0. JavaScript developers who understand the plugin development guidelines can quickly expand new data stream. Thanks for this part of the work @leezng\\n\\n### Other features and bug fixes\\nFor related content, please refer to the version [release notes](https://github.com/apache/incubator-inlong/blob/master/CHANGES.md), which list the features, enhancements and bug fixes of this version in detail, as well as specific contributors.\\n\\n## Apache InLong(incubating) follow-up planning\\nIn subsequent versions, we will support lightweight Sort, and expand more data sources and targets to cover more usage scenarios, including:\\n- Flink SQL support\\n- Elasticsearch, HBase support"}]}')}}]);