"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[54297],{15680:(e,n,t)=>{t.d(n,{xA:()=>c,yg:()=>u});var a=t(96540);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,o=function(e,n){if(null==e)return{};var t,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var s=a.createContext({}),d=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},c=function(e){var n=d(e.components);return a.createElement(s.Provider,{value:n},e.children)},p="mdxType",g={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,o=e.mdxType,r=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),p=d(t),m=o,u=p["".concat(s,".").concat(m)]||p[m]||g[m]||r;return t?a.createElement(u,i(i({ref:n},c),{},{components:t})):a.createElement(u,i({ref:n},c))}));function u(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var r=t.length,i=new Array(r);i[0]=m;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[p]="string"==typeof e?e:o,i[1]=l;for(var d=2;d<r;d++)i[d]=t[d];return a.createElement.apply(null,i)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},39824:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>i,default:()=>g,frontMatter:()=>r,metadata:()=>l,toc:()=>d});var a=t(58168),o=(t(96540),t(15680));const r={title:"Sort Extension Connector",sidebar_position:1},i="Sort Extension Connector",l={unversionedId:"development/extension_sort/extension_connector",id:"development/extension_sort/extension_connector",title:"Sort Extension Connector",description:"Overview",source:"@site/docs/development/extension_sort/extension_connector.md",sourceDirName:"development/extension_sort",slug:"/development/extension_sort/extension_connector",permalink:"/docs/next/development/extension_sort/extension_connector",draft:!1,editUrl:"https://github.com/apache/inlong-website/edit/master/docs/development/extension_sort/extension_connector.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"Sort Extension Connector",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Manager Custom Data Node",permalink:"/docs/next/development/extension_manager/inlong_manger_data_node_extension"},next:{title:"InLong sort format extend",permalink:"/docs/next/development/extension_sort/inlong_sort_data_organization_and_binary_protocol"}},s={},d=[{value:"Overview",id:"overview",level:2},{value:"Extending Extract &amp; Load Node",id:"extending-extract--load-node",level:2},{value:"Adding Extract &amp; Load Node Definitions",id:"adding-extract--load-node-definitions",level:3},{value:"Extend Extract Node",id:"extend-extract-node",level:3},{value:"Extend Load Node",id:"extend-load-node",level:3},{value:"Integrate Entrance",id:"integrate-entrance",level:3},{value:"InlongMetric",id:"inlongmetric",level:2},{value:"Metadata",id:"metadata",level:2},{value:"Extending Apache Flink Connector",id:"extending-apache-flink-connector",level:2},{value:'<span id="jump1">How to Integrate Inlong Audit into Custom Connector</span>',id:"how-to-integrate-inlong-audit-into-custom-connector",level:2}],c={toc:d},p="wrapper";function g(e){let{components:n,...r}=e;return(0,o.yg)(p,(0,a.A)({},c,r,{components:n,mdxType:"MDXLayout"}),(0,o.yg)("h1",{id:"sort-extension-connector"},"Sort Extension Connector"),(0,o.yg)("h2",{id:"overview"},"Overview"),(0,o.yg)("p",null,"InLong Sort is an ETL service based on Apache Flink SQL, the powerful expressive power of Flink SQL brings high scalability and flexibility.\nBasically, the semantics supported by Flink SQL are supported by InLong Sort. In some scenarios, when the built-in functions of Flink SQL do not meet the requirements,\nthey can also be extended through various UDFs in InLong Sort. At the same time, it will be easier for those who have used SQL, especially Flink SQL, to get started."),(0,o.yg)("p",null,"This article describes how to extend a new source (abstracted as extract node in inlong) or a new sink (abstracted as load node in inlong) in InLong Sort.\nThe architecture of inlong sort can be represented by UML object relation diagram as:"),(0,o.yg)("p",null,(0,o.yg)("img",{alt:"sort_UML",src:t(57026).A,width:"2576",height:"869"})),(0,o.yg)("p",null,"The concepts of each component are:"),(0,o.yg)("table",null,(0,o.yg)("thead",{parentName:"table"},(0,o.yg)("tr",{parentName:"thead"},(0,o.yg)("th",{parentName:"tr",align:null},(0,o.yg)("strong",{parentName:"th"},"Name")),(0,o.yg)("th",{parentName:"tr",align:null},(0,o.yg)("strong",{parentName:"th"},"Description")))),(0,o.yg)("tbody",{parentName:"table"},(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("strong",{parentName:"td"},"Group")),(0,o.yg)("td",{parentName:"tr",align:null},"data flow group, including multiple data flows, one group represents one data access")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("strong",{parentName:"td"},"Stream")),(0,o.yg)("td",{parentName:"tr",align:null},"data flow, a data flow has a specific flow direction")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("strong",{parentName:"td"},"GroupInfo")),(0,o.yg)("td",{parentName:"tr",align:null},"encapsulation of data flow in sort. a groupinfo can contain multiple dataflowinfo")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("strong",{parentName:"td"},"StreamInfo")),(0,o.yg)("td",{parentName:"tr",align:null},"abstract of data flow in sort, including various sources, transformations, destinations, etc.")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("strong",{parentName:"td"},"Node")),(0,o.yg)("td",{parentName:"tr",align:null},"abstraction of data source, data transformation and data destination in data synchronization")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("strong",{parentName:"td"},"ExtractNode")),(0,o.yg)("td",{parentName:"tr",align:null},"source-side abstraction for data synchronization")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("strong",{parentName:"td"},"TransformNode")),(0,o.yg)("td",{parentName:"tr",align:null},"transformation process abstraction of data synchronization")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("strong",{parentName:"td"},"LoadNode")),(0,o.yg)("td",{parentName:"tr",align:null},"destination abstraction for data synchronization")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("strong",{parentName:"td"},"NodeRelationShip")),(0,o.yg)("td",{parentName:"tr",align:null},"abstraction of each node relationship in data synchronization")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("strong",{parentName:"td"},"FieldRelationShip")),(0,o.yg)("td",{parentName:"tr",align:null},"abstraction of the relationship between upstream and downstream node fields in data synchronization")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("strong",{parentName:"td"},"FieldInfo")),(0,o.yg)("td",{parentName:"tr",align:null},"node field")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("strong",{parentName:"td"},"MetaFieldInfo")),(0,o.yg)("td",{parentName:"tr",align:null},"node meta fields")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("strong",{parentName:"td"},"Function")),(0,o.yg)("td",{parentName:"tr",align:null},"abstraction of transformation function")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("strong",{parentName:"td"},"FunctionParam")),(0,o.yg)("td",{parentName:"tr",align:null},"input parameter abstraction of function")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("strong",{parentName:"td"},"ConstantParam")),(0,o.yg)("td",{parentName:"tr",align:null},"constant parameters")))),(0,o.yg)("h2",{id:"extending-extract--load-node"},"Extending Extract & Load Node"),(0,o.yg)("p",null,"The Extract nodes is a set of Source Connectors based on ",(0,o.yg)("a",{href:"https://flink.apache.org/"},"Apache Flink",(0,o.yg)("sup",null,"\xae"))," used to extract data from different source systems.\nThe Load nodes is a set of Sink Connectors based on ",(0,o.yg)("a",{href:"https://flink.apache.org/"},"Apache Flink",(0,o.yg)("sup",null,"\xae"))," used to load data into different storage systems."),(0,o.yg)("p",null,"When Apache InLong Sort starts, it translates a set of Extract and Load Node configurations into corresponding Flink SQL and submits them to the Flink cluster, initiating the data extraction and loading tasks specified by the user."),(0,o.yg)("h3",{id:"adding-extract--load-node-definitions"},"Adding Extract & Load Node Definitions"),(0,o.yg)("p",null,"To customize an ",(0,o.yg)("inlineCode",{parentName:"p"},"Extract Node"),", you need to inherit the ",(0,o.yg)("inlineCode",{parentName:"p"},"org.apache.inlong.sort.protocol.node.ExtractNode")," class, and to customize a ",(0,o.yg)("inlineCode",{parentName:"p"},"Load Node"),", you need to inherit the ",(0,o.yg)("inlineCode",{parentName:"p"},"org.apache.inlong.sort.protocol.node.LoadNode")," class. Both must selectively implement methods from the ",(0,o.yg)("inlineCode",{parentName:"p"},"org.apache.inlong.sort.protocol.node.Node")," interface."),(0,o.yg)("table",null,(0,o.yg)("thead",{parentName:"table"},(0,o.yg)("tr",{parentName:"thead"},(0,o.yg)("th",{parentName:"tr",align:null},"Method Name"),(0,o.yg)("th",{parentName:"tr",align:null},"Meaning"),(0,o.yg)("th",{parentName:"tr",align:null},"Default Value"))),(0,o.yg)("tbody",{parentName:"table"},(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},"getId"),(0,o.yg)("td",{parentName:"tr",align:null},"Get node ID"),(0,o.yg)("td",{parentName:"tr",align:null},"Inlong StreamSource Id")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},"getName"),(0,o.yg)("td",{parentName:"tr",align:null},"Get node name"),(0,o.yg)("td",{parentName:"tr",align:null},"Inlong StreamSource Name")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},"getFields"),(0,o.yg)("td",{parentName:"tr",align:null},"Get field information"),(0,o.yg)("td",{parentName:"tr",align:null},"Fields defined by Inlong Stream")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},"getProperties"),(0,o.yg)("td",{parentName:"tr",align:null},"Get additional node properties"),(0,o.yg)("td",{parentName:"tr",align:null},"Empty Map")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},"tableOptions"),(0,o.yg)("td",{parentName:"tr",align:null},"Get Flink SQL table properties"),(0,o.yg)("td",{parentName:"tr",align:null},"Additional node properties")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},"genTableName"),(0,o.yg)("td",{parentName:"tr",align:null},"Generate Flink SQL table name"),(0,o.yg)("td",{parentName:"tr",align:null},"No default value")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},"getPrimaryKey"),(0,o.yg)("td",{parentName:"tr",align:null},"Get primary key"),(0,o.yg)("td",{parentName:"tr",align:null},"null")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},"getPartitionFields"),(0,o.yg)("td",{parentName:"tr",align:null},"Get partition fields"),(0,o.yg)("td",{parentName:"tr",align:null},"null")))),(0,o.yg)("h3",{id:"extend-extract-node"},"Extend Extract Node"),(0,o.yg)("p",null,"There are three steps to extend an ExtractNode:"),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 1"),"\uff1aInherit the ExtractNode class,the location of the class is:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"inlong-sort/sort-common/src/main/java/org/apache/inlong/sort/protocol/node/ExtractNode.java\n")),(0,o.yg)("p",null,"Specify the connector in the implemented ExtractNode."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-Java"},'// Inherit ExtractNode class and implement specific classes, such as MongoExtractNode\n@EqualsAndHashCode(callSuper = true)\n@JsonTypeName("MongoExtract")\n@Data\npublic class MongoExtractNode extends ExtractNode implements Serializable {\n    @JsonInclude(Include.NON_NULL)\n    @JsonProperty("primaryKey")\n    private String primaryKey;\n    ...\n\n    @JsonCreator\n    public MongoExtractNode(@JsonProperty("id") String id, ...) { ... }\n\n    @Override\n    public Map<String, String> tableOptions() {\n        Map<String, String> options = super.tableOptions();\n        // configure the specified connector, here is mongodb-cdc\n        options.put("connector", "mongodb-cdc");\n        ...\n        return options;\n    }\n}\n')),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 2"),"\uff1aadd the Extract to JsonSubTypes in ExtractNode and Node"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-java"},'// add field in JsonSubTypes of ExtractNode and Node\n...\n@JsonSubTypes({\n        @JsonSubTypes.Type(value = MongoExtractNode.class, name = "mongoExtract")\n})\n...\npublic abstract class ExtractNode implements Node{...}\n\n...\n@JsonSubTypes({\n        @JsonSubTypes.Type(value = MongoExtractNode.class, name = "mongoExtract")\n})\npublic interface Node {...}\n')),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 3"),"\uff1aExpand the Sort connector and check whether the corresponding connector already exists in the (",(0,o.yg)("inlineCode",{parentName:"p"},"InLong Agentinlong-sort/sort-connectors/mongodb-cdc"),") directory. If you haven't already,\nyou need to refer to the official flink documentation ",(0,o.yg)("a",{parentName:"p",href:"https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/datastream/overview/#datastream-connectors"},"DataStream Connectors")," to extend,\ndirectly call the existing flink-connector (such as",(0,o.yg)("inlineCode",{parentName:"p"},"inlong-sort/sort-connectors/mongodb-cdc"),") or implement the related connector by yourself."),(0,o.yg)("h3",{id:"extend-load-node"},"Extend Load Node"),(0,o.yg)("p",null,"There are three steps to extend an LoadNode:"),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 1"),"\uff1aInherit the LoadNode class, the location of the class is:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"inlong-sort/sort-common/src/main/java/org/apache/inlong/sort/protocol/node/LoadNode.java\n")),(0,o.yg)("p",null,"specify the connector in the implemented LoadNode."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-java"},'// Inherit LoadNode class and implement specific classes, such as KafkaLoadNode\n@EqualsAndHashCode(callSuper = true)\n@JsonTypeName("kafkaLoad")\n@Data\n@NoArgsConstructor\npublic class KafkaLoadNode extends LoadNode implements Serializable {\n    @Nonnull\n    @JsonProperty("topic")\n    private String topic;\n    ...\n\n    @JsonCreator\n    public KafkaLoadNode(@Nonnull @JsonProperty("topic") String topic, ...) {...}\n\n    // configure and use different connectors according to different conditions\n    @Override\n    public Map<String, String> tableOptions() {\n      ...\n        if (format instanceof JsonFormat || format instanceof AvroFormat || format instanceof CsvFormat) {\n            if (StringUtils.isEmpty(this.primaryKey)) {\n                // kafka connector\n                options.put("connector", "kafka");\n                options.putAll(format.generateOptions(false));\n            } else {\n                options.put("connector", "upsert-kafka"); // upsert-kafka connector\n                options.putAll(format.generateOptions(true));\n            }\n        } else if (format instanceof CanalJsonFormat || format instanceof DebeziumJsonFormat) {\n            // kafka-inlong connector\n            options.put("connector", "kafka-inlong");\n            options.putAll(format.generateOptions(false));\n        } else {\n            throw new IllegalArgumentException("kafka load Node format is IllegalArgument");\n        }\n        return options;\n    }\n}\n')),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 2"),"\uff1aadd the Load to JsonSubTypes in ExtractNode and Node"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-java"},'// add field in JsonSubTypes of LoadNode and Node\n...\n@JsonSubTypes({\n        @JsonSubTypes.Type(value = KafkaLoadNode.class, name = "kafkaLoad")\n})\n...\npublic abstract class LoadNode implements Node{...}\n\n...\n@JsonSubTypes({\n        @JsonSubTypes.Type(value = KafkaLoadNode.class, name = "kafkaLoad")\n})\npublic interface Node {...}\n')),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 3"),"\uff1aExtend the Sort connector, Kafka's sort connector is in ",(0,o.yg)("inlineCode",{parentName:"p"},"inlong-sort/sort-connectors/kafka"),"."),(0,o.yg)("h3",{id:"integrate-entrance"},"Integrate Entrance"),(0,o.yg)("p",null,"To integrate extract and load into the InLong Sort mainstream, you need to implement the semantics mentioned in the overview section: group, stream, node, etc. The entry class of InLong Sort is in :"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"inlong-sort/sort-core/src/main/java/org/apache/inlong/sort/Entrance.java\n")),(0,o.yg)("p",null,"How to integrate extract and load into InLong Sort can refer to the following ut. First, build the corresponding extractnode and loadnode, then build noderelation, streaminfo and groupinfo, and finally use FlinkSqlParser to execute."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-java"},'public class MongoExtractToKafkaLoad extends AbstractTestBase {\n\n    // create MongoExtractNode\n    private MongoExtractNode buildMongoNode() {\n        List<FieldInfo> fields = Arrays.asList(new FieldInfo("name", new StringFormatInfo()), ...);\n        return new MongoExtractNode(..., fields, ...);\n    }\n\n    // create KafkaLoadNode\n    private KafkaLoadNode buildAllMigrateKafkaNode() {\n        List<FieldInfo> fields = Arrays.asList(new FieldInfo("name", new StringFormatInfo()), ...);\n        List<FieldRelation> relations = Arrays.asList(new FieldRelation(new FieldInfo("name", new StringFormatInfo()), ...), ...);\n        CsvFormat csvFormat = new CsvFormat();\n        return new KafkaLoadNode(..., fields, relations, csvFormat, ...);\n    }\n\n    // create NodeRelation\n    private NodeRelation buildNodeRelation(List<Node> inputs, List<Node> outputs) {\n        List<String> inputIds = inputs.stream().map(Node::getId).collect(Collectors.toList());\n        List<String> outputIds = outputs.stream().map(Node::getId).collect(Collectors.toList());\n        return new NodeRelation(inputIds, outputIds);\n    }\n\n    // test the main flow: mongodb to kafka\n    @Test\n    public void testMongoDbToKafka() throws Exception {\n        EnvironmentSettings settings = EnvironmentSettings. ... .build();\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        ...\n        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);\n        Node inputNode = buildMongoNode();\n        Node outputNode = buildAllMigrateKafkaNode();\n        StreamInfo streamInfo = new StreamInfo("1", Arrays.asList(inputNode, outputNode), ...);\n        GroupInfo groupInfo = new GroupInfo("1", Collections.singletonList(streamInfo));\n        FlinkSqlParser parser = FlinkSqlParser.getInstance(tableEnv, groupInfo);\n        ParseResult result = parser.parse();\n        Assert.assertTrue(result.tryExecute());\n    }\n}\n')),(0,o.yg)("p",null,"Additionally, Sort has added two extra interfaces, ",(0,o.yg)("inlineCode",{parentName:"p"},"InlongMetric")," and ",(0,o.yg)("inlineCode",{parentName:"p"},"Metadata"),", to support richer semantics."),(0,o.yg)("h2",{id:"inlongmetric"},"InlongMetric"),(0,o.yg)("p",null,"If a custom node needs to report Inlong metrics, it must implement the ",(0,o.yg)("inlineCode",{parentName:"p"},"org.apache.inlong.sort.protocol.InlongMetric")," interface.\nWhen Sort parses the configuration, it adds the startup parameter ",(0,o.yg)("inlineCode",{parentName:"p"},"'inlong.metric.labels' = 'groupId={g}&streamId={s}&nodeId={n}'")," to the table option, which is used to configure Inlong Audit.\nFor details, see ",(0,o.yg)("a",{parentName:"p",href:"#jump1"},"How to Integrate Inlong Audit into Custom Connector")),(0,o.yg)("h2",{id:"metadata"},"Metadata"),(0,o.yg)("p",null,"If a custom node needs to specify a field as a Flink SQL Metadata field, it must implement the ",(0,o.yg)("inlineCode",{parentName:"p"},"org.apache.inlong.sort.protocol.Metadata")," interface.\nSort will automatically mark the corresponding field as Metadata when parsing the configuration."),(0,o.yg)("h2",{id:"extending-apache-flink-connector"},"Extending Apache Flink Connector"),(0,o.yg)("p",null,"Sort is implemented based on Apache Flink version 1.15. For information on how to extend the Apache Flink Connector, refer to ",(0,o.yg)("a",{href:"https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/dev/table/sourcessinks/"},"User-defined Sources & Sinks")),(0,o.yg)("h2",{id:"how-to-integrate-inlong-audit-into-custom-connector"},(0,o.yg)("span",{id:"jump1"},"How to Integrate Inlong Audit into Custom Connector")),(0,o.yg)("p",null,"Inlong Sort encapsulates the metric reporting process in the ",(0,o.yg)("inlineCode",{parentName:"p"},"org.apache.inlong.sort.base.metric.SourceExactlyMetric")," and ",(0,o.yg)("inlineCode",{parentName:"p"},"org.apache.inlong.sort.base.metric.SinkExactlyMetric")," classes. Developers only need to initialize the corresponding ",(0,o.yg)("inlineCode",{parentName:"p"},"Metric")," object according to the Source/Sink type to implement metric reporting."),(0,o.yg)("p",null,"The common practice is to pass parameters such as the InLong Audit address when constructing the Source/Sink, and initialize the ",(0,o.yg)("inlineCode",{parentName:"p"},"SourceExactlyMetric/SinkExactlyMetric")," object when calling the open() method to initialize the Source/Sink operator. After processing the actual data, call the corresponding audit reporting method."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"public class StarRocksDynamicSinkFunctionV2<T> extends StarRocksDynamicSinkFunctionBase<T> {\n\n    private static final long serialVersionUID = 1L;\n    private static final Logger log = LoggerFactory.getLogger(StarRocksDynamicSinkFunctionV2.class);\n\n    private transient SinkExactlyMetric sinkExactlyMetric;\n\n    private String inlongMetric;\n    private String auditHostAndPorts;\n    private String auditKeys;\n    private String stateKey;\n\n    public StarRocksDynamicSinkFunctionV2(StarRocksSinkOptions sinkOptions,\n            TableSchema schema,\n            StarRocksIRowTransformer<T> rowTransformer, String inlongMetric,\n            String auditHostAndPorts, String auditKeys) {\n        this.sinkOptions = sinkOptions;\n        \n        // pass the params of inlong audit\n        this.auditHostAndPorts = auditHostAndPorts;\n        this.inlongMetric = inlongMetric;\n        this.auditKeys = auditKeys;\n    }\n\n    @Override\n    public void open(Configuration parameters) {\n\n        // init SinkExactlyMetric in open()\n        MetricOption metricOption = MetricOption.builder().withInlongLabels(inlongMetric)\n                .withAuditAddress(auditHostAndPorts)\n                .withAuditKeys(auditKeys)\n                .build();\n\n        if (metricOption != null) {\n            sinkExactlyMetric = new SinkExactlyMetric(metricOption, getRuntimeContext().getMetricGroup());\n        }\n    }\n    \n    @Override\n    public void invoke(T value, Context context)\n            throws IOException, ClassNotFoundException, JSQLParserException {\n        Object[] data = rowTransformer.transform(value, sinkOptions.supportUpsertDelete());\n\n        sinkManager.write(\n                null,\n                sinkOptions.getDatabaseName(),\n                sinkOptions.getTableName(),\n                serializer.serialize(schemaUtils.filterOutTimeField(data)));\n\n        // output audit after write data to sink\n        if (sinkExactlyPropagateMetric != null) {\n            sinkExactlyPropagateMetric.invoke(1, getDataSize(value), schemaUtils.getDataTime(data));\n        }\n    }\n")))}g.isMDXComponent=!0},57026:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/sort_uml-d90bb6f0835781e064b7417f266b7b30.png"}}]);